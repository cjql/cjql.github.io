<!DOCTYPE html>
<html lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">

    <!--[if lt IE 9]>
        <style>body {display: none; background: none !important} </style>
        <meta http-equiv="Refresh" Content="0; url=//outdatedbrowser.com/" />
    <![endif]-->


<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="format-detection" content="telephone=no">
<meta name="author" content="CJ">
<meta name="renderer" content="webkit">
<meta name="google" value="notranslate">



<meta name="description" content="爬虫体系归纳。">
<meta property="og:type" content="article">
<meta property="og:title" content="爬虫知识结构">
<meta property="og:url" content="https://cjql.github.io/爬虫知识结构/index.html">
<meta property="og:site_name" content="CJ">
<meta property="og:description" content="爬虫体系归纳。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2020-04-30T09:44:16.577Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="爬虫知识结构">
<meta name="twitter:description" content="爬虫体系归纳。">

<link rel="apple-touch-icon" href="/apple-touch-icon.png">


    <link rel="alternative" href="/atom.xml" title="CJ" type="application/atom+xml">



    <link rel="shortcut icon" href="/resources/favicon.ico">



    <link href="//cdn.bootcss.com/animate.css/3.5.1/animate.min.css" rel="stylesheet">



    <link href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet">



    <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
    <link href="//cdn.bootcss.com/pace/1.0.2/themes/yellow/pace-theme-minimal.css" rel="stylesheet">


<link rel="stylesheet" href="/css/style.css">



<link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">


<title>爬虫知识结构 | CJ</title>

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

<script>
    var yiliaConfig = {
        fancybox: true,
        animate: true,
        isHome: false,
        isPost: true,
        isArchive: false,
        isTag: false,
        isCategory: false,
        fancybox_js: "//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js",
        scrollreveal: "//cdn.bootcss.com/scrollReveal.js/3.1.4/scrollreveal.min.js",
        search: true
    }
</script>


    <script>
        yiliaConfig.jquery_ui = [true, "//cdn.bootcss.com/jqueryui/1.10.4/jquery-ui.min.js", "//cdn.bootcss.com/jqueryui/1.10.4/css/jquery-ui.min.css"];
    </script>



    <script> yiliaConfig.rootUrl = "\/";</script>





    <script>
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?a4938c90e05043bd9774ff12368e45aa";
            var s = document.getElementsByTagName("script")[0]; 
            s.parentNode.insertBefore(hm, s);
        })();
    </script>


</head></html>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            <img src="/resources/apple-touch-icon.png" class="animated zoomIn">
        </a>
        <!-- <hgroup>
          <h1 class="header-author"><a href="/">CJ</a></h1>
        </hgroup> -->

        
        <p class="header-subtitle">cs</p>
        

        
            <form id="search-form">
            <input type="text" id="local-search-input" name="q" placeholder="search..." class="search form-control" autocomplete="off" autocorrect="off" searchonload="false">
            <i class="fa fa-times" onclick="resetSearch()"></i>
            </form>
            <div id="local-search-result"></div>
            <p class="no-result">No results found <i class="fa fa-spinner fa-pulse"></i></p>
        


        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>菜单</li>
                        <li>标签</li>
                        
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="/archives/">优先级</a></li>
                        
                            <li><a href="/tags/">标签云</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa GitHub" href="https://github.com/" title="GitHub"></a>
                            
                                <a class="fa 豆瓣" href="https://book.douban.com/" title="豆瓣"></a>
                            
                                <a class="fa Twitter" href="//twitter.com/" title="Twitter"></a>
                            
                                <a class="fa StackOverflow" href="https://stackoverflow.com/" title="StackOverflow"></a>
                            
                                <a class="fa Flickr" href="https://www.flickr.com/" title="Flickr"></a>
                            
                                <a class="fa 博客园" href="https://www.cnblogs.com/" title="博客园"></a>
                            
                                <a class="fa CSDN" href="https://www.csdn.net/" title="CSDN"></a>
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/API/">API</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LeetCode/">LeetCode</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLP/">NLP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/工具/">工具</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/数据库/">数据库</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/方法论/">方法论</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/目录/">目录</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/算法/">算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/编程语言/">编程语言</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/计算机/">计算机</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/计算机基础/">计算机基础</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/计算机组成/">计算机组成</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/计算机网络/">计算机网络</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/记忆/">记忆</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/软件开发/">软件开发</a></li></ul>
                    </div>
                </section>
                
                
                

                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">CJ</a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                <img src="/resources/apple-touch-icon.png" class="animated zoomIn">
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页">CJ</a></h1>
            </hgroup>
            
            <p class="header-subtitle">cs</p>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/archives/">优先级</a></li>
                
                    <li><a href="/tags/">标签云</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa GitHub" target="_blank" href="https://github.com/" title="GitHub"></a>
                            
                                <a class="fa 豆瓣" target="_blank" href="https://book.douban.com/" title="豆瓣"></a>
                            
                                <a class="fa Twitter" target="_blank" href="//twitter.com/" title="Twitter"></a>
                            
                                <a class="fa StackOverflow" target="_blank" href="https://stackoverflow.com/" title="StackOverflow"></a>
                            
                                <a class="fa Flickr" target="_blank" href="https://www.flickr.com/" title="Flickr"></a>
                            
                                <a class="fa 博客园" target="_blank" href="https://www.cnblogs.com/" title="博客园"></a>
                            
                                <a class="fa CSDN" target="_blank" href="https://www.csdn.net/" title="CSDN"></a>
                            
                        </ul>
            </nav>
        </header>                
    </div>
</nav>
      <div class="body-wrap"><article id="post-爬虫知识结构" class="article article-type-post" itemscope="" itemprop="blogPost">
    
      <div class="article-meta">
        <a href="/爬虫知识结构/" class="article-date">
      <time datetime="2034-12-31T16:00:00.000Z" itemprop="datePublished">2035-01-01</time>
</a>


      </div>
    
    <div class="article-inner">
      
        <input type="hidden" class="isFancy">
      
      
        <header class="article-header">
          
  
    <h1 class="article-title" itemprop="name">
      爬虫知识结构
    </h1>
  

        </header>
        
        <div class="article-info article-info-post">
          

          
          <div class="clearfix"></div>
        </div>
        
      
      <div class="article-entry" itemprop="articleBody">
        
        
            
          <p>爬虫体系归纳。<br><a id="more"></a></p>
<h1 id="1-爬虫基础"><a href="#1-爬虫基础" class="headerlink" title="1. 爬虫基础"></a>1. 爬虫基础</h1><p>爬虫工程师的前景、爬虫工作原理、网络爬虫工作原理详解</p>
<ul>
<li>从结构化（HTML）和非结构化（流数据）的数据中获取信息<br>什么是网络爬虫、如何分析静态网站、如何开发一个完整的爬虫<br>动态网站分析和爬取的两种思路<br>如何从HTML页面中提取出有效的数据，以及对如何将数据合理地存储成各类文件以实现持久化。<br>一个爬虫所应该具有的全部功能组件以及编码实现<br>分布式爬虫，功能和基础爬虫一致，在单机爬虫的基础上进行分布式改进，帮助大家从根本上了解分布式爬虫，消除分布式爬虫的神秘感。<br>、动态网站的抓取、协议分析<br>爬虫的设计及实现流程<br>熟悉抓包分析请求并模拟<br>正则表达式、Path、css选择器、Xpath<br>熟练使用selenium、lxml、bs4，对xml、html的文本进行抓取解析清洗<br>完整地介绍了爬虫程序的每一个知识模块<br>网络爬虫<br>组成<br>什么是<br>初识<br>爬虫（网页）抓取原理及技术<br>基本议题和框架<br>遇到爬虫问题的时候的各种解决方法<h2 id="1-1-为什么要学"><a href="#1-1-为什么要学" class="headerlink" title="1.1. 为什么要学"></a>1.1. 为什么要学</h2>为什么、好处、重要性、作用、意义、优势、不足、历史、现状、趋势、大背景。<br>不断解决遇到的疑惑。<br>科技如何给大家带来实效<br>数据的存储对公司有什么影响<br>如何存储数据⇒高效利用 方便对接其他部门和业务<br>如何使用淘宝网上所有绿色产品（如空气净化器）的销量数据来做潜在市场评估<br>如何一直高效率、持续不断地从日新月异的网站中获取信息<br>互联网的运作和结构<br>爬虫程序是收集信息的基础。</li>
<li>学习网络爬虫的原因</li>
<li>网络爬虫带来的价值</li>
<li>飞速发展的大数据时代。<br>市场营销：丰富的数据信息让我们有能力更好地了解消费者、顾客和竞争对手。电商网站评论收集可以及时知悉顾客对于产品的看法，通过微博数据收集可以及时洞察潜在消费者的购买意向和需求。<br>企业管理人员：通过对手网站信息收集可以及时知晓对手的实时动态，真正做到运筹帷幄之中，决胜千里之外。<br>打开数据信息收集大门的钥匙。<br>有志于在数据分析方面有所突破。<br>使用Python编写网络爬虫程序获取互联网上的大数据是当前的热门专题。<br>技术创新驱动变革的潮流。<br>数据量爆发式增长的互联网时代。<br>大数据分析的火热。<br>大数据成为业界与学术界最火热的话题之一。<br>数据已经成为每个公司极为重要的资产。<br>互联网大量的公开数据为个人和公司提供了以往想象不到的可以获取的数据量。<br>网络爬虫技术是大数据分析的第一环。有助于获取有用的公开数据集。<br>理解了信息的获取、存储和整理，才有可能系统地收集和应用不同源头和千变万化的网站信息。<br>DT的核心是从信息的源头去理解和分析，以做出能打动对方的行动决策方案。<br>由谷歌搜索到现在的大数据时代，爬虫技术的重要性和广泛性一直很突出。<br>爬取目标网站的资料、分析和建立应用。 获取数据自动、实时、及时、省时。<br>电商市场的重要性日益凸显。了解对手的产品特点、价格以及销量情况，及时跟进产品开发进度和营销策略，从而知己知彼，赢得竞争。过去，两个痛点——无法自动化和无法实时获取。产品研发部门会手动访问一个个电商产品页面，人工复制并粘贴到Excel表格中，制作竞品分析报告。但是这种重复性的手动工作不仅浪费宝贵的时间，一不留神复制少了一个数字还会导致数据错误；对手产品的销量则是由某一家咨询公司提供报告，每周一次，但是报告缺乏实时性，难以针对快速多变的市场及时调整价格和营销策略。<br>学会一项新的技术<br>第一方企业（也就是拥有这些数据的企业）做出更好的决策<br>第三方企业也可从中受益<br>数据共享<br>Python：热门的开源软件（这意味着有人源源不断地开发更新且更强大的包给你用）<br>Python：简单、简洁、易学、有效、可扩展性的计算机语言。 最受欢迎的程序语言之一。 强大而丰富的库。<br>C语言：底层，学习成本大。<h2 id="1-2-类型"><a href="#1-2-类型" class="headerlink" title="1.2. 类型"></a>1.2. 类型</h2>有哪些</li>
<li>聚焦</li>
<li>增量式<h2 id="1-3-技能"><a href="#1-3-技能" class="headerlink" title="1.3. 技能"></a>1.3. 技能</h2>技能总览<br>实现原理<br>实现技术<br>可以做什么<br>开发技术</li>
<li>三大库</li>
<li>中间件（Middleware）、中间件的变化、中间件延伸</li>
<li>提升速度、迅速技巧、倍速</li>
<li>抓包与中间人</li>
<li>定向爬取技术<br>部署与计划运行<br>攻防战<br>Android原生App<br>基于逆向分析小程序的<br>常见搜索算法<br>JSONAPl和AJAX<h2 id="1-4-应用场景"><a href="#1-4-应用场景" class="headerlink" title="1.4. 应用场景"></a>1.4. 应用场景</h2>一些附加值更高的“事”，如人工智能、统计建模等。<br>机器学习和统计算法分析<br>在营销领域可以帮助企业做好4P（Product：产品创新，Place：智能选址，Price：动态价格，Promotion：数据驱动的营销活动）<br>在金融领域，数据驱动的征信等应用会带来越来越大的价值。<br>公开数据的应用价值<br>所有网络数据<br>社交媒体的每一条发帖。社交媒体在用户生态圈的自我交互下产生大量文本、图片和视频数据。<br>团购网站的价格及点评。电商商产品的描述、价格<br>招聘网站的招聘信息<br>搜索引擎从数据库中提取搜索结果<h2 id="1-5-流程图"><a href="#1-5-流程图" class="headerlink" title="1.5. 流程图"></a>1.5. 流程图</h2>具体步骤及各步骤之间的关系。<br>获【取】网页、解【析】网页（提取数据）、【存】储数据、整【理】。</li>
<li>获取网页：给一个网址发送请求，该网址会返回整个网页的数据。类似于在浏览器中键入网址并按回车键，然后可以看到网站的整个页面。</li>
<li>解析网页：从整个网页的数据中提取想要的数据。类似于在浏览器中看到网站的整个页面，但是你想找的是产品的价格，价格就是你想要的数据。</li>
<li>存储数据：把数据存储下来。<br>三个流程的技术实现:</li>
<li>获取网页<br>获取网页的基础技术：request、urllib和selenium（模拟浏览器）。<br>获取网页的进阶技术：多进程多线程抓取、登录抓取、突破IP封禁和服务器抓取。<br>并发编程(多线程(池)、多进程(池)、futures)<br>异步编程(asyncio)</li>
<li>解析网页<br>解析网页的基础技术：re正则表达式、BeautifulSoup和lxml。<br>解析网页的进阶技术：解决中文乱码。</li>
<li>存储数据<br>存储数据的基础技术：存入txt文件和存入csv文件。<br>存储数据的进阶技术：存入MySQL数据库和存入MongoDB数据库。<br>发起请求——通过HTTP库向⽬目标站点发起请求，即发送一个Request，请求可以包含额外的headers等信息，等待服务器器响应。<br>获取响应内容 ——如果服务器器能正常响应，会得到一个Response，Response的内容便便是所要获取的⻚页⾯面内容，类型可能有HTML，Json字符串串，二进制数据（如图⽚片视频）等类型。<br>解析内容 ——得到的内容可能是HTML，可以⽤用正则表达式、⽹网⻚页解析库进⾏解析。可能是Json，可以直接转为Json对象解析，可能是二进制数据，可以做保存或者进一步的处理理。<br>保存数据 ——保存形式多样，可以存为文本，也可以保存⾄至数据库，或者保存特定格式的文件。<h1 id="2-获取网页"><a href="#2-获取网页" class="headerlink" title="2. 获取网页"></a>2. 获取网页</h1><h2 id="2-1-法律和道德、注意事项"><a href="#2-1-法律和道德、注意事项" class="headerlink" title="2.1. 法律和道德、注意事项"></a>2.1. 法律和道德、注意事项</h2>网络爬虫是否合法<br>法律问题<br>道德协议<br>在充满爬虫的世界里做一个好公民<br>用户爬虫的那些事儿<br>是否合法<br>基本议题<br>成规模的爬虫一般都会使用集群，一般的小网站服务器规模可能不如爬虫集群的规模大。所以很多时候我们最好对要爬的网站限制一下频率。否则这些爬虫就相当于DoS攻击集群了！一般的网站都会有robots.txt可以参考。<br>爬虫有哪些潜在的法律纠纷、公司的爬虫合不合法 。<br>建立共利的互联网环境，不能把爬虫作为窃取数据的工具。<br>爬虫必须在合情、合法、合理的情况下获取和应用。<br>尊重数据供应者的知识产权和正常运作才能产生长久共利的环境。<br>保障对方平台的正常运作是每个程序员都应当做到的<br>法律：<br>互联网世界已经通过自身的协议建立起一定的道德规范（Robots协议）。该协议是国际互联网界通行的道德规范，虽然没有写入法律，但是每一个爬虫都应该遵守这项协议。<br>法律部分还在建立和完善中。<br>如果抓取的数据属于个人使用或科研范畴，基本不存在问题。当你爬取网站数据时，无论是否仅供个人使用，都应该遵守Robots协议。<br>而如果数据属于商业盈利范畴，就要就事而论，有可能属于违法行为，也有可能不违法。<br>大部分网站不欢迎使用程序进行登录，因为需要登录才能查看的数据不属于公开数据。最好不要使用此程序获取非公开数据或批量注册，若出现了问题，可能需负法律责任。<br>建议使用API。<br>Robots协议<br>Robots协议（爬虫协议）的全称是“网络爬虫排除标准”（Robots Exclusion Protocol），网站通过Robots协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取。<br><a href="https://www.taobao.com/robots.txt。" target="_blank" rel="noopener">https://www.taobao.com/robots.txt。</a><br>Allow开头的URL是允许robot访问的。例如，Allow:/article允许百度爬虫引擎访问/article.htm、/article/12345.com等。<br>Disallow不允许百度爬虫引擎访问的。例如，Disallow:/product/不允许百度爬虫引擎访问/product/12345.com等。<br>Disallow:/禁止百度爬虫访问除了Allow规定页面外的其他所有页面。<br>taobao的robots.txt对不同的搜索引擎所允许爬行范围不同。/product项对应淘宝内部的产品信息。当在搜索框中搜索“淘宝iphone7”的时候，Google可搜到淘宝中的产品，而百度不能。<br>过于快速或者频密的网络爬虫都会对服务器产生巨大的压力。→调集资源限制爬虫，保护用户的流量和减少有价值数据的流失。<br>反爬方维权：网站封锁你IP，法律行动。<br>将请求的速度限定在一个合理的范围之内。<br>每年的三月份会迎来一个爬虫高峰期。因为有大量的大学生五月份交论文，在写论文的时候会选择爬取数据，也就是3月份爬取数据，4月份分析数据，5月份交论文。<br>2007年，爱帮网利用垂直搜索技术获取了大众点评网上的商户简介和消费者点评，并且直接大量使用，大众点评网多次要求爱帮网停止使用这些内容，而爱帮网以自己是使用垂直搜索获得的数据为由，拒绝停止抓取大众点评网上的内容，并且质疑大众点评网对这些内容所享有的著作权。为此，双方开打了两场官司。2011年1月，北京海淀法院做出判决：爱帮网侵犯大众点评网著作权成立，应当停止侵权并赔偿大众点评网经济损失和诉讼必要支出。<br>2013年10月，百度诉360违反Robots协议。百度方面认为，360违反了Robots协议，擅自抓取、复制百度网站内容并生成快照向用户提供。2014年8月7日，北京市第一中级人民法院做出一审判决，法院认为被告奇虎360的行为违反了《反不正当竞争法》相关规定，应赔偿原告百度公司70万元。<br>虽然说大众点评上的点评数据、百度知道的问答由用户创建而非企业，但是搭建平台需要投入运营、技术和人力成本，所以平台拥有对数据的所有权、使用权和分发权。【网站的知识产权】<br>以上两起败诉告诉我们，在爬取网站的时候需要限制自己的爬虫，遵守Robots协议和约束网络爬虫程序的速度。如果违反了这些规定，很可能会吃官司，并且败诉的概率相当高。<h2 id="2-2-网页特点"><a href="#2-2-网页特点" class="headerlink" title="2.2. 网页特点"></a>2.2. 网页特点</h2>静态网页：纯粹HTML格式的网页通常被称为静态网页。其在浏览器中展示的内容都在HTML源代码中。早期的网站一般都是由静态网页制作的。容易爬。<br>动态网页：使用JavaScript、AJAX等技术展现的网页。很多内容并不会出现在HTML源代码中。主流网站一般都会使用。不容易爬。<br>如何获取网页</li>
<li>如何使用API获取数据【API是官方提供的数据获取通道，因此数据的获取是没有争议的。如果一个网站提供API获取数据，那么最好使用API获取，既简单又方便。】</li>
<li>如何安装Requests库？如何使用Requests库获取响应内容（整个网页的源代码）</li>
<li>基于深度和广度的爬虫。取深度加大到第3层，看看最短在多少秒之内能够完成前3层的爬虫。【深度优先的递归爬虫，广度优先的多线程爬虫】</li>
<li>静态网页是啥</li>
<li>动态网页是啥</li>
<li>动态网页的实例</li>
<li>什么是动态抓取？两种动态网页抓取技术获取动态网页的数据<ul>
<li>通过浏览器审查元素解析真实网页地址【AJAX动态解析地址】</li>
<li>使用selenium模拟浏览器的方法<h3 id="2-2-1-获取动态网页的真实地址"><a href="#2-2-1-获取动态网页的真实地址" class="headerlink" title="2.2.1. 获取动态网页的真实地址"></a>2.2.1. 获取动态网页的真实地址</h3>Chrome浏览器的检查（审查元素）功能：浏览器右键⇒检查⇒Network⇒XHR或JS选项<br>Network：显示浏览器从网页服务器中得到的所有文件。一般这些数据以json文件格式获取。<br>在Network选项卡下，找到真正的评论文件。<br>单击Preview标签即可查看数据。可以按 ctrl+F 进行查找。顶部search也可以。<br>Elements会出现相应的code所在的地方。<h2 id="2-3-抓包分析工具"><a href="#2-3-抓包分析工具" class="headerlink" title="2.3. 抓包分析工具"></a>2.3. 抓包分析工具</h2>Fiddler<br>什么是<br>与爬虫的关系<br>基本原理与基本界面<br>捕获会话功能<br>QuickExec命令行<br>断点功能<br>会话查找功能<br>的其他功能<h2 id="2-4-API"><a href="#2-4-API" class="headerlink" title="2.4. API"></a>2.4. API</h2>网站API分析<br>使用<br>秘钥获取<br>数据获取<h2 id="2-5-常用库"><a href="#2-5-常用库" class="headerlink" title="2.5. 常用库"></a>2.5. 常用库</h2>Urllib库</li>
</ul>
</li>
<li>什么是</li>
<li>快速使用</li>
<li>URLError异常处理<br>requests<br>BeautifulSoup4<br>CSVFeedSpider<br>强大的Requests库能够让你轻易地发送HTTP请求，这个库功能完善，而且操作非常简单<h2 id="2-6-requests"><a href="#2-6-requests" class="headerlink" title="2.6. requests"></a>2.6. requests</h2><a href="http://cn.python-requests.org/zh_CN/latest/user/quickstart.html" target="_blank" rel="noopener">http://cn.python-requests.org/zh_CN/latest/user/quickstart.html</a></li>
</ul>
<p>chardet</p>
<source lang="py"><br>import requests<br>import chardet<br>r = requests.get(‘<a href="http://www.baidu.com&#39;" target="_blank" rel="noopener">http://www.baidu.com&#39;</a>)<br>print(chardet.detect(r.content))<br>r.encoding = chardet.detect(r.content)[‘encoding’]<br>print(r.text)<br>

<source lang="py"><br>import requests<br><br># POST<br>postdata = {‘username’: ‘blog:qiyeboy’, ‘password’:1}<br>r = requests.post(‘<a href="http://www.xxxxxx.com/login&#39;" target="_blank" rel="noopener">http://www.xxxxxx.com/login&#39;</a>, data=postdata)<br><br># OTHERS<br>r = requests.put(‘<a href="http://www.xxxxxx.com/put&#39;" target="_blank" rel="noopener">http://www.xxxxxx.com/put&#39;</a>, data = {‘key’:’value’})<br>r = requests.delete(‘<a href="http://www.xxxxxx.com/delete&#39;" target="_blank" rel="noopener">http://www.xxxxxx.com/delete&#39;</a>)<br>r = requests.head(‘<a href="http://www.xxxxxx.com/get&#39;" target="_blank" rel="noopener">http://www.xxxxxx.com/get&#39;</a>)<br>r = requests.options(‘<a href="http://www.xxxxxx.com/get&#39;" target="_blank" rel="noopener">http://www.xxxxxx.com/get&#39;</a>)<br><br># 带参数的get【?&amp;】：网址后面紧跟着“? ”, “? ”后面还有参数<br>payload = {‘Keywords’: ‘blog:qiyeboy’, ‘pageindex’:1}<br>r = requests.get(‘<a href="http://zzk.cnblogs.com/s/blogpost&#39;" target="_blank" rel="noopener">http://zzk.cnblogs.com/s/blogpost&#39;</a>, params=payload)<br>print(r.url)<br><br># 编码<br>r.content #返回字节<br>r.text #返回文本<br>r.encoding #根据HTTP头猜测的网页编码格式<br>r.encoding=’utf-8’ # 自行设置编码格式<br>print(r.text)<br><br># 使用chardet  confidence 检测精度<br>import chardet<br>print(chardet.detect(r.content)) # {‘encoding’: ‘utf-8’, ‘confidence’: 0.99, ‘language’: ‘’}<br>r.encoding = chardet.detect(r.content)[‘encoding’]  #直接将chardet检测到的编码赋值给r.encoding后text就不会出现乱码<br><br># 流模式<br><br>顺序page for循环<br>urllib.request.urlretrive()<br>信息过滤<br>唯一特殊标识<br>

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">r = requests.get(<span class="string">'http://www.santostang.com/'</span>)</span><br><span class="line"><span class="comment"># ========【r的方法】========</span></span><br><span class="line"><span class="comment"># r response响应对象，存储了服务器响应的内容，以从中获取需要的信息</span></span><br><span class="line"><span class="comment"># r.encoding  服务器内容使用的文本编码。</span></span><br><span class="line"><span class="comment"># r.status_code 响应状态码。检测请求是否正确响应。</span></span><br><span class="line"><span class="comment"># r.text  字符串方式的响应体。会自动根据响应头部的字符编码进行解码。</span></span><br><span class="line"><span class="comment"># r.content 字节方式的响应体。会自动解码gzip和deflate编码的响应数据。gzip文件用这个。</span></span><br><span class="line"><span class="comment"># r.json()  Requests中内置的JSON解码器。</span></span><br><span class="line"><span class="comment"># r.url r对应的请求的页面网址</span></span><br><span class="line"><span class="comment"># ========【requests.get的参数设置】========</span></span><br><span class="line"><span class="comment">## URL参数、请求头、发送POST请求、设置超时</span></span><br><span class="line"><span class="comment">## ----------【params】：dict ----------</span></span><br><span class="line"><span class="comment">### get传递url参数。http://httpbin.org/get?key1=value1&amp;key2=value2</span></span><br><span class="line">key_dict = &#123;<span class="string">'key1'</span>: <span class="string">'value1'</span>, <span class="string">'key2'</span>: <span class="string">'value2'</span>&#125;</span><br><span class="line">r = requests.get(<span class="string">'http://httpbin.org/get'</span>, params=key_dict)</span><br><span class="line"><span class="comment">## ----------【headers】：dict ----------</span></span><br><span class="line"><span class="comment">### 有的网站不带请求头会返回错误的数据。带请求头使程序更像人的手动行为</span></span><br><span class="line">headers = &#123;</span><br><span class="line"><span class="string">'user-agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.82 Safari/537.36'</span>,</span><br><span class="line"><span class="string">'Host'</span>: <span class="string">'www.santostang.com'</span></span><br><span class="line">&#125;</span><br><span class="line">r = requests.get(<span class="string">'http://www.santostang.com/'</span>, headers=headers)</span><br><span class="line"><span class="comment">## ----------【data】: dict ----------</span></span><br><span class="line"><span class="comment">### 用于提交表单。data在发出请求的时候会自动编码为表单形式。</span></span><br><span class="line">key_dict = &#123;<span class="string">'key1'</span>: <span class="string">'value1'</span>, <span class="string">'key2'</span>: <span class="string">'value2'</span>&#125;</span><br><span class="line">r = requests.post(<span class="string">'http://httpbin.org/post'</span>, data=key_dict)</span><br><span class="line"><span class="comment">## ----------【timeout】: 单位为秒 ----------</span></span><br><span class="line"><span class="comment">### 如果服务器在timeout秒内没有应答，就返回异常。一般会把这个值设置为20秒。</span></span><br><span class="line">link = <span class="string">"http://www.santostang.com/"</span></span><br><span class="line">r = requests.get(link, timeout= <span class="number">0.001</span>)</span><br><span class="line"><span class="comment">## 返回的异常为：</span></span><br><span class="line"><span class="comment">## ConnectTimeout: HTTPConnectionPool(host='www.santostang.com', port=80): Max retries exceeded with url: / (Caused by ConnectTimeoutError(&lt;requests.packages.urllib3.connection.HTTPConnection object at 0x00000000077806D8&gt;, 'Connection to www.santostang.com timed out. (connect timeout=0.001)'))</span></span><br><span class="line"><span class="comment">## 异常值的意思是，时间限制在0.001秒内，连接到地址为www.santostang.com的时间已到。</span></span><br><span class="line"><span class="comment"># https://github.com/Santostang/PythonScraping/blob/master/第一版/Cha 3 -静态网页抓取/Cha 3 -静态网页抓取.ipynb</span></span><br></pre></td></tr></table></figure>
<h2 id="2-7-urllib"><a href="#2-7-urllib" class="headerlink" title="2.7. urllib"></a>2.7. urllib</h2><h3 id="2-7-1-核心代码"><a href="#2-7-1-核心代码" class="headerlink" title="2.7.1. 核心代码"></a>2.7.1. 核心代码</h3><p><code>urllib2.urlopen(&lt;&gt;).read()</code><br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 核心代码</span></span><br><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"><span class="comment">## 请求1</span></span><br><span class="line">response = urllib2.urlopen(&lt;&gt;)</span><br><span class="line"><span class="comment">## 请求2</span></span><br><span class="line">values = &#123;&#125;</span><br><span class="line">values[<span class="string">'username'</span>] = <span class="string">"XX"</span></span><br><span class="line">values[<span class="string">'password'</span>] = <span class="string">"XXXX"</span></span><br><span class="line"><span class="comment"># 此三行等价于  values = &#123;"username":"XX","password":"XXXX"&#125;</span></span><br><span class="line">data = urllib.urlencode(values)</span><br><span class="line">request = urllib2.Request(url,data)</span><br><span class="line">response = urllib2.urlopen(request)</span><br><span class="line"><span class="comment"># 读取</span></span><br><span class="line">text = response.read()</span><br></pre></td></tr></table></figure></p>
<h3 id="2-7-2-eg-1-直接GET"><a href="#2-7-2-eg-1-直接GET" class="headerlink" title="2.7.2. eg.1 直接GET"></a>2.7.2. eg.1 直接GET</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line">response = urllib2.urlopen(<span class="string">"http://www.baidu.com"</span>)</span><br><span class="line"><span class="keyword">print</span> response.read()</span><br></pre></td></tr></table></figure>
<h3 id="2-7-3-eg-2-带参数的GET"><a href="#2-7-3-eg-2-带参数的GET" class="headerlink" title="2.7.3. eg.2 带参数的GET"></a>2.7.3. eg.2 带参数的GET</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"> </span><br><span class="line">values=&#123;&#125;</span><br><span class="line">values[<span class="string">'username'</span>] = <span class="string">"XX"</span></span><br><span class="line">values[<span class="string">'password'</span>]=<span class="string">"XXXX"</span></span><br><span class="line">data = urllib.urlencode(values)</span><br><span class="line">url = <span class="string">"http://passport.csdn.net/account/login"</span></span><br><span class="line">geturl = url + <span class="string">"?"</span>+data</span><br><span class="line">request = urllib2.Request(geturl)</span><br><span class="line">response = urllib2.urlopen(request)</span><br><span class="line"><span class="keyword">print</span> response.read()</span><br></pre></td></tr></table></figure>
<h3 id="2-7-4-eg-3-POST"><a href="#2-7-4-eg-3-POST" class="headerlink" title="2.7.4. eg.3 POST"></a>2.7.4. eg.3 POST</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"> </span><br><span class="line">values = &#123;<span class="string">"username"</span>:<span class="string">"XX"</span>,<span class="string">"password"</span>:<span class="string">"XXXX"</span>&#125;</span><br><span class="line">data = urllib.urlencode(values)</span><br><span class="line">url = <span class="string">"&lt;post网址&gt;"</span></span><br><span class="line">request = urllib2.Request(url,data)</span><br><span class="line">response = urllib2.urlopen(request)</span><br><span class="line"><span class="keyword">print</span> response.read()</span><br></pre></td></tr></table></figure>
<h3 id="2-7-5-等效写法"><a href="#2-7-5-等效写法" class="headerlink" title="2.7.5. 等效写法"></a>2.7.5. 等效写法</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># response = urllib2.urlopen("http://www.baidu.com")</span></span><br><span class="line">request = urllib2.Request(<span class="string">"http://www.baidu.com"</span>)</span><br><span class="line">response = urllib2.urlopen(request)</span><br><span class="line"><span class="comment"># request对象，推荐写法，通过构建一个request，服务器响应请求得到应答，逻辑上清晰明确</span></span><br></pre></td></tr></table></figure>
<h3 id="2-7-6-类方法解析"><a href="#2-7-6-类方法解析" class="headerlink" title="2.7.6. 类方法解析"></a>2.7.6. 类方法解析</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">urlopen(url, data, timeout)</span><br><span class="line">url，必选。</span><br><span class="line">data，可选。默认为空None。访问URL时要传送的数据。</span><br><span class="line">timeout，可选。默认为 socket._GLOBAL_DEFAULT_TIMEOUT。设置超时时间。</span><br><span class="line">urlopen方法之后，返回一个response对象。</span><br></pre></td></tr></table></figure>
<h3 id="2-7-7-response"><a href="#2-7-7-response" class="headerlink" title="2.7.7. response"></a>2.7.7. response</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> response</span><br><span class="line">&lt;addinfourl at <span class="number">139728495260376</span> whose fp = &lt;socket._fileobject object at <span class="number">0x7f1513fb3ad0</span>&gt;</span><br></pre></td></tr></table></figure>
<h3 id="2-7-8-设置代理"><a href="#2-7-8-设置代理" class="headerlink" title="2.7.8. 设置代理"></a>2.7.8. 设置代理</h3><p><code>proxy_handler = urllib2.ProxyHandler({&quot;http&quot; : &#39;http://some-proxy.com:8080&#39;})</code><br>设置代理 ProxyHandler 参数为http字典<br><code>opener = urllib2.build_opener(proxy_handler)</code><br>打开代理 build_opener 参数为ProxyHandler<br><code>urllib2.install_opener(opener)</code><br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line">enable_proxy = <span class="keyword">True</span></span><br><span class="line">proxy_handler = urllib2.ProxyHandler(&#123;<span class="string">"http"</span> : <span class="string">'http://some-proxy.com:8080'</span>&#125;)</span><br><span class="line">proxy_handler_null = urllib2.ProxyHandler(&#123;&#125;)</span><br><span class="line"><span class="keyword">if</span> enable_proxy:</span><br><span class="line">    opener = urllib2.build_opener(proxy_handler)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    opener = urllib2.build_opener(proxy_handler_null)</span><br><span class="line">urllib2.install_opener(opener)</span><br></pre></td></tr></table></figure></p>
<p>urllib2 默认会使用环境变量 http_proxy 来设置 HTTP Proxy。<br>有些网站会检测某一时间段内某个IP 的访问次数，如果访问次数过多，网站会禁止访问。<br>这时可以设置一些代理服务器来工作，每隔一段时间换一个代理。 </p>
<h3 id="2-7-9-设置headers"><a href="#2-7-9-设置headers" class="headerlink" title="2.7.9. 设置headers"></a>2.7.9. 设置headers</h3><p><code>request = urllib2.Request(url, data, headers)</code><br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">headers = &#123; <span class="string">'User-Agent'</span> : <span class="string">'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'</span>  , </span><br><span class="line">            <span class="string">'Referer'</span>:<span class="string">'http://www.zhihu.com/articles'</span> &#125; </span><br><span class="line"><span class="comment"># 有些防盗链，服务器会识别headers中的referer是不是它自己，如果不是，服务器就不响应</span></span><br><span class="line">user_agent = <span class="string">'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'</span></span><br><span class="line">headers = &#123; <span class="string">'User-Agent'</span> : user_agent &#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="2-8-Selenium"><a href="#2-8-Selenium" class="headerlink" title="2.8. Selenium"></a>2.8. Selenium</h2><p>Selenium模拟浏览器<br>Selenium和PhantomJS<br>Selenium和PhantomJS的配合使用<br>Selenium选择元素的方法有很多。<br>xpath和css_selector是比较好的方法，一方面比较清晰，另一方面相对其他方法定位元素比较准确。<br><figure class="highlight xpath"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">查找单个元素：</span><br><span class="line">find_element_by_class_name：class选择</span><br><span class="line">    如&lt;p class=<span class="string">"content"</span>&gt;Site content goes here.&lt;/p&gt;⇒driver.find_element_by_class_name(<span class="string">'content'</span>)。</span><br><span class="line">find_element_by_css_selector：class选择</span><br><span class="line">    如&lt;div class=<span class="string">'bdy-inner'</span>&gt;test&lt;/div&gt;⇒driver.find_element_by_css_selector (<span class="string">'div.bdy-inner'</span>)。</span><br><span class="line">find_element_by_id：id选择</span><br><span class="line">    如&lt;div id=<span class="string">'bdy-inner'</span>&gt;test&lt;/div&gt;⇒driver.find_element_by_id(<span class="string">'bdy-inner'</span>)。</span><br><span class="line">find_element_by_link_text：链接地址选择</span><br><span class="line">    如&lt;a href=<span class="string">"continue.html"</span>&gt;Continue&lt;/a&gt;⇒driver.find_element_by_link_text(<span class="string">'Continue'</span>)。</span><br><span class="line">find_element_by_name：name选择</span><br><span class="line">    如&lt;input name=<span class="string">"username"</span>type=<span class="string">"text"</span> /&gt;⇒driver.find_element_by_name(<span class="string">'username'</span>)。</span><br><span class="line">find_element_by_partial_link_text：链接的部分地址选择</span><br><span class="line">    如 &lt;a href=<span class="string">"continue.html"</span>&gt;Continue&lt;/a&gt;⇒driver.find_element_by_partial_link_text(<span class="string">'Conti'</span>)。</span><br><span class="line">find_element_by_tag_name：名称选择</span><br><span class="line">    如&lt;h1&gt;Welcome&lt;/h1&gt;⇒driver.find_element_by_tag_name(<span class="string">'h1'</span>)。</span><br><span class="line">find_element_by_xpath：通过xpath选择</span><br><span class="line">    如&lt;form id=<span class="string">"loginForm"</span>&gt; ⇒driver.find_element_by_xpath(<span class="string">"//form[@id='loginForm']"</span>)。</span><br><span class="line">查找多个元素时，[<span class="literal">element</span>]后加上s：</span><br><span class="line">find_elements_by_class_name</span><br><span class="line">find_elements_by_css_selector</span><br><span class="line">find_elements_by_link_text</span><br><span class="line">find_elements_by_name</span><br><span class="line">find_elements_by_partial_link_text</span><br><span class="line">find_elements_by_tag_name</span><br><span class="line">find_elements_by_xpath</span><br><span class="line">除了Selenium的click操作元素方法，常见的操作元素方法：</span><br><span class="line">● Clear清除元素的内容。</span><br><span class="line">● send_keys模拟按键输入。</span><br><span class="line">● Click单击元素。</span><br><span class="line">● Submit提交表单。</span><br></pre></td></tr></table></figure></p>
<p>comment = driver.find_element_by_css_selector(‘div.bdy-inner’)<br>content = comment.find_element_by_tag_name(‘p’)<br>Selenium的高级操作：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">fp = webdriver.FirefoxProfile()</span><br><span class="line"><span class="comment"># 1. 限制CSS的页面</span></span><br><span class="line">fp.set_preference(<span class="string">"permissions.default.stylesheet"</span>,<span class="number">2</span>) </span><br><span class="line"><span class="comment"># 2. 限制图片的显示。极大地提高网络爬虫的效率。图片文件相对于文字、CSS、JavaScript等文件都比较大，加载需要较长时间。</span></span><br><span class="line">fp.set_preference(<span class="string">"permissions.default.image"</span>,<span class="number">2</span>) </span><br><span class="line"><span class="comment"># 3. 控制JavaScript的运行。大多数网页都会利用JavaScript异步加载很多内容，如果这些内容不是需要的，其加载会浪费时间。</span></span><br><span class="line">fp.set_preference(<span class="string">"javascript.enabled"</span>, <span class="keyword">False</span>)</span><br></pre></td></tr></table></figure></p>
<p>全部限制对于加载速度的提升效果最好。如果能够限制，那么最好限制多种加载，这样的效果最好。<br>具体的加载速度提升还得看相应的网页，若网页的图片比较多，则限制图片的加载肯定效果很好。<br>参考链接：<a href="https://github.com/Santostang/PythonScraping/blob/master/%E7%AC%AC%E4%B8%80%E7%89%88/Cha%204%20-%E5%8A%A8%E6%80%81%E7%BD%91%E9%A1%B5%E6%8A%93%E5%8F%96/Cha%204%20-%E5%8A%A8%E6%80%81%E7%BD%91%E9%A1%B5%E6%8A%93%E5%8F%96.ipynb" target="_blank" rel="noopener">selenium</a><br>Selenium官方文档：<a href="http://selenium-python.readthedocs.io/index.html。" target="_blank" rel="noopener">http://selenium-python.readthedocs.io/index.html。</a><br>Selenium要在整个网页加载出来后才开始爬取内容，速度往往较慢。<br>Selenium可以实现的功能：<br>操作元素对浏览器中的网页进行各种操作，包括登录。<br>模拟鼠标单击、双击、拖拽<br>获得网页中各个元素的大小<br>模拟键盘<br>浏览器渲染引擎。直接用浏览器在显示网页时解析HTML、应用CSS样式并执行JavaScript的语句。Selenium使用浏览器渲染，数据已经渲染到了HTML代码中。用chrome定位标签即可。<br>用脚本控制浏览器操作。Python的Selenium库模拟浏览器完成抓取。<br>Selenium：用于Web应用程序测试的工具。Selenium测试直接运行在浏览器中，浏览器自动按照脚本代码做出单击、输入、打开、验证等操作，就像真正的用户在操作一样。<br>用Selenium控制浏览器加载的内容，可加快Selenium的爬取速度。此类常用的方法有：<br>（1）控制CSS的加载。<br>（2）控制图片文件的显示。<br>（3）控制JavaScript的运行。<br>（1）控制CSS。因为抓取过程中仅仅抓取页面的内容，CSS样式文件是用来控制页面的外观和元素放置位置的，对内容并没有影响，所以我们可以限制网页加载CSS，从而减少抓取时间。<br>支持多个浏览器的调用：IE（7、8、9、10、11）、Firefox、Safari、Google Chrome、Opera等。最常用的是Firefox。</p>
<h2 id="2-9-爬虫与反爬"><a href="#2-9-爬虫与反爬" class="headerlink" title="2.9. 爬虫与反爬"></a>2.9. 爬虫与反爬</h2><p>==术语==<br>网络爬虫：使用任何技术手段自动批量获取网站信息的一种方式。<br>反爬虫：  使用任何技术手段阻止批量获取网站信息的一种方式。<br>出书的人和经验丰富的实战工程师区别大概就在于此。<br>与爬虫的斗争<br>问题<br>为什么会被<br>方式有哪些、技术突破、如何“反反爬虫”<br>反爬虫（反爬虫会增加获取数据的难度。限制封锁。）与反反爬（初级的反反爬虫方法只能初步帮助我们顺利地完成爬虫程序）</p>
<ul>
<li>如登录后才可以查看【处理登录表单：使用Python登录表单。Selenium爬取网站】</li>
<li>登录时设置验证码【处理验证码：通过程序识别图片中的文字（人工或者OCR）】</li>
<li>如何保存cookies</li>
<li><p>中文编码问题。为啥可以解决编码问题。为什么、怎么回事</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.getdefaultencoding</span><br><span class="line">sys.getdefaultencoding()</span><br><span class="line">s.encoding(<span class="string">'utf-8'</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>IP封锁与更换。【多服务器或Tor爬虫。如何让爬虫程序运行在“云”上，也能够让你随意改变自己的IP地址，进而走出爬虫被封IP的困境。】</p>
</li>
<li>如何解决Python中文乱码的问题【什么是字符编码，Python的字符编码是什么】<br><a href="https://blog.csdn.net/fei2636/article/details/78999318" target="_blank" rel="noopener">参考链接</a><br>就像攻击武器与防御武器一样，双方总是在不断升级。爬虫和反爬是典型的攻防双方的互相升级。但这种升级不像军事，军事是无尽头的，但是爬虫和反爬是有尽头的。<h3 id="2-9-1-爬虫的尽头"><a href="#2-9-1-爬虫的尽头" class="headerlink" title="2.9.1. 爬虫的尽头"></a>2.9.1. 爬虫的尽头</h3>就是浏览器，一旦使用浏览器，程序完全可以模拟真实用户发出请求，<br>消耗资源，因为需要新开一个进程，解析DOM，运行客户端JavaScript代码。（<a href="https://github.com/GoogleChrome/puppeteer" target="_blank" rel="noopener">chrome的node api</a>在github开源仅仅两天，就拿到8k个star）<h3 id="2-9-2-反爬的尽头"><a href="#2-9-2-反爬的尽头" class="headerlink" title="2.9.2. 反爬的尽头"></a>2.9.2. 反爬的尽头</h3>就是像Google这种超级厉害的验证码，毕竟验证码的根本目的就是识别人类和机器的。<h3 id="2-9-3-网站为什么要“反爬虫”"><a href="#2-9-3-网站为什么要“反爬虫”" class="headerlink" title="2.9.3. 网站为什么要“反爬虫”"></a>2.9.3. 网站为什么要“反爬虫”</h3>第一，网络爬虫浪费网站的流量，也就是浪费钱。爬虫对于一个网站来说并不算是真正用户的流量，而且往往能够不知疲倦地爬取网站，更有甚者，使用分布式的多台机器爬虫，造成网站浏览量增高，浪费网站流量。<br>第二，数据是每家公司非常宝贵的资源。在大数据时代，数据的价值越来越突出，很多公司都把它作为自己的战略资源。由于数据都是公开在互联网上的，如果竞争对手能够轻易获取数据，并使用这些数据采取针对性的策略，长此以往，就会导致公司竞争力的下降。<br>因此，有实力的大公司便开始利用技术进行反爬虫，如淘宝、京东、携程等。反爬虫是指使用任何技术手段阻止别人批量获取自己网站信息的一种方式。<br>再次特地声明，大家在获取数据时一定要有节制、有节操地爬虫。本书中的爬虫也仅用于学习、研究用途，请不要用于非法用途。任何由此引发的法律纠纷请自行负责。<h3 id="2-9-4-爬虫与反爬一览"><a href="#2-9-4-爬虫与反爬一览" class="headerlink" title="2.9.4. 爬虫与反爬一览"></a>2.9.4. 爬虫与反爬一览</h3>反爬|应对<br>–|–<br>频率限制|随机sleep<br>登陆限制|加上cookie<br>header|header池<br>JS|<a href="#JavaScript脚本动态获取网站数据">js反爬</a><br>验证码|机器学习<br>ip限制|代理池和高匿代理等好用的东西<br>内容反爬|OCR</li>
<li>反爬技术与反爬问题、网络异常  </li>
<li>负责爬虫技术攻坚，丰富爬虫反爬手段、反爬策略的设计及优化，快速解决  </li>
<li>设计爬虫策略和防屏蔽规则（常见的反爬手段及其应对措施=熟知当前各类反爬手段，对反爬机制有研究可破解（有能力解决复杂的反爬限制），有应对这些反爬手段的实际经验）  </li>
<li>登录爬取问题和验证码问题 解决办法和分析实例<ul>
<li>JavaScript反爬  </li>
<li>验证码识别技术、复杂图片验证码、滑动块识验码、账号限制、ip限制  </li>
<li>优化爬虫路由调度策略  </li>
<li>对网站的cookie时效性处理有经验  </li>
</ul>
</li>
<li>当在PC网页端爬取遇到困难时，爬取方式可以向手机网页端转变。<h3 id="2-9-5-常见的反爬措施"><a href="#2-9-5-常见的反爬措施" class="headerlink" title="2.9.5. 常见的反爬措施"></a>2.9.5. 常见的反爬措施</h3><h4 id="2-9-5-1-浏览器伪装技术"><a href="#2-9-5-1-浏览器伪装技术" class="headerlink" title="2.9.5.1. 浏览器伪装技术"></a>2.9.5.1. 浏览器伪装技术</h4>什么是<br>准备工作<br>Headers属性<br>登录<br>验证码<br>表单交互<br>需要登录的爬虫<br>注册、登录及创建项目<h4 id="2-9-5-2-Cookie的使用"><a href="#2-9-5-2-Cookie的使用" class="headerlink" title="2.9.5.2. Cookie的使用"></a>2.9.5.2. Cookie的使用</h4><h4 id="2-9-5-3-访问频率"><a href="#2-9-5-3-访问频率" class="headerlink" title="2.9.5.3. 访问频率"></a>2.9.5.3. 访问频率</h4>很好理解，如果访问太频繁网站可能针对你的ip封锁一段时间，这和防DDoS的原理一样。对于爬虫来说，碰到这样的限制一下任务的频率就可以了，可以尽量让爬虫想人类一样访问网页（比如随机sleep一段时间，如果每隔3s访问一次网站很显然不是正常人的行为）。<h4 id="2-9-5-4-登录限制"><a href="#2-9-5-4-登录限制" class="headerlink" title="2.9.5.4. 登录限制"></a>2.9.5.4. 登录限制</h4>也比较常见。不过公开信息的网站一般不会有这个限制，这样让用户也麻烦了。其实反爬措施都或多或少的影响真实用户，反爬越严格，误杀用户的可能性也越高。对爬虫来说，登录同样可以通过模拟登录的方式解决，加个cookie就行了（话又说回来，网络的原理很重要）。<h4 id="2-9-5-5-通过Header封杀"><a href="#2-9-5-5-通过Header封杀" class="headerlink" title="2.9.5.5. 通过Header封杀"></a>2.9.5.5. 通过Header封杀</h4>一般浏览器访问网站会有header，比如Safari或者Chrome等等，还有操作系统信息。如果使用程序访问并不会有这样的header。破解也很简单，访问的时候加上header就行。<h4 id="2-9-5-6-JavaScript脚本动态获取网站数据"><a href="#2-9-5-6-JavaScript脚本动态获取网站数据" class="headerlink" title="2.9.5.6. JavaScript脚本动态获取网站数据"></a>2.9.5.6. JavaScript脚本动态获取网站数据</h4>有一些网站（尤其是单页面网站）的内容并不是通过服务器直接返回的，而是服务器只返回一个客户端JavaScript程序，然后JavaScript获取内容。更高级的是，JavaScript在本地计算一个token，然后拿这个token来进行AJAX获取内容。而本地的JavaScript又是经过代码混淆和加密的，这样我们做爬虫的通过看源代码几乎不可能模拟出来这个请求（主要是token不可能破解），但是我们可以从另一个角度：headless的浏览器，也就是我们直接运行这个客户端程序，这可以100%地模拟真实用户！<h4 id="2-9-5-7-验证码"><a href="#2-9-5-7-验证码" class="headerlink" title="2.9.5.7. 验证码"></a>2.9.5.7. 验证码</h4>这几乎是终极武器了，验证码是专门用来区分人和计算机的手段。对于反爬方来说，这种方式对真实用户和搜索引擎（其实可以通过记录搜索引擎爬虫的ip来区别对待，可以解决）的危害比较大，相信读者都有输入验证码的痛苦经历。但这种方法也并不是无敌的！通过现在很火的机器学习可以轻松的识别大部分的验证码！Google的reCAPTCHA是一种非常高级的验证码，但是听过通过模拟浏览器也是可以破解的。有的网站需要验证码验证拿到一个token，token长得很像一个时间戳，本地自己生成一个时间戳发现也是能用的！于是就这样绕过了验证码。<h4 id="2-9-5-8-ip限制"><a href="#2-9-5-8-ip限制" class="headerlink" title="2.9.5.8. ip限制"></a>2.9.5.8. ip限制</h4>网站可能将识别的ip永久封杀，这种方式需要的人力比较大，而且误伤用户的代价也很高。但是破解办法却非常简单。目前代理池几乎是搞爬虫的标配了，甚至还有很多高匿代理等好用的东西。所以这基本上只能杀杀小爬虫。<h4 id="2-9-5-9-服务器采集"><a href="#2-9-5-9-服务器采集" class="headerlink" title="2.9.5.9. 服务器采集"></a>2.9.5.9. 服务器采集</h4>代理服务器的设置<br>为什么使用服务器采集<br>使用动态IP拨号服务器<br>使用Tor代理服务器<h4 id="2-9-5-10-网站内容反爬"><a href="#2-9-5-10-网站内容反爬" class="headerlink" title="2.9.5.10. 网站内容反爬"></a>2.9.5.10. 网站内容反爬</h4>有一些网站将网站内容用只有人类可以接收的形式来呈现（其实反爬就是区别对待人类和机器嘛）。比如将内容用图片的形式显示。但是近几年来人类和机器的差别越来越小，图片可以用OCR准确率非常高地去识别。<br>网络爬虫（又被称为网页蜘蛛，网络机器人，在FOAF社区中间，更经常的称为网页追逐者），是一种按照一定的规则，自动的抓取万维网信息的程序或者脚本。<br>爬虫，即网络爬虫，大家可以理解为在网络上爬行的一直蜘蛛，互联网就比作一张大网，而爬虫便是在这张网上爬来爬去的蜘蛛咯，如果它遇到资源，那么它就会抓取下来。想抓取什么？这个由你来控制它咯。<br>比如它在抓取一个网页，在这个网中他发现了一条道路，其实就是指向网页的超链接，那么它就可以爬到另一张网上来获取数据。这样，整个连在一起的大网对这之蜘蛛来说触手可及，分分钟爬下来不是事儿。<br>用户看到的网页实质是由 HTML 代码构成的，爬虫爬来的便是这些内容，通过分析和过滤这些 HTML 代码，实现对图片、文字等资源的获取。<br>爬虫爬取数据时必须要有一个目标的URL才可以获取数据，因此，它是爬虫获取数据的基本依据，准确理解它的含义对爬虫学习有很大帮助。<br>怎样扒网页呢？其实就是根据URL来获取它的网页信息，虽然我们在浏览器中看到的是一幅幅优美的画面，但是其实是由浏览器解释才呈现出来的，实质它是一段HTML代码，加 JS、CSS，如果把网页比作一个人，那么HTML便是他的骨架，JS便是他的肌肉，CSS便是它的衣服。所以最重要的部分是存在于HTML中的，下面我们就写个例子来扒一个网页下来。<h2 id="2-10-编码"><a href="#2-10-编码" class="headerlink" title="2.10. 编码"></a>2.10. 编码</h2>什么是字符编码<br>Python的字符编码</li>
<li>文本、数字、bit、字节：计算机只能处理数字，文本转换为数字才能处理，</li>
<li>字节：计算机中8个bit作为一个字节，一个字节能表示的最大数字就是255</li>
<li>ASCII: 计算机是美国人发明的，所以一个字节就可以标识所有单个字符，ASCII(一个字节)编码就成为美国人的标准编码</li>
<li>GB2312: 中文不止255个汉字，ASCII处理中文明显不够，所以中国制定了GB2312编码，用两个字节表示一个汉字。GB2312将ASCII也包含进去了。</li>
<li>Unicode：同理，日文，韩文，越来越多的国家为了解决这个问题就都发展了一套编码，标准越来越多，如果出现多种语言混合显示就一定会出现乱码</li>
<li><ul>
<li>于是unicode出现了，它将所有语言包含进去了。</li>
</ul>
</li>
<li>“utf-8：可变长的编码。英文：1字节，汉字3字节，特别生僻的变成4-6字节。<br>比较：</li>
<li>如果内容全是英文，unicode编码比ASCII编码需要多一倍的存储空间，传输也会变慢。</li>
<li>传输大量的英文，utf8作用就很明显。<br>示例：</li>
<li>ASCII和unicode编码:</li>
<li><ul>
<li>字母A：ASCII编码十进制是65，二进制 0100 0001，unicode：00000000 0100 0001（编码只需要在二进制前面补0）</li>
</ul>
</li>
<li><ul>
<li>汉字”中” 已近超出ASCII编码的范围，用unicode编码是20013二进制是01001110 00101101<br>解决：</li>
</ul>
</li>
<li>读取文件，进行操作时转换为unicode编码进行处理</li>
<li>保存文件时，转换为utf-8编码。以便于传输</li>
<li>读文件的库会转换为unicode<br>默认：</li>
<li>python2 默认编码格式为ASCII，</li>
<li>Python3 默认编码为 utf-8<br>ython2 默认编码格式为 ASCII，Python3 默认编码为 utf-8<h2 id="2-11-爬虫进阶"><a href="#2-11-爬虫进阶" class="headerlink" title="2.11. 爬虫进阶"></a>2.11. 爬虫进阶</h2>怎么处理图片验证码<br>反爬破解策略<br>消息队列<br>任务调度<br>代理的使用</li>
<li>用户代理池</li>
<li>IP代理池</li>
<li>同时使用用户代理池与IP代理池的方法<h2 id="2-12-异常处理"><a href="#2-12-异常处理" class="headerlink" title="2.12. 异常处理"></a>2.12. 异常处理</h2>为什么我抓到的和浏览器看到的不一样？怎样解决JavaScript渲染的问题<br>分析Ajax请求、Selenium/WebDriver、PyV8、Ghost.py 、Splash<h1 id="3-数据解析、清洗和组织"><a href="#3-数据解析、清洗和组织" class="headerlink" title="3. 数据解析、清洗和组织"></a>3. 数据解析、清洗和组织</h1>直接处理、正则表达式、XPath、Json解析、BeautifulSoup、PyQuery<br>难度：正则表达式&gt;BeautifulSoup、lxml（可能在寻找正则表达式上耗费时间、BeautifulSoup的find方法很容易学）<br>速度：BeautifulSoup≈lxml（BeautifulSoup已经支持lxml解析，因此速度和lxml差不多）<br>XMLFeedSpider<br>BeautifulSoup<br>lxml<br>JSON<br>高性能HTML内容解析<br>解析真实地址抓取<br>解析数据<br>如何解析网页上的数据。3种方法各有千秋，各自的优缺点</li>
<li>解析JSON数据</li>
<li>BeautifulSoup解析网页</li>
<li>正则表达式</li>
<li>BeautifulSoup（find方法）</li>
<li>XPath、lxml</li>
</ul>
<p>BeautifulSoup<br>Urllib中使用XPath表达式<br>PhantomJS 、浏览器伪装<br>超时设置</p>
<h2 id="3-1-bs4"><a href="#3-1-bs4" class="headerlink" title="3.1. bs4"></a>3.1. bs4</h2><p>使用BeautifulSoup解析网页<br>BeautifulSoup是一个工具箱。通过【解析文档】来提取数据。</p>
<ul>
<li>可以从HTML或XML文件中提取数据。</li>
<li>可以提供一些简单的、Python式的函数用来处理导航、搜索、修改分析树等。<br>简单，不需要多少代码就可以写出一个完整的应用程序。非常强大。<br>支持Python标准库中的HTML解析器，还支持一些第三方的解析器。<br>BeautifulSoup 4主要特性、适合做什么、怎样使用<br>使用BeautifulSoup获取博客标题<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">link = <span class="string">"http://www.santostang.com/"</span></span><br><span class="line">headers = &#123;<span class="string">'User-Agent'</span> : <span class="string">'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'</span>&#125; </span><br><span class="line">r = requests.get(link, headers= headers)</span><br><span class="line">soup = BeautifulSoup(r.text,<span class="string">"html.parser"</span>) <span class="comment"># 将网页响应体的字符串转化为soup对象</span></span><br><span class="line"><span class="comment"># &lt;h1&gt;元素，class为' post-title'，提取&lt;a&gt;元素中的文字，strip()的功能是把字符串左右的空格去掉。find只是用来找到第一条结果。</span></span><br><span class="line">first_title = soup.find(<span class="string">"h1"</span>, class_=<span class="string">"post-title"</span>).a.text.strip() </span><br><span class="line"><span class="keyword">print</span> (<span class="string">"第一篇文章的标题是："</span>, first_title)</span><br><span class="line">title_list = soup.find_all(<span class="string">"h1"</span>, class_=<span class="string">"post-title"</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(title_list)):</span><br><span class="line">    title = title_list[i].a.text.strip()</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'第 %s 篇文章的标题是：%s'</span> %(i+<span class="number">1</span>, title))</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>找所有结果，用find_all。find_all返回列表。<br>BeautifulSoup的其他功能<br>soup.prettify()  代码美化<br>首先，需要把：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">soup = BeautifulSoup(html, <span class="string">"html.parser"</span>)</span><br></pre></td></tr></table></figure></p>
<p>代码转化成BeautifulSoup对象。<br>BeautifulSoup对象是一个复杂的【树】形结构，它的每一个【节点】都是一个【Python对象】。<br>提取对象的3种方法：</p>
<blockquote>
<p>遍历文档树<br>搜索文档树<br>CSS选择器<br>1．遍历文档树<br>先爬树干，然后小树干，最后树枝。<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">soup.header.h3：获取取&lt;h3&gt;标签。如结果为：&lt;h3 id="name"&gt;大数据@唐松Santos&lt;/h3&gt;）。</span><br><span class="line">soup.header.div.contents：列出某个标签的所有子节点。只能获取第一代子标签。</span><br><span class="line">soup.header.div.contents[<span class="number">1</span>]：索引为<span class="number">1</span>的子标签。</span><br><span class="line">soup.header.div.children：获得所有子标签。只能获取第一代子标签。</span><br><span class="line">soup.header.div.descendants：获得所有子子孙孙标签</span><br><span class="line">soup.header.div.a.parent：获得父节点的内容：</span><br></pre></td></tr></table></figure></p>
</blockquote>
<p>遍历文档树的方法其实使用得比较少。<br>2．搜索文档树<br>最常用的是搜索文档树。<br>最常用的是find()和find_all()。<br>find()和find_all()方法还可以和re正则结合起来使用<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> tag <span class="keyword">in</span> soup.find_all(re.compile(<span class="string">"^h"</span>)):  <span class="comment"># 找出所有以h开头的标签，这表示&lt;header&gt;和&lt;h3&gt;的标签都会被找到</span></span><br><span class="line">    print(tag.name)</span><br><span class="line"><span class="comment"># 输出的结果是：</span></span><br><span class="line"><span class="comment"># header</span></span><br><span class="line"><span class="comment"># h3</span></span><br></pre></td></tr></table></figure></p>
<p>如果传入正则表达式作为参数，Beautiful Soup就会通过正则表达式的match()来匹配内容。</p>
<ol>
<li>CSS选择器<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">通过tag标签逐层查找：soup.select("header h3")⇒得到的结果是：[<span class="tag">&lt;<span class="name">h3</span> <span class="attr">id</span>=<span class="string">"name"</span>&gt;</span>大数据@唐松Santos<span class="tag">&lt;/<span class="name">h3</span>&gt;</span>]</span><br><span class="line">通过某个tag标签下的直接子标签遍历，：</span><br><span class="line">soup.select("header &gt; h3") ⇒[<span class="tag">&lt;<span class="name">h3</span> <span class="attr">id</span>=<span class="string">"name"</span>&gt;</span>大数据@唐松Santos<span class="tag">&lt;/<span class="name">h3</span>&gt;</span>]</span><br><span class="line">soup.select("div &gt; a") ⇒ <span class="tag">&lt;<span class="name">div</span>&gt;</span>下所有的<span class="tag">&lt; <span class="attr">a</span>&gt;</span>标签</span><br><span class="line">[<span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://www.santostang.com/feed/"</span> <span class="attr">rel</span>=<span class="string">"nofollow"</span> <span class="attr">target</span>=<span class="string">"_blank"</span><span class="attr">title</span>=<span class="string">"RSS"</span>&gt;</span><span class="tag">&lt;<span class="name">i</span>　<span class="attr">aria-hidden</span>=<span class="string">"true"</span>　<span class="attr">class</span>=<span class="string">"fa　fa-rss"</span>&gt;</span><span class="tag">&lt;/<span class="name">i</span>&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span>,　<span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://weibo.com/santostang"</span> <span class="attr">rel</span>=<span class="string">"nofollow"</span> <span class="attr">target</span>=<span class="string">"_blank"</span> <span class="attr">title</span>=<span class="string">"Weibo"</span>&gt;</span><span class="tag">&lt;<span class="name">i</span> <span class="attr">aria-hidden</span>=<span class="string">"true"</span> <span class="attr">class</span>=<span class="string">"fa fa-weibo"</span>&gt;</span><span class="tag">&lt;/<span class="name">i</span>&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span>, …]</span><br><span class="line">soup.select('a[href^="http://www.santostang.com/"]')：找所有链接以http://www.santostang.com/开始的<span class="tag">&lt;<span class="name">a</span>&gt;</span>标签</span><br><span class="line">得到的结果是：</span><br><span class="line">[<span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://www.santostang.com/feed/"</span> <span class="attr">rel</span>=<span class="string">"nofollow"</span> <span class="attr">target</span>=<span class="string">"_blank"</span><span class="attr">title</span>=<span class="string">"RSS"</span>&gt;</span><span class="tag">&lt;<span class="name">i</span> <span class="attr">aria-hidden</span>=<span class="string">"true"</span> <span class="attr">class</span>=<span class="string">"fa fa-rss"</span>&gt;</span><span class="tag">&lt;/<span class="name">i</span>&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span>,</span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://www.santostang.com/"</span>&gt;</span>首页<span class="tag">&lt;/<span class="name">a</span>&gt;</span>,</span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://www.santostang.com/about-me/"</span>&gt;</span>关于我<span class="tag">&lt;/<span class="name">a</span>&gt;</span>,</span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://www.santostang.com/post-search/"</span>&gt;</span>文章搜索<span class="tag">&lt;/<span class="name">a</span>&gt;</span>,</span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://www.santostang.com/wp-login.php"</span>&gt;</span>登录<span class="tag">&lt;/<span class="name">a</span>&gt;</span>]</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>5.3 使用lxml解析网页<br>一些比较流行的解析库<br>Xpath语法（如lxml），同样是效率比较高的解析方法。lxml使用C语言编写，解析速度比不使用lxml解析器的BeautifulSoup快一些。<br>5.3.2 使用lxml获取博客标题<br>使用lxml提取网页源代码数据的3种方法<br>    XPath选择器<br>    CSS选择器<br>    BeautifulSoup的find()方法<br>和BeautifulSoup相比，lxml还多了一种XPath选择器方法。<br>XPath是一门在XML文档中查找信息的语言。<br>XPath使用路径表达式来选取XML文档中的节点或节点集，也可以用在HTML获取数据中。<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">link = <span class="string">"http://www.santostang.com/"</span></span><br><span class="line">headers = &#123;<span class="string">'User-Agent'</span> : <span class="string">'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'</span>&#125; </span><br><span class="line">r = requests.get(link, headers= headers)</span><br><span class="line">html = etree.HTML(r.text) <span class="comment"># 解析为lxml的格式</span></span><br><span class="line">title_list = html.xpath(<span class="string">'//h1[@class="post-title"]/a/text()'</span>) <span class="comment"># 用XPath读取里面的内容</span></span><br><span class="line"><span class="keyword">print</span> (title_list)</span><br><span class="line">//：无论在文档中什么位置</span><br><span class="line">//h1：所有&lt;h1&gt;元素</span><br><span class="line">//h1[@class="post-title"]：&lt;h1&gt;中class为"post-title"的元素</span><br><span class="line">/a表示选取&lt;h1&gt;子元素的&lt;a&gt;元素</span><br><span class="line">/text()表示提取&lt;a&gt;元素中的所有文本。</span><br></pre></td></tr></table></figure></p>
<p>chrome审查，右键，选取元素，Copy→Copy XPath<br>5.3.3 XPath的选取方法<br>XPath使用路径表达式可以在网页源代码中选取节点，它是沿着路径来选取的，如表5-3所示。<br>XPath路径表达式及其描述 <a href="https://res.weread.qq.com/wrepub/epub_928559_47" target="_blank" rel="noopener">https://res.weread.qq.com/wrepub/epub_928559_47</a><br>下面是一个XML文档，我们将用XPath提取其中的一些数据。<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;? xml version=&quot;1.0&quot; encoding=&quot;ISO-8859-1&quot;? &gt;</span><br><span class="line">&lt;bookstore&gt;</span><br><span class="line">    &lt;book&gt;</span><br><span class="line">        &lt;title lang=&quot;en&quot;&gt;Harry Potter&lt;/title&gt;</span><br><span class="line">        &lt;author&gt;J K. Rowling&lt;/author&gt;</span><br><span class="line">        &lt;year&gt;2005&lt;/year&gt;</span><br><span class="line">        &lt;price&gt;29.99&lt;/price&gt;</span><br><span class="line">    &lt;/book&gt;</span><br><span class="line">&lt;/bookstore&gt;</span><br></pre></td></tr></table></figure></p>
<p>XPath的一些路径表达式及其结果：<a href="https://res.weread.qq.com/wrepub/epub_928559_48" target="_blank" rel="noopener">https://res.weread.qq.com/wrepub/epub_928559_48</a><br><a href="https://github.com/Santostang/PythonScraping/blob/master/%E7%AC%AC%E4%B8%80%E7%89%88/Cha%205%20-%E8%A7%A3%E6%9E%90%E7%BD%91%E9%A1%B5/Cha%205%20-%E8%A7%A3%E6%9E%90%E7%BD%91%E9%A1%B5.ipynb" target="_blank" rel="noopener">https://github.com/Santostang/PythonScraping/blob/master/%E7%AC%AC%E4%B8%80%E7%89%88/Cha%205%20-%E8%A7%A3%E6%9E%90%E7%BD%91%E9%A1%B5/Cha%205%20-%E8%A7%A3%E6%9E%90%E7%BD%91%E9%A1%B5.ipynb</a><br>5.5 BeautifulSoup爬虫实践：房屋价格数据<br>目的：获取安居客网站上北京二手房的数据。获取前10页二手房源的名称、价格、几房几厅、大小、建造年份、联系人、地址、标签。<br>网址：<a href="https://beijing.anjuke.com/sale/。" target="_blank" rel="noopener">https://beijing.anjuke.com/sale/。</a><br>5.5.1 网站分析<br>5.5.2 项目实践<br>通过以上分析已经能够获得各个数据所在的地址，接下来用requests加上BeautifulSoup获取安居客北京二手房结果的第一页数据，代码如下：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">headers = &#123;<span class="string">'User-Agent'</span> : <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.98 Safari/537.36'</span>&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">11</span>):</span><br><span class="line">    link = <span class="string">'https://beijing.anjuke.com/sale/p'</span> + str(i)</span><br><span class="line">    r = requests.get(link, headers = headers)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'现在爬取的是第'</span>, i, <span class="string">'页'</span>)</span><br><span class="line">    soup = BeautifulSoup(r.text, <span class="string">'lxml'</span>)</span><br><span class="line">    house_list = soup.find_all(<span class="string">'li'</span>, class_=<span class="string">"list-item"</span>)</span><br><span class="line">    <span class="keyword">for</span> house <span class="keyword">in</span> house_list:</span><br><span class="line">        name = house.find(<span class="string">'div'</span>, class_ =<span class="string">'house-title'</span>).a.text.strip()</span><br><span class="line">        price = house.find(<span class="string">'span'</span>, class_=<span class="string">'price-det'</span>).text.strip()</span><br><span class="line">        price_area = house.find(<span class="string">'span'</span>, class_=<span class="string">'unit-price'</span>).text.strip()</span><br><span class="line">        no_room = house.find(<span class="string">'div'</span>, class_=<span class="string">'details-item'</span>).span.text</span><br><span class="line">        area = house.find(<span class="string">'div'</span>, class_=<span class="string">'details-item'</span>).contents[<span class="number">3</span>].text</span><br><span class="line">        floor = house.find(<span class="string">'div'</span>, class_=<span class="string">'details-item'</span>).contents[<span class="number">5</span>].text</span><br><span class="line">        year = house.find(<span class="string">'div'</span>, class_=<span class="string">'details-item'</span>).contents[<span class="number">7</span>].text</span><br><span class="line">        broker = house.find(<span class="string">'span'</span>, class_=<span class="string">'brokername'</span>).text</span><br><span class="line">        broker = broker[<span class="number">1</span>:]</span><br><span class="line">        address = house.find(<span class="string">'span'</span>, class_=<span class="string">'comm-address'</span>).text.strip()</span><br><span class="line">        address = address.replace(<span class="string">'\xa0\xa0\n                    '</span>,<span class="string">'  '</span>)</span><br><span class="line">        tag_list = house.find_all(<span class="string">'span'</span>, class_=<span class="string">'item-tags'</span>)</span><br><span class="line">        tags = [i.text <span class="keyword">for</span> i <span class="keyword">in</span> tag_list] </span><br><span class="line">        <span class="keyword">print</span> (name, price, price_area, no_room, area, floor, year, broker, address, tags)</span><br><span class="line">    time.sleep(<span class="number">5</span>)</span><br></pre></td></tr></table></figure></p>
<p>进阶：获取其中的各项数据，如小区名称、房屋类型、房屋朝向、参考首付等。<br><a href="https://github.com/Santostang/PythonScraping/blob/master/%E7%AC%AC%E4%B8%80%E7%89%88/Cha%205%20-%E8%A7%A3%E6%9E%90%E7%BD%91%E9%A1%B5/Cha%205%20_%E7%AB%A0%E6%9C%AB%E5%AE%9E%E6%88%98.ipynb" target="_blank" rel="noopener">https://github.com/Santostang/PythonScraping/blob/master/%E7%AC%AC%E4%B8%80%E7%89%88/Cha%205%20-%E8%A7%A3%E6%9E%90%E7%BD%91%E9%A1%B5/Cha%205%20_%E7%AB%A0%E6%9C%AB%E5%AE%9E%E6%88%98.ipynb</a></p>
<h2 id="3-2-Xpath"><a href="#3-2-Xpath" class="headerlink" title="3.2. Xpath"></a>3.2. Xpath</h2><p>xpath语法<br>表达式|说明|<br>–|–|<br>article|选取所有article元素的所有子节点|<br>/article|选取根元素article|<br>article/a|选取所有属于article的子元素的a元素|<br>//div|选取所有div元素（不管出现在文档里的任何地方）|<br>article//div|选取所有属于article元素的后代的div元素，不管它出现在article之下的任何位置|<br>/div/<em>|选取属于div元素的所有子节点|<br>//</em>|选取所有元素|<br>//div[@*]|选取所有带属性的div 元素|<br>//@class|选取所有名为class的属性|<br>//div/a 丨//div/p|选取所有div元素的a和p元素|<br>//span丨//ul|选取文档中的span和ul元素|<br>article/div/p丨//span|选取所有属于article元素的div元素的p元素以及文档中所有的 span元素<br>xpath语法-谓语:<br>表达式|说明|<br>–|–|<br>/article/div[1]|选取属于article子元素的第一个div元素|<br>/article/div[last()]|选取属于article子元素的最后一个div元素|<br>/article/div[last()-1]|选取属于article子元素的倒数第二个div元素|<br>//div[@color]|选取所有拥有color属性的div元素|<br>//div[@color=’red’]|选取所有color属性值为red的div元素|<br>xpath的使用<br>会右键查看就能获取网页上任何内容</p>
<ol>
<li>xpath简介</li>
<li>xpath术语与语法</li>
<li>xpath抓取误区：javasrcipt生成html与html源文件的区别</li>
<li>xpath抓取实例<br>为什么要使用xpath？<br>xpath使用路径表达式在xml和html中进行导航<br>xpath包含有一个标准函数库<br>xpath是一个w3c的标准<br>xpath速度要远远超beautifulsoup。<br>xpath节点关系<br>父节点、上一层节点<br>子节点<br>兄弟节点、同胞节点<br>先辈节点、父节点、爷爷节点<br>后代节点、儿子、孙子<br>xpath抓取误区<br>firebugs插件<br>取某一个网页上元素的xpath地址<br>如:<a href="http://blog.jobbole.com/110287/" target="_blank" rel="noopener">http://blog.jobbole.com/110287/</a><br>在标题处右键使用firebugs查看元素。<br>然后在<h1>2016 腾讯软件开发面试题（部分）</h1>右键查看xpath<h2 id="3-3-解析器对比分析"><a href="#3-3-解析器对比分析" class="headerlink" title="3.3. 解析器对比分析"></a>3.3. 解析器对比分析</h2>主要的解析器及其优缺点<br><a href="https://res.weread.qq.com/wrepub/epub_928559_44" target="_blank" rel="noopener">https://res.weread.qq.com/wrepub/epub_928559_44</a><br><a href="https://res.weread.qq.com/wrepub/epub_928559_49" target="_blank" rel="noopener">https://res.weread.qq.com/wrepub/epub_928559_49</a><br>使用lxml的解析器将会解析得更快。<h1 id="4-数据存储"><a href="#4-数据存储" class="headerlink" title="4. 数据存储"></a>4. 数据存储</h1>数据存储<br>方式</li>
</ol>
<ul>
<li>TXT</li>
<li>CSV</li>
<li>Excel</li>
<li>JSON</li>
<li>MySQL</li>
<li>MongoDB及其优化建议<br>如何存储数据</li>
<li>存TXT。【写入和读取都非常方便，可以很快速地打开文件查看。用来存储测试用的数据。文件大时打开很慢。数据修改麻烦】</li>
<li>存CSV。【同上】</li>
<li>存MySQL。【数据量比较大、要与别人交换或别人也要访问时】</li>
<li>存MongoDB。【同上。JSON格式数据而不用进行解析】<br>文本——纯文本、Json、Xml等。<br>二进制文件——如图片、视频、⾳频等直接保存成特定格式即可。<br>关系型数据库——如MySQL、Oracle、SQL Server等具有结构化表结构形式存储。<br>非关系型数据库——如MongoDB、Redis等Key-Value形式存储。<h2 id="4-1-数据库"><a href="#4-1-数据库" class="headerlink" title="4.1. 数据库"></a>4.1. 数据库</h2>创建、连接和查询<br>数据导入<br>Python3操作<br>与标准Python客户端建立数据库接口<br>数据库设计  </li>
<li>oracle、Cassandra  </li>
<li>熟悉关系型（mysql/postgresql）、nosql（mongodb/hbase/elasticsearch/HBase/HIVE）、缓存sql（redis/memcached）  </li>
<li>调优和海量存储经验优先；能进行简单优化 有大数据开发经验优先 熟悉hadoop、spark、storm  </li>
<li>至少精通大数据量的一种关系型开发  </li>
<li>三种数据库的存储方式、SQLite、MySQL和MongoDB三种数据库的操作方式，实现爬取数据存储的多样化<br>MySQL<br>MongoDB<br>Redis<br>SQLite<br>Excel表格自动合并<br>‘’’⽹页文本’’’——如HTML文档、Json格式文本等。<br>‘’’图⽚’’’——获取到的是二进制文件，保存为图片格式。<br>‘’’视频’’’——同为二进制文件，保存为视频格式即可。<br>‘’’其他’’’——只要是能请求到的，都能获取。<h1 id="5-高性能爬取策略"><a href="#5-高性能爬取策略" class="headerlink" title="5. 高性能爬取策略"></a>5. 高性能爬取策略</h1>基础爬虫<br>简单分布式爬虫<br>Scrapy爬虫<br>Scrapy分布式爬虫<br>深度优先的递归<br>广度优先<br>定向爬取<br>爬虫的浏览器伪装技术<br>HTTP协议请求<br>DebugLog<br>异常处理神器——URLError<br>Cookiejar精析<br>并发和并行，同步和异步、异步加载技术与爬虫方法、多协程、多线程、多进程<br>如何提升爬虫的速度效率（速度实现成倍提升）。比较。<br>二叉树的遍历问题<br>深度优先(递归实现)<br>顺着一条路，走到最深处。然后回头。垂直方向<br>广度优先(队列实现)<br>分层遍历：遍历完儿子辈。然后遍历孙子辈。水平方向<br>深度优先算法</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">depth_tree</span><span class="params">(tree_node)</span>:</span></span><br><span class="line">   <span class="keyword">if</span> tree_node <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">       <span class="keyword">print</span> (tree_node._data)</span><br><span class="line">       <span class="keyword">if</span> tree_node._left <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">           <span class="keyword">return</span> depth_tree(tree_node.left)</span><br><span class="line">       <span class="keyword">if</span> tree_node._right <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">           <span class="keyword">return</span> depth_tree(tree_node._right)</span><br></pre></td></tr></table></figure>
<p>广度优先算法<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">level_queue</span><span class="params">(root)</span>:</span></span><br><span class="line">    <span class="comment">#利用队列实现树的广度优先遍历</span></span><br><span class="line">    <span class="keyword">if</span> root <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    my_queue = []</span><br><span class="line">    node = root</span><br><span class="line">    my_queue.append(node)</span><br><span class="line">    <span class="keyword">while</span> my_queue:</span><br><span class="line">        node = my_queue.pop(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">print</span> (node.elem)</span><br><span class="line">        <span class="keyword">if</span> node.lchild <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            my_queue.append(node.lchild)</span><br><span class="line">        <span class="keyword">if</span> node.rchild <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            my_queue.append(node.rchild)</span><br></pre></td></tr></table></figure></p>
<h2 id="5-1-性能"><a href="#5-1-性能" class="headerlink" title="5.1. 性能"></a>5.1. 性能</h2><p>标准性能模型<br>解决性能问题<br>系统性能<br>示例1：非常简单的管道<br>示例2：测量吞吐量和延时的扩展<br>超时设置<br>容错处理<br>定时爬取<br>自动化爬取的重要性<br>定向爬取的相关步骤与策略<br>动态渲染页面的爬取、JavaScript与AJAX数据爬取<br>简单的矩形区域抓取方式<br>高级区域抓取方式<br>创建自定义监控命令<br>使用ApacheSpark流计算偏移量</p>
<h1 id="6-Scrapy分布式爬虫"><a href="#6-Scrapy分布式爬虫" class="headerlink" title="6. Scrapy分布式爬虫"></a>6. Scrapy分布式爬虫</h1><ul>
<li>Scrapy框架</li>
<li>框架安装难点解决技巧</li>
<li><a href="https://mp.weixin.qq.com/s/hgYFE3Ga_j25BT9AB8jQuw" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/hgYFE3Ga_j25BT9AB8jQuw</a></li>
<li>Scrapy常见指令</li>
<li>更好的是爬取和存入相分离。Scrapy特色。</li>
</ul>
<p>好（较为成熟）的爬虫方案：多X程、分布式、将爬虫部署在服务器上把自己的个人计算机解放出来<br>如何通过Redis实现了一个分布式爬虫，让其在不同服务器之间通信。<br>分布式爬虫的好处是什么？</p>
<ul>
<li>队列的分配是依靠master的，当你获取数据的某一台slave奴隶服务器因为各种原因停止爬虫了，也不会让整个爬虫程序停下来。</li>
<li>分布式爬虫既可成倍提升爬虫效率，又可保证爬虫的稳定性。</li>
<li>（1）服务器之间有通信，每个服务器的待爬网页无需手动分配。</li>
<li>（2）数据集中存储到某一个服务器或数据库中。统一管理能够实现从不同服务器爬虫的队列管理到数据存储的优化。<br>Scrapy框架的安装<br>Scrapy框架基本使用<br>Scrapy命令行详解<br>Scrapy中选择器的用法<br>Scrapy中Spiders的用法<br>Scrapy中Item Pipeline的用法<br>Scrapy中Download Middleware的用法<br>Scrapy爬取知乎用户信息实战<br>Scrapy+Cookies池抓取新浪微博<br>Scrapy+Tushare爬取微博股票数据<br>命令行创建scrapy项目<br>cd desktop<br>scrapy startproject ArticleSpider<br>scrapy目录结构<br>scrapy借鉴了django的项目思想<br>scrapy.cfg：配置文件。<br>setings.py：设置<br>SPIDER_MODULES = [‘ArticleSpider.spiders’] #存放spider的路径<br>NEWSPIDER_MODULE = ‘ArticleSpider.spiders’<br>pipelines.py: 数据存储相关<br>middilewares.py: 自己定义的middlewares 定义方法，处理响应的IO操作<br>init.py:项目的初始化文件。<br>items.py：定义我们所要爬取的信息的相关属性。Item对象是种类似于表单，用来保存获取到的数据<br>创建spider<br>cd ArticleSpider<br>scrapy genspider jobbole blog.jobbole.com<br>自动生成<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">-*- coding: utf<span class="number">-8</span> -*-</span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JobboleSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"jobbole"</span></span><br><span class="line">    allowed_domains = [<span class="string">"blog.jobbole.com"</span>]</span><br><span class="line">    <span class="comment"># start_urls是一个带爬的列表，</span></span><br><span class="line">    <span class="comment">#spider会为我们把请求下载网页做到，直接到parse阶段</span></span><br><span class="line">    start_urls = [<span class="string">'http://blog.jobbole.com/'</span>]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>在命令行启动Spider<br>scrapy crawl jobbole<br>创建调试工具类<br>在项目根目录里创建main.py作为调试工具文件<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># _*_ coding: utf-8 _*_</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">__author__ = <span class="string">'mtianyan'</span></span><br><span class="line">__date__ = <span class="string">'2017/3/28 12:06'</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> scrapy.cmdline <span class="keyword">import</span> execute</span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 将系统当前目录设置为项目根目录</span></span><br><span class="line"><span class="comment"># os.path.abspath(__file__)为当前文件所在绝对路径</span></span><br><span class="line"><span class="comment"># os.path.dirname为文件所在目录</span></span><br><span class="line"><span class="comment"># H:\CodePath\spider\ArticleSpider\main.py</span></span><br><span class="line"><span class="comment"># H:\CodePath\spider\ArticleSpider</span></span><br><span class="line">sys.path.append(os.path.dirname(os.path.abspath(__file__)))</span><br><span class="line"><span class="comment"># 执行命令，相当于在控制台cmd输入改名了</span></span><br><span class="line">execute([<span class="string">"scrapy"</span>, <span class="string">"crawl"</span> , <span class="string">"jobbole"</span>])</span><br></pre></td></tr></table></figure></p>
<p>设置不遵守reboots协议<br>settings.py的<br>ROBOTSTXT_OBEY = False<br>⭕在jobble.py打上断点<br>def parse(self, response):<br>    pass<br>返回的htmlresponse对象:<br>    body:网页内容<br>    _DEFAULT_ENCODING= ‘ascii’<br>    encoding= ‘utf-8’<br>scrapy已经为我们做到了将网页下载下来。而且编码也进行了转换.<br>import scrapy<br>class JobboleSpider(scrapy.Spider):<br>    name = “jobbole”<br>    allowed_domains = [“blog.jobbole.com”]<br>    start_urls = [‘<a href="http://blog.jobbole.com/110287/&#39;]" target="_blank" rel="noopener">http://blog.jobbole.com/110287/&#39;]</a><br>    def parse(self, response):<br>        re_selector = response.xpath(“/html/body/div[3]/div[3]/div[1]/div[1]/h1”)</p>
<pre><code># print(re_selector)
pass
</code></pre><p>调试debug可以看到<br>re_selector =(selectorlist)[]<br>可以看到返回的是一个空列表，<br> 列表是为了如果我们当前的xpath路径下还有层级目录时可以进行选取<br> 空说明没取到值：</p>
<p> //*[@id=”post-110287”]/div[1]/h1<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JobboleSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"jobbole"</span></span><br><span class="line">    allowed_domains = [<span class="string">"blog.jobbole.com"</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://blog.jobbole.com/110287/'</span>]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        re_selector = response.xpath(<span class="string">'//*[@id="post-110287"]/div[1]/h1'</span>)</span><br><span class="line">        <span class="comment"># print(re_selector)</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure></p>
<h2 id="6-1-概述"><a href="#6-1-概述" class="headerlink" title="6.1. 概述"></a>6.1. 概述</h2><p>分布式系统概述<br>初识Scrapy Scrapy是一个Twisted应用<br>喜欢Scrapy的更多理由<br>Scrapy不是什么<br>框架介绍、架构概述、核心架构<br>目录结构<br>Scrapy设置<br>常用的Scrapy组件详解<br>Scrapy工作流<br>Scrapy引擎——一种直观方式<br>Scrapy高级应用<br>理解Scrapy性能</p>
<h2 id="6-2-部署到Scrapinghub"><a href="#6-2-部署到Scrapinghub" class="headerlink" title="6.2. 部署到Scrapinghub"></a>6.2. 部署到Scrapinghub</h2><p>搜索引擎核心<br>爬行策略<br>网页更新策略<br>身份识别<br>什么是Cookie<br>常用工具命令<br>Spider类参数传递<br>避免被禁止<br>UR2lM——基本抓取流程<br>抽取更多的URL<br>创建手机应用<br>访问item<br>配置与管理<br>基本设置<br>进阶设置<br>信号<br>管道秘诀<br>使用RESTAPl<br>使用Twisted专用客户端建立服务接口<br>为CPU密集型、阻塞或遗留功能建立接口<br>使用telnet获得组件利用率<br>基准系统<br>故障排除流程</p>
<h2 id="6-3-代码"><a href="#6-3-代码" class="headerlink" title="6.3. 代码"></a>6.3. 代码</h2><p>items的编写<br>pipelines的编写<br>settings的编写<br>Items的编写<br>Spider的编写</p>
<h2 id="6-4-使用Scrapy填充数据库"><a href="#6-4-使用Scrapy填充数据库" class="headerlink" title="6.4. 使用Scrapy填充数据库"></a>6.4. 使用Scrapy填充数据库</h2><p>Scrapy的中文输出<br>Scrapy的中文存储<br>Scrapy与MongoDB<br>Scrapy与Redis</p>
<h2 id="6-5-Scrapyd"><a href="#6-5-Scrapyd" class="headerlink" title="6.5. Scrapyd"></a>6.5. Scrapyd</h2><p>分布式爬虫与Scrapy<br>Scrapyd部署分布式爬虫<br>Scrapyd与实时分析进行分布式爬取</p>
<h2 id="6-6-Scrapy项目"><a href="#6-6-Scrapy项目" class="headerlink" title="6.6. Scrapy项目"></a>6.6. Scrapy项目</h2><p>用Scrapy进行爬虫项目管理<br>Scrapy爬虫多开技能<br>从Scrapy到移动应用<br>财经新闻数据<br>博客<br>Redis分布式爬虫实践<br>认识框架<br>框架安装难点解决技巧<br>常见指令<br>使用<br>You-get源码分析<br>与Urllib的整合<br>多线程<br>分布式<br><a href="https://github.com/LUCY78765580/Python-web-scraping" target="_blank" rel="noopener">https://github.com/LUCY78765580/Python-web-scraping</a><br><a href="https://github.com/qiyeboy/SpiderBook" target="_blank" rel="noopener">https://github.com/qiyeboy/SpiderBook</a><br>分布式爬虫的架构解析<br>分布式爬虫实现原理<br>分布式爬虫之Docker基础<br>分布式爬虫之Redis基础<br>分布式爬虫构建实战<br>Scrapy分布式原理及Scrapy-Redis源码解析<br>Scrapy分布式架构搭建抓取知乎<br>Scrapy分布式的部署详解<br><a href="http://cuiqingcai.com/3179.html" target="_blank" rel="noopener">小白爬虫第一弹之抓取妹子图</a><br><a href="http://cuiqingcai.com/3256.html" target="_blank" rel="noopener">小白爬虫第二弹之健壮的小爬虫</a><br><a href="http://cuiqingcai.com/3314.html" target="_blank" rel="noopener">小白爬虫第三弹之去重去重</a><br><a href="http://cuiqingcai.com/3363.html" target="_blank" rel="noopener">小白爬虫第四弹之爬虫快跑（多进程+多线程）</a><br><a href="http://cuiqingcai.com/3472.html" target="_blank" rel="noopener">小白进阶之Scrapy第一篇</a><br><a href="http://cuiqingcai.com/3952.html" target="_blank" rel="noopener">小白进阶之Scrapy第二篇（登录篇）</a><br><a href="http://cuiqingcai.com/4048.html" target="_blank" rel="noopener">小白进阶之</a><a href="http://cuiqingcai.com/4020.html" target="_blank" rel="noopener">Scrapy分布式的前篇–让redis和MongoDB安全点</a><br><a href="http://cuiqingcai.com/4048.html" target="_blank" rel="noopener">小白进阶之Scrapy第三篇（基于Scrapy-Redis的分布式以及cookies池）</a><br><a href="http://cuiqingcai.com/4421.html" target="_blank" rel="noopener">小白进阶之Scrapy第四篇（图片下载管道篇）</a><br><a href="http://cuiqingcai.com/4725.html" target="_blank" rel="noopener">小白进阶之Scrapy第五篇（Scrapy-Splash配合CrawlSpider；瞎几把整的）</a><br><a href="http://cuiqingcai.com/4652.html" target="_blank" rel="noopener">利用新接口抓取微信公众号的所有文章</a><br><a href="https://cuiqingcai.com/6058.html" target="_blank" rel="noopener">小白进阶之</a><a href="http://cuiqingcai.com/4725.html" target="_blank" rel="noopener">Scrapy第六篇</a><a href="https://cuiqingcai.com/6058.html" target="_blank" rel="noopener">Scrapy-Redis详解</a><br><a href="https://www.jianshu.com/p/cd4054bbc757" target="_blank" rel="noopener">https://www.jianshu.com/p/cd4054bbc757</a><br><a href="https://cloud.tencent.com/developer/article/1114535" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1114535</a><br>爬虫只运行在一台机器上时，受计算能力和网络带宽的影响，即使使用了异步和多线程技术，在待爬数据量较大时，需要耗费的时间会比较长，爬取效率也非常有限。<br>而多台主机协同爬取，效率会成倍增加。此即为分布式爬取，在网络中的多台计算机上同时运行爬虫程序，共同完成一个大型爬取任务。<br>Scrapy本身并不是一个为分布式爬取而设计的框架，但第三方库scrapy-redis为其拓展了分布式爬取的功能，两者结合便是一个分布式Scrapy爬虫框架。<br>在分布式爬虫框架中，需要使用某种通信机制协调各个爬虫的工作，让每一个爬虫明确自己的任务，其中包括：<br>（1）当前的爬取任务，即下载+提取数据（分配任务）。<br>（2）当前爬取任务是否已经被其他爬虫执行过（任务去重）。<br>（3）如何存储爬取到的数据（数据存储）。<br>scrapy-redis利用Redis数据库作为多个爬虫的数据共享实现以上功能。<br>网站的树结构(url分层设计)<br>网站url树结构分层设计:<br>bogbole.com<br>blog.bogbole.com<br>python.bogbole.com<br>python.bogbole.com/123<br>去重问题与策略<br>环路链接问题<br>从首页到下面节点。<br>但是下面的链接节点又会有链接指向首页。<br>有些已经爬过<br>将访问过的url保存到数据库中<br>将url保存到set中。只需要O(1)的代价就可以查询到url</p>
<blockquote>
<p>100000000<em>2byte</em>50个字符/1024/1024/1024 = 9G<br>url经过md5等方法哈希后保存到set中，将url压缩到固定长度而且不重复<br>用bitmap方法，将访问过的url通过hash函数映射到某一位<br>bloomfilter方法对bitmap进行改进，多重hash函数降低冲突（scrapy去重，分布式scrapy-redis）<br><a href="https://wenku.baidu.com/view/18e85d4117fc700abb68a98271fe910ef02dae47.html" target="_blank" rel="noopener">基于Hadoop的分布式网络爬虫系统的设计与实现</a></p>
</blockquote>
<h1 id="7-数据分析"><a href="#7-数据分析" class="headerlink" title="7. 数据分析"></a>7. 数据分析</h1><h2 id="7-1-可视化"><a href="#7-1-可视化" class="headerlink" title="7.1. 可视化"></a>7.1. 可视化</h2><p>地图<br>轨迹</p>
<h2 id="7-2-NumPy"><a href="#7-2-NumPy" class="headerlink" title="7.2. NumPy"></a>7.2. NumPy</h2><p>一维数组<br>多维数组<br>数组的运算</p>
<h2 id="7-3-pandas"><a href="#7-3-pandas" class="headerlink" title="7.3. pandas"></a>7.3. pandas</h2><p>数据清洗<br>数据分组、分割、合并和变形<br>缺失值、异常值和重复值处理<br>时序数据处理<br>数据类型转换</p>
<h1 id="8-框架"><a href="#8-框架" class="headerlink" title="8. 框架"></a>8. 框架</h1><p>爬虫架构设计<br>爬虫框架<br>什么是<br>常见的</p>
<ul>
<li>CrawlSpider与链接提取器、CrawlSpider实例</li>
<li>Crawley</li>
<li>Portia</li>
<li>newspaper</li>
<li>Python-goose</li>
<li>PySpider<br>手机应用框架<br>PySpider框架基本使用及抓取TripAdvisor实战<br>PySpider架构概述及用法详解<br>PyQuery详解<br>Selenium详解<h1 id="9-项目实施"><a href="#9-项目实施" class="headerlink" title="9. 项目实施"></a>9. 项目实施</h1>熟练掌握一种开源爬虫工具，有研发爬虫框架经验者优先；scrapy、webmagic、nutch、heritrix、Requests、Selenium、Appium、PhantomS  </li>
<li>著名爬虫框架Scrapy的运用、</li>
<li>通过Redis和Scrapy的结合实现分布式爬虫</li>
<li>整个的实现过程以及注意事项</li>
<li>PySpider（基本功能）<br>熟悉HttpClient、HtmlParser、Jsoup、Lucene、Nutch中的一种或多种开源技术<br>研究各种网页特点和规律<br>项目描述<br>功能分析<br>实现思路<br>网站分析<br>编写实战<br>调试与运行<br>项目实施<br>优化方案<br>代码优化<br>效率优化<br>网页分析算法<br>数据来源分析<br>工作流程<br>通过热力图分析为用户提供出行建议<br>从数据到产品<br>产品设计<br>产品交付<br>数据获取渠道<br>发现数据的价值<br>pyecharts业服务<br>从价值探索到交付落地<br>创新的不确定性<br>房产的标题是如何影响价格的<br>通过 2 个爬虫免试获得 2 个业界知名公司 offer</li>
</ul>
<ul>
<li>具备一个爬虫开发工程师需要的全部技能</li>
<li>理论性、经验性的内容为主，远远无法达到一个系统的标准</li>
<li>完整的爬虫开发需要学习的知识的体系</li>
<li>在过去的工作中完成过上百个抓取任务</li>
<li>参与和维护着使用 Celery、Twisted 等技术完成每天上亿次抓取量的抓取服务</li>
<li>3个写爬虫实现功能的视频</li>
<li>17年4月在知乎开过一场叫做 爬虫从入门到进阶 的知乎Live，目前已经有3.3k+人参加，评分4.9分(满分5分)<h2 id="9-1-框架搭建"><a href="#9-1-框架搭建" class="headerlink" title="9.1. 框架搭建"></a>9.1. 框架搭建</h2>架构设计、系统规划、建立、开发、研发、维护（完善）与管理、日常监控、优化、建模、调研<br>爬虫系统的架构设  </li>
</ul>
<ul>
<li>精通信息抓取抽取技术和整合技术、抽取算法保证抽取、去重、分类、解析、增量融合入库等流程之后的数据结果；内容提取  <ul>
<li>网页去重  、大规模爬取中的去重问题 、海量数据的去重方式以及各种去重方式的优劣比较</li>
</ul>
</li>
<li>网页信息调度、采集抓取提取、维护、验证、抓取规划、解析、清洗、入库以及汇总、清洗、整理、整合及合并、有数据分析能力。研发和优化工作  </li>
<li>分布式网络爬虫、高并发、高可用爬虫平台架构的设计和优化  </li>
<li>爬虫系统与数据分析系统数据接口设计和开发  </li>
<li>高性能爬虫系统的后台监控、报警模块的开发  </li>
<li>爬虫引擎核心功能  </li>
<li>能快速部署新的爬虫应用  <ul>
<li>多线程、多进程、协程相关知识及编程经验 、网路编程以及Web认证机制  </li>
<li>分布式、</li>
<li>抓取调度，多样化抓取，页面解析和结构化抽取，海量数据存储和读取、实时高并发海量数据爬取  </li>
</ul>
</li>
<li>核心算法的策略优化：充分利用资源，避免限制；  <ul>
<li>模板提高扩展性、效率和质量、持续优化系统提高系统的稳定性、监控抓取数据的完整性  </li>
<li>调权调度、分析预测、质量判断、封禁与反封禁研究  </li>
</ul>
</li>
<li>备份  <h2 id="9-2-扩展技能"><a href="#9-2-扩展技能" class="headerlink" title="9.2. 扩展技能"></a>9.2. 扩展技能</h2>熟悉搜索引擎、有搜索开发经验加分、优化搜索、匹配、抓取等关键程序的性能及效率<br>有网站开发经历加分、web挖掘能力<br>挖掘算法优化<br>数据分类及分布统计，文本分类、统计分析<br>特征挖掘。具有数据挖掘、自然语言处理、信息检索<br>个性化相关的机器学习算法、精通主流分词算法、分类、提取摘要、大规模网页聚类、索引等相关开发经验者优先。<br>有客户端及相关安全领域经验者优先  <h2 id="9-3-后起"><a href="#9-3-后起" class="headerlink" title="9.3. 后起"></a>9.3. 后起</h2>网页外挂机器人开发及维护<br>进行技术分享与培训<br>网页数据的自动化爬取脚本<br>数据爬取平台相关工具平台的架构设计与产品开发<br>参与数据层建设，专注于基础数据采集平台建设<br>爬取10w量级的数据<br>统计分析、可视化展示，发掘数据价值 【彩色直方图、折线图】<br>Django展示数据图<br>多平台信息的抓取和分析  <h2 id="9-4-加分项目"><a href="#9-4-加分项目" class="headerlink" title="9.4. 加分项目"></a>9.4. 加分项目</h2>垂直领域数据 社交、新闻媒体、论坛类、商业大数据、金融证券行业<br>网络数据源  </li>
</ul>
<ul>
<li>网站（页）：电商网站、定期爬取指定网站（如亚马逊、ebay平台）的数据、有大型B2C、C2C电商网站  </li>
<li>APP<br>1，项目的目标。【爬取知乎Live的所有实时语音分享以及知乎Live的听众。知乎Live的URL地址为<a href="https://www.zhihu.com/lives】" target="_blank" rel="noopener">https://www.zhihu.com/lives】</a><br>2.列举出各项目所采用的技术。<br>3.初始Url<br>4.项目步骤</li>
</ul>
<ul>
<li>一个简单的Python网络爬虫</li>
<li>维基百科<ul>
<li>维基百科是一个网络百科全书，在一般情况下允许用户编辑任何条目。当前维基百科由非营利组织维基媒体基金会负责营运。维基百科一词是由网站核心技术Wiki和具有百科全书之意的encyclopedia共同创造出来的新混合词Wikipedia。</li>
</ul>
</li>
<li>大众点评<ul>
<li><a href="http://www.dianping.com/search/category/7/10" target="_blank" rel="noopener">http://www.dianping.com/search/category/7/10</a></li>
<li>有很多商户的信息和用户点评数据。</li>
</ul>
</li>
<li>（1）通过大众点评的搜索结果获取餐厅的基本信息和地址。</li>
<li>（2）进入每家店铺的网页，获取大众点评餐厅的详细信息和评价。</li>
<li>百度地图API<ul>
<li><a href="http://lbsyun.baidu.com/index.php?title=webapi/guide/webservice-placeapi" target="_blank" rel="noopener">http://lbsyun.baidu.com/index.php?title=webapi/guide/webservice-placeapi</a></li>
<li>获取中国所有城市的公园数据，并且获取每一个公园具体的评分、描述等详情，最终将数据存储到MySQL数据库中。</li>
<li>拥有丰富的餐馆、房地产等数据，是一款网络地图搜索服务。可查地理位置，最近位置。街道、商场、楼盘、餐馆、学校、银行、公园等。百度地图提供了丰富的API供开发者调用，我们可以免费地获取各类地点的具体信息。</li>
<li>免费API：新浪微博、豆瓣电影、饿了么、豆瓣音乐、Facebook、Twitter。</li>
<li>收费API：百度API Store、聚合数据</li>
</ul>
</li>
<li>知乎<ul>
<li>知乎是中文互联网一个非常大的知识社交平台。在知乎上，用户可以通过问答等交流方式获取知识。区别于百度知道等问答网站，知乎的回答往往非常深入，都是回答者精心写的，知乎上聚集了中国互联网科技、商业、文化等领域里最具创造力的人群，将高质量的内容通过人的节点形成规模的生产和分享，构建高价值人际关系网络。</li>
</ul>
</li>
<li>爬取了博客文章评论<h1 id="10-实际爬虫"><a href="#10-实际爬虫" class="headerlink" title="10. 实际爬虫"></a>10. 实际爬虫</h1>Requests+正则表达式<br>分析Ajax请求<br>使用Selenium模拟浏览器<br>使用Redis+Flask<br>房产<br>基于位置信息的Ⅰ<br>猫眼电影<br>今日头条街拍美图<br>淘宝商品美食信息<br>维护动态代理池<br>维护动态Cookies池<br>使用代理处理反爬抓取微信文章<br>图片<br>链接<br>糗事百科<br>微信<br>博客类<br>图片类<br>模拟登录<br>抢票软件的实现<br>PEXELS图片<br>糗事百科网的用户地址信息<br>豆瓣音乐TOP250的数据<br>豆瓣电影TOP250的数据<br>豆瓣网图书TOP250的数据<br>维基百科<br>餐厅点评<br>知乎Live<br>百度地图API<br>畅销书籍<br>TOP250电影数据<br>房屋价格数据<br>虎扑论坛<br>酷狗TOP500的数据<br>知乎网Python精华话题<br>简书网热门专题信息<br>简书网专题收录文章<br>简书网推荐信息<br>简书网热评文章<br>简书网用户动态信息<br>简书网7日热门信息<br>拉勾网招聘信息<br>起点中文网小说信息<br>《斗破苍穹》全文小说<br>新浪微博好友圈信息<br>QQ空间好友说说<br>转转网二手市场商品信息<br>淘宝商品信息<br>北京地区短租房信息<br>糗事百科网的段子信息<br>电商网站(商品)数据——大型爬虫(Selenium)<br>BOSS直聘爬虫<br>Keep热门<br>果壳网自动登录<br>大麦网演出<br>小说网<br>博客<br>猜数游戏半自动爬虫开发<br>股票行情（用Scrapy）<br>深圳短租数据（Selenium）<br>用API爬取天气预报数据<br>知乎(asyncio)<br>新浪微博<br>京东<br>淘宝<br>出版社信息<br>京东图书<br>当当网<br>新闻<br>豆瓣网登陆爬虫与验证码自动识别<br>模拟登录<br>腾讯动漫（JS动态触发+id随机生成反爬破解实战）<br>微信爬虫<br>腾讯视频评论爬虫思路介绍<br>腾讯视频评论爬虫实战<br>糗事百科<br>抓包分析实战<br>tt商品图片爬虫实战<br>tt商品大型爬虫项目与自动写入数据库实战）<br>作业讲解：博文信息的爬取<br>腾讯微信和视频<br>百度信息自动搜索<br>自动POST<h3 id="10-0-1-简单的爬虫"><a href="#10-0-1-简单的爬虫" class="headerlink" title="10.0.1. 简单的爬虫"></a>10.0.1. 简单的爬虫</h3>【用到的库】requests + bs4<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://github.com/Santostang/PythonScraping/blob/master/第一版/Cha 2 - 编写你的第一个网络爬虫/Cha 2 _章末实战.ipynb</span></span><br><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup   <span class="comment">#从bs4这个库中导入BeautifulSoup</span></span><br><span class="line"><span class="comment"># 第一步：获取页面</span></span><br><span class="line">link = <span class="string">"http://www.santostang.com/"</span></span><br><span class="line">headers = &#123;<span class="string">'User-Agent'</span> : <span class="string">'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'</span>&#125;</span><br><span class="line">r = requests.get(link, headers= headers) <span class="comment"># requests的headers伪装成浏览器访问。r是requests的Response回复对象。</span></span><br><span class="line"><span class="comment"># 第二步：提取需要的数据</span></span><br><span class="line">soup = BeautifulSoup(r.text, <span class="string">"html.parser"</span>)      <span class="comment"># 使用BeautifulSoup解析这段网页。把HTML代码转化为soup对象。r.text是获取的网页内容代码</span></span><br><span class="line">title = soup.find(<span class="string">"h1"</span>, class_=<span class="string">"post-title"</span>).a.text.strip() <span class="comment"># 提取第一篇文章的标题</span></span><br><span class="line"><span class="keyword">print</span> (title)</span><br><span class="line"><span class="comment"># 第三步：存储数据</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'title_test.txt'</span>, <span class="string">"a+"</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(title)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="10-0-2-爬取豆瓣电影TOP250"><a href="#10-0-2-爬取豆瓣电影TOP250" class="headerlink" title="10.0.2. 爬取豆瓣电影TOP250"></a>10.0.2. 爬取豆瓣电影TOP250</h3><p>【用到的库】requests + bs4<br>获取豆瓣电影TOP250的所有电影的名称<br>网页地址为：<a href="https://movie.douban.com/top250" target="_blank" rel="noopener">https://movie.douban.com/top250</a><br>第一页有25个电影<br>获取所有的250页电影<br>总共10页的内容<br>第二页：<a href="https://movie.douban.com/top250" target="_blank" rel="noopener">https://movie.douban.com/top250</a>? start=25<br>第三页：<a href="https://movie.douban.com/top250" target="_blank" rel="noopener">https://movie.douban.com/top250</a>? start=50<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_movies</span><span class="params">()</span>:</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">    <span class="string">'user-agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.82 Safari/537.36'</span>,</span><br><span class="line">    <span class="string">'Host'</span>: <span class="string">'movie.douban.com'</span></span><br><span class="line">    &#125;</span><br><span class="line">    movie_list = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">10</span>):</span><br><span class="line">        link = <span class="string">'https://movie.douban.com/top250?start='</span> + str(i * <span class="number">25</span>)</span><br><span class="line">        r = requests.get(link, headers=headers, timeout= <span class="number">10</span>)</span><br><span class="line">        <span class="keyword">print</span> (str(i+<span class="number">1</span>),<span class="string">"页响应状态码:"</span>, r.status_code)</span><br><span class="line"> </span><br><span class="line">        soup = BeautifulSoup(r.text, <span class="string">"lxml"</span>)</span><br><span class="line">        div_list = soup.find_all(<span class="string">'div'</span>, class_=<span class="string">'hd'</span>)</span><br><span class="line">        <span class="keyword">for</span> each <span class="keyword">in</span> div_list:</span><br><span class="line">            movie = each.a.span.text.strip()</span><br><span class="line">            movie_list.append(movie)</span><br><span class="line">    <span class="keyword">return</span> movie_list</span><br><span class="line"> </span><br><span class="line">movies = get_movies()</span><br><span class="line"><span class="keyword">print</span> (movies)</span><br><span class="line"><span class="comment"># 原文有误</span></span><br><span class="line"><span class="comment"># 用 ]: 便于在 ipynb 中查找下一项</span></span><br></pre></td></tr></table></figure></p>
<p>参考链接：豆瓣电影(<a href="https://github.com/Santostang/PythonScraping/blob/master/第一版/Cha" target="_blank" rel="noopener">https://github.com/Santostang/PythonScraping/blob/master/第一版/Cha</a> 3 -静态网页抓取/Cha 3 _章末实战.ipy)<br>进阶问题：获取TOP 250电影的英文名、港台名、导演、主演、上映年份、电影分类以及评分。</p>
<h3 id="10-0-3-爬取动态网页"><a href="#10-0-3-爬取动态网页" class="headerlink" title="10.0.3. 爬取动态网页"></a>10.0.3. 爬取动态网页</h3><p>【用到的库】requests + json<br>AJAX加载的动态网页，有两种爬取方法：<br>（1）通过浏览器审查元素解析地址。<br>（2）通过Selenium模拟浏览器抓取。<br>两个特别重要的变量，即offset和limit。<br>limit：每一页评论数量的最大值<br>offset：本页的第一条评论是总的第几条<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">single_page_comment</span><span class="params">(link)</span>:</span></span><br><span class="line">    headers = &#123;<span class="string">'User-Agent'</span> : <span class="string">'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'</span>&#125;</span><br><span class="line">    r = requests.get(link, headers= headers)</span><br><span class="line">    <span class="comment"># 获取 json 的 string</span></span><br><span class="line">    json_string = r.text</span><br><span class="line">    json_string = json_string[json_string.find(<span class="string">'&#123;'</span>):<span class="number">-2</span>]</span><br><span class="line">    json_data = json.loads(json_string) <span class="comment"># 使用json.loads()把字符串格式的响应体数据转化为json数据</span></span><br><span class="line">    comment_list = json_data[<span class="string">'results'</span>][<span class="string">'parents'</span>] <span class="comment"># json数据的结构提取</span></span><br><span class="line">    <span class="keyword">for</span> eachone <span class="keyword">in</span> comment_list:</span><br><span class="line">        message = eachone[<span class="string">'content'</span>]</span><br><span class="line">        <span class="keyword">print</span> (message)</span><br><span class="line"><span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">4</span>):</span><br><span class="line">    link1 = <span class="string">"https://api-zero.livere.com/v1/comments/list?callback=jQuery112407875296433383039_1506267778283&amp;limit=10&amp;offset="</span></span><br><span class="line">    link2 = <span class="string">"&amp;repSeq=3871836&amp;requestPath=%2Fv1%2Fcomments%2Flist&amp;consumerSeq=1020&amp;livereSeq=28583&amp;smartloginSeq=5154&amp;_=1506267778285"</span></span><br><span class="line">    page_str = str(page)</span><br><span class="line">    link = link1 + page_str + link2</span><br><span class="line">    <span class="keyword">print</span> (link)</span><br><span class="line">    single_page_comment(link)</span><br></pre></td></tr></table></figure></p>
<p>参考链接：<br><a href="https://github.com/Santostang/PythonScraping/blob/master/第一版/Cha" target="_blank" rel="noopener">https://github.com/Santostang/PythonScraping/blob/master/第一版/Cha</a> 4 -动态网页抓取/Cha 4 -动态网页抓取.ipynb</p>
<h3 id="10-0-4-通过Selenium模拟浏览器抓取"><a href="#10-0-4-通过Selenium模拟浏览器抓取" class="headerlink" title="10.0.4. 通过Selenium模拟浏览器抓取"></a>10.0.4. 通过Selenium模拟浏览器抓取</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line">driver = webdriver.Firefox()</span><br><span class="line">driver.get(<span class="string">"https://www.dianping.com/search/category/7/10/p1"</span>)</span><br><span class="line"><span class="comment"># 如果运行之后，发现程序报错：</span></span><br><span class="line"><span class="comment">#     selenium.common.exceptions.WebDriverException: Message: 'geckodriver' executable needs to be in PATH.</span></span><br><span class="line"><span class="comment"># 可以到https://github.com/mozilla/geckodriver/releases下载最新版的geckodriver，解压后可以放在Python安装目录（可能是Script子文件夹）下（可能需并放在环境变量的PATH中）。</span></span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.firefox.firefox_binary <span class="keyword">import</span> FirefoxBinary </span><br><span class="line">caps = webdriver.DesiredCapabilities().FIREFOX</span><br><span class="line">caps[<span class="string">"marionette"</span>] = <span class="keyword">False</span></span><br><span class="line"> </span><br><span class="line">path =  <span class="string">r'D:\\Program Files\\Mozilla Firefox\\firefox.exe'</span></span><br><span class="line">binary = FirefoxBinary(path) <span class="comment"># Firefox程序的地址</span></span><br><span class="line">driver = webdriver.Firefox(firefox_binary=binary, capabilities=caps)</span><br><span class="line">driver.get(<span class="string">"http://www.santostang.com/2017/03/02/hello-world/"</span>)</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    load_more = driver.find_element_by_css_selector(<span class="string">'div.tie-load-more'</span>)   <span class="comment"># 更多或下一页</span></span><br><span class="line">    load_more.click()            <span class="comment"># 模拟单击</span></span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">comments = driver.find_elements_by_css_selector(<span class="string">'div.bdy- inner'</span>) </span><br><span class="line">time.sleep(<span class="number">5</span>)</span><br><span class="line">user = driver.find_element_by_name(<span class="string">"username"</span>)  <span class="comment">#找到用户名输入框</span></span><br><span class="line">user.clear  <span class="comment">#清除用户名输入框内容</span></span><br><span class="line">user.send_keys(<span class="string">"1234567"</span>)  <span class="comment">#在框中输入用户名</span></span><br><span class="line">pwd = driver.find_element_by_name(<span class="string">"password"</span>)  <span class="comment">#找到密码输入框</span></span><br><span class="line">pwd.clear  <span class="comment">#清除密码输入框内容</span></span><br><span class="line">pwd.send_keys(<span class="string">"＊＊＊＊＊＊"</span>)    <span class="comment">#在框中输入密码</span></span><br><span class="line">driver.find_element_by_id(<span class="string">"loginBtn"</span>).click()  <span class="comment">#单击登录</span></span><br></pre></td></tr></table></figure>
<h3 id="10-0-5-深圳短租"><a href="#10-0-5-深圳短租" class="headerlink" title="10.0.5. 深圳短租"></a>10.0.5. 深圳短租</h3><p>目的：获取Airbnb深圳前20页的短租房源的名称、价格、评价数量、房屋类型、床数量和房客数量。监控和了解竞争对手的房屋名称和价格，让自己的房子有竞争力。<br>网址：<a href="https://zh.airbnb.com/s/Shenzhen--China?page=1" target="_blank" rel="noopener">https://zh.airbnb.com/s/Shenzhen--China?page=1</a><br>4.4.1 网站分析<br>一个房子的所有数据。地址为：div.infoContainer_v72lrv。<br>价格数据，地址为：div.priceContainer_4ml1ll<br>评价数据，地址为：span.text_5mbkop-o_O-size_micro_16wifzf-o_O-inline_g86r3e<br>房屋名称数据，地址为：div.listingNameContainer_kq7ac0-o_O-ellipsized_1iurgbx<br>房间类型、床数量和房客数量，地址为：span.detailWithoutWrap_j1kt73<br>4.4.2 项目实践<br>用Selenium获取Airbnb第一页的数据。<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.firefox.firefox_binary <span class="keyword">import</span> FirefoxBinary</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">caps = webdriver.DesiredCapabilities().FIREFOX</span><br><span class="line">caps[<span class="string">"marionette"</span>] = <span class="keyword">True</span></span><br><span class="line">binary = FirefoxBinary(<span class="string">r'C:\Program Files\Firefox Developer Edition\firefox.exe'</span>)</span><br><span class="line"><span class="comment"># 把上述地址改成你电脑中Firefox程序的地址 </span></span><br><span class="line"><span class="comment"># 如果没改，会出现selenium.common.exceptions.SessionNotCreatedException: Message: Unable to find a matching set of capabilities</span></span><br><span class="line"><span class="comment">#用 selenium 的 driver 来启动 firefox</span></span><br><span class="line">driver = webdriver.Firefox(firefox_binary=binary, capabilities=caps)</span><br><span class="line"><span class="comment">#在虚拟浏览器中打开 Airbnb 页面。使用Selenium打开该页面</span></span><br><span class="line">driver.get(<span class="string">"https://zh.airbnb.com/s/Shenzhen--China?page=1"</span>)</span><br><span class="line">time.sleep(<span class="number">20</span>)</span><br><span class="line"><span class="comment">#找到页面中所有的出租房。用Selenium的css selector获取所有房屋的div数据</span></span><br><span class="line">rent_list = driver.find_elements_by_css_selector(<span class="string">'div._1788tsr0'</span>)</span><br><span class="line"><span class="comment">#对于每一个出租房</span></span><br><span class="line"><span class="keyword">for</span> eachhouse <span class="keyword">in</span> rent_list:</span><br><span class="line">    <span class="comment">#找到评论数量</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        comment = eachhouse.find_element_by_css_selector(<span class="string">'span._gb7fydm'</span>)</span><br><span class="line">        comment = comment.text</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        comment = <span class="number">0</span></span><br><span class="line">    <span class="comment">#找到价格</span></span><br><span class="line">    price = eachhouse.find_element_by_css_selector(<span class="string">'span._hylizj6'</span>)</span><br><span class="line">    price = price.text[<span class="number">4</span>:]</span><br><span class="line">    <span class="comment">#找到名称</span></span><br><span class="line">    name = eachhouse.find_element_by_css_selector(<span class="string">'div._ew0cqip'</span>)</span><br><span class="line">    name = name.text</span><br><span class="line">    <span class="comment">#找到房屋类型，大小</span></span><br><span class="line">    details = eachhouse.find_elements_by_css_selector(<span class="string">'div._saba1yg small div span'</span>)</span><br><span class="line">    details = details[<span class="number">0</span>].text</span><br><span class="line">    house_type = details.split(<span class="string">" · "</span>)[<span class="number">0</span>]</span><br><span class="line">    bed_number = details.split(<span class="string">" · "</span>)[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">print</span> (comment, price, name, house_type, bed_number)</span><br></pre></td></tr></table></figure></p>
<p>进阶：将Selenium的控制CSS加载、控制图片加载和控制JavaScript加载加入本实践项目的代码中，从而提升爬虫的速度。<br><a href="https://github.com/Santostang/PythonScraping/blob/master/%E7%AC%AC%E4%B8%80%E7%89%88/Cha%204%20-%E5%8A%A8%E6%80%81%E7%BD%91%E9%A1%B5%E6%8A%93%E5%8F%96/Cha%204%20_%E7%AB%A0%E6%9C%AB%E5%AE%9E%E6%88%98.ipynb" target="_blank" rel="noopener">https://github.com/Santostang/PythonScraping/blob/master/%E7%AC%AC%E4%B8%80%E7%89%88/Cha%204%20-%E5%8A%A8%E6%80%81%E7%BD%91%E9%A1%B5%E6%8A%93%E5%8F%96/Cha%204%20_%E7%AB%A0%E6%9C%AB%E5%AE%9E%E6%88%98.ipynb</a></p>
<h1 id="11-相关参考"><a href="#11-相关参考" class="headerlink" title="11. 相关参考"></a>11. 相关参考</h1><h2 id="11-1-allitebooks"><a href="#11-1-allitebooks" class="headerlink" title="11.1. allitebooks"></a>11.1. allitebooks</h2><p>Web Scraping with Python（有2版）<br>Website Scraping with Python    -<br>Learning Scrapy<br>Webbots, Spiders, and Screen Scrapers<br>Getting Started with Beautiful Soup</p>
<h2 id="11-2-其他"><a href="#11-2-其他" class="headerlink" title="11.2. 其他"></a>11.2. 其他</h2><p>Web Scraping with Python Collecting More Data from the Modern Web- 城通网盘  ||似乎与Ryan Mitchell的是同一本<br>Learning Selenium Testing Tools with Python<br>Python Requests Essentials<br>Web Scraping with Python, 2nd Edition<br>Practical Web Scraping for Data Science<br>Python Web Scraping Cookbook - 2018<br>Spider Webb’s Angels<br>Python Web Scraping<br>Python数据抓取技术与实战   仅京东 </p>
<h2 id="11-3-中文书籍"><a href="#11-3-中文书籍" class="headerlink" title="11.3. 中文书籍"></a>11.3. 中文书籍</h2><ul>
<li>Python网络爬虫从入门到实践，唐松</li>
</ul>
<ul>
<li>Github：<a href="https://github.com/Santostang/PythonScraping" target="_blank" rel="noopener">https://github.com/Santostang/PythonScraping</a><br>百度网：<a href="http://pan.baidu.com/s/1c2w9rck" target="_blank" rel="noopener">http://pan.baidu.com/s/1c2w9rck</a><br>书本对应的Python网络爬虫的教学:<a href="http://www.santostang.com" target="_blank" rel="noopener">www.santostang.com</a><br>网站不会更改设计和框架，本书的网络爬虫代码可以一直使用<br>作者自己的博客网站，可以避免一些法律上的风险</li>
</ul>
<ul>
<li><a href="https://github.com/qiyeboy/SpiderBook" target="_blank" rel="noopener">https://github.com/qiyeboy/SpiderBook</a> python爬虫开发与项目实战<h2 id="11-4-暂缺电子书"><a href="#11-4-暂缺电子书" class="headerlink" title="11.4. 暂缺电子书"></a>11.4. 暂缺电子书</h2>Spidering Hacks-<br>Python轻松学:爬虫、游戏与架站-<br>Python网络爬虫实例教程-<br>Python项目案例开发从入门到实战-<br>Python网络爬虫技术-<br>解密搜索引擎技术实战：Lucene &amp;Java精华版(第2版) : Lucene &amp; Java精华版-<br>Web Scraping with Python-Richard Lawson- <a href="https://book.douban.com/subject/26386962/" target="_blank" rel="noopener">https://book.douban.com/subject/26386962/</a><h2 id="11-5-网址"><a href="#11-5-网址" class="headerlink" title="11.5. 网址"></a>11.5. 网址</h2><a href="https://github.com/cjql/scrapy-redis/tree/master/src" target="_blank" rel="noopener">https://github.com/cjql/scrapy-redis/tree/master/src</a><h2 id="11-6-GitHub上的爬虫项目"><a href="#11-6-GitHub上的爬虫项目" class="headerlink" title="11.6. GitHub上的爬虫项目"></a>11.6. GitHub上的爬虫项目</h2><h3 id="11-6-1-爬虫框架"><a href="#11-6-1-爬虫框架" class="headerlink" title="11.6.1. 爬虫框架"></a>11.6.1. 爬虫框架</h3><a href="https://github.com/binux/pyspider" target="_blank" rel="noopener">https://github.com/binux/pyspider</a><br><a href="https://github.com/cuanboy/scrapyTest" target="_blank" rel="noopener">https://github.com/cuanboy/scrapyTest</a><br><a href="https://github.com/marchtea/scrapy_doc_chs" target="_blank" rel="noopener">https://github.com/marchtea/scrapy_doc_chs</a> scrapy中文翻译文档<h3 id="11-6-2-财经类"><a href="#11-6-2-财经类" class="headerlink" title="11.6.2. 财经类"></a>11.6.2. 财经类</h3><a href="https://github.com/cedricporter/funcat" target="_blank" rel="noopener">https://github.com/cedricporter/funcat</a> Funcat 将同花顺、通达信、文华财经麦语言等的公式写法移植到了 Python 中<br><a href="https://github.com/fmzquant/fmz_extend_api_demo" target="_blank" rel="noopener">https://github.com/fmzquant/fmz_extend_api_demo</a> 零成本快速打造你自己专属的多用户量化交易平台<br><a href="https://github.com/40robber/ScrapyDouban" target="_blank" rel="noopener">https://github.com/40robber/ScrapyDouban</a> 豆瓣电影/豆瓣读书 Scarpy 爬虫<br><a href="https://github.com/96chh/crawl-zsxq" target="_blank" rel="noopener">https://github.com/96chh/crawl-zsxq</a> 知识星球<br><a href="https://github.com/airingursb/bilibili-user" target="_blank" rel="noopener">https://github.com/airingursb/bilibili-user</a> bilibili<br><a href="https://github.com/airingursb/bilibili-video" target="_blank" rel="noopener">https://github.com/airingursb/bilibili-video</a><br><a href="https://github.com/bowenpay/wechat-spider" target="_blank" rel="noopener">https://github.com/bowenpay/wechat-spider</a> 微信公众号<br><a href="https://github.com/chenjiandongx/51job-spider" target="_blank" rel="noopener">https://github.com/chenjiandongx/51job-spider</a> 前程无忧 Python 招聘岗位信息爬取和分析 图文分析<br><a href="https://github.com/chenjiandongx/mzitu" target="_blank" rel="noopener">https://github.com/chenjiandongx/mzitu</a><br><a href="https://github.com/eqblog/91_porn_spider" target="_blank" rel="noopener">https://github.com/eqblog/91_porn_spider</a> 91porn爬虫<br><a href="https://github.com/GuozhuHe/webspider" target="_blank" rel="noopener">https://github.com/GuozhuHe/webspider</a> <strong>很漂亮</strong><br><a href="https://github.com/houyf/crawler" target="_blank" rel="noopener">https://github.com/houyf/crawler</a> 中大爬虫 基于scrapy爬虫框架<br><a href="https://github.com/lanbing510/DouBanSpider" target="_blank" rel="noopener">https://github.com/lanbing510/DouBanSpider</a> 豆瓣读书<br><a href="https://github.com/lanbing510/LianJiaSpider" target="_blank" rel="noopener">https://github.com/lanbing510/LianJiaSpider</a> 链家<br><a href="https://github.com/LiuRoy/zhihu_spider" target="_blank" rel="noopener">https://github.com/LiuRoy/zhihu_spider</a> 知乎<br><a href="https://github.com/LiuXingMing/SinaSpider" target="_blank" rel="noopener">https://github.com/LiuXingMing/SinaSpider</a> 新浪微博爬虫（Scrapy、Redis）<br><a href="https://github.com/MarvelousDick/WeiboSpiderSimple" target="_blank" rel="noopener">https://github.com/MarvelousDick/WeiboSpiderSimple</a> 微博爬虫简单<br><a href="https://github.com/Python3WebSpider/WeiboList" target="_blank" rel="noopener">https://github.com/Python3WebSpider/WeiboList</a> 微博<br><a href="https://github.com/samrayleung/jd_spider" target="_blank" rel="noopener">https://github.com/samrayleung/jd_spider</a> 京东爬虫<br><a href="https://github.com/starFalll/Spider" target="_blank" rel="noopener">https://github.com/starFalll/Spider</a> 新浪微博爬虫(Sina weibo spider)，百度搜索结果 爬虫<br><a href="https://github.com/TonyK-T/qichachaScrapy" target="_blank" rel="noopener">https://github.com/TonyK-T/qichachaScrapy</a> 企查查 网站爬取 scrapy<br><a href="https://github.com/wqh0109663/JobSpiders" target="_blank" rel="noopener">https://github.com/wqh0109663/JobSpiders</a> scrapy框架爬取51job(scrapy.Spider)，智联招聘(扒接口)，拉勾网(CrawlSpider)<br><a href="https://github.com/xjr7670/QQzone_crawler" target="_blank" rel="noopener">https://github.com/xjr7670/QQzone_crawler</a> QQ 空间动态爬虫，利用cookie登录获取所有可访问好友空间的动态保存到本地<br><a href="https://github.com/zkqiang/Zhihu-Login" target="_blank" rel="noopener">https://github.com/zkqiang/Zhihu-Login</a> 知乎模拟登录，支持验证码和保存 Cookies<br><a href="https://github.com/fst034356/crawler" target="_blank" rel="noopener">https://github.com/fst034356/crawler</a><br><a href="https://github.com/librauee/Reptile" target="_blank" rel="noopener">https://github.com/librauee/Reptile</a> 各类爬虫<br><a href="https://github.com/Nyloner/Nyspider" target="_blank" rel="noopener">https://github.com/Nyloner/Nyspider</a> 💗💗💗💗💗<br><a href="https://github.com/pythonsite/spider" target="_blank" rel="noopener">https://github.com/pythonsite/spider</a><br><a href="https://github.com/jhao104/proxy_pool" target="_blank" rel="noopener">https://github.com/jhao104/proxy_pool</a> Python爬虫代理IP池(proxy pool)<br><a href="https://github.com/Karmenzind/fp-server" target="_blank" rel="noopener">https://github.com/Karmenzind/fp-server</a> 免费本地代理<br><a href="https://github.com/Alfred1984/interesting-python/blob/master/ICU996/analysis/996.ipynb" target="_blank" rel="noopener">https://github.com/Alfred1984/interesting-python/blob/master/ICU996/analysis/996.ipynb</a><br><a href="https://nbviewer.jupyter.org/github/Alfred1984/interesting-python/blob/master/ICU996/analysis/996.ipynb" target="_blank" rel="noopener">https://nbviewer.jupyter.org/github/Alfred1984/interesting-python/blob/master/ICU996/analysis/996.ipynb</a><br><a href="https://chenjiabing666.github.io" target="_blank" rel="noopener">https://chenjiabing666.github.io</a><br><a href="http://github.com/Chyroc/WechatSogou" target="_blank" rel="noopener">http://github.com/Chyroc/WechatSogou</a> 微信公众号爬虫。基于搜狗微信搜索的微信公众号爬虫接口，可以扩展成基于搜狗搜索的爬虫，返回结果是列表，每一项均是公众号具体信息字典。<br><a href="http://github.com/lanbing510/DouBanSpider" target="_blank" rel="noopener">http://github.com/lanbing510/DouBanSpider</a> 豆瓣读书爬虫。可以爬下豆瓣读书标签下的所有图书，按评分排名依次存储，存储到Excel中，可方便大家筛选搜罗，比如筛选评价人数&gt;1000的高分书籍；可依据不同的主题存储到Excel不同的Sheet ，采用User Agent伪装为浏览器进行爬取，并加入随机延时来更好的模仿浏览器行为，避免爬虫被封。<br><a href="http://github.com/LiuRoy/zhihu_spider" target="_blank" rel="noopener">http://github.com/LiuRoy/zhihu_spider</a> 知乎爬虫。此项目的功能是爬取知乎用户信息以及人际拓扑关系，爬虫框架使用scrapy，数据存储使用mongo<br><a href="http://github.com/airingursb/bilibili-user" target="_blank" rel="noopener">http://github.com/airingursb/bilibili-user</a> Bilibili用户爬虫。总数据数：20119918，抓取字段：用户id，昵称，性别，头像，等级，经验值，粉丝数，生日，地址，注册时间，签名，等级与经验值等。抓取之后生成B站用户数据报告。<br><a href="http://github.com/LiuXingMing/SinaSpider" target="_blank" rel="noopener">http://github.com/LiuXingMing/SinaSpider</a> 新浪微博爬虫。主要爬取新浪微博用户的个人信息、微博信息、粉丝和关注。代码获取新浪微博Cookie进行登录，可通过多账号登录来防止新浪的反扒。主要使用 scrapy 爬虫框架。<br><a href="http://github.com/gnemoug/distribute_crawler" target="_blank" rel="noopener">http://github.com/gnemoug/distribute_crawler</a> 小说下载分布式爬虫。使用scrapy,Redis, MongoDB,graphite实现的一个分布式网络爬虫,底层存储MongoDB集群,分布式使用Redis实现,爬虫状态显示使用graphite实现，主要针对一个小说站点。<br><a href="http://github.com/yanzhou/CnkiSpider" target="_blank" rel="noopener">http://github.com/yanzhou/CnkiSpider</a> 中国知网爬虫。设置检索条件后，执行src/CnkiSpider.py抓取数据，抓取数据存储在/data目录下，每个数据文件的第一行为字段名称。<br><a href="http://github.com/lanbing510/LianJiaSpider" target="_blank" rel="noopener">http://github.com/lanbing510/LianJiaSpider</a> 链家网爬虫。爬取北京地区链家历年二手房成交记录。涵盖链家爬虫一文的全部代码，包括链家模拟登录代码。<br><a href="http://github.com/taizilongxu/scrapy_jingdong" target="_blank" rel="noopener">http://github.com/taizilongxu/scrapy_jingdong</a> 京东爬虫。基于scrapy的京东网站爬虫，保存格式为csv。<br><a href="http://github.com/caspartse/QQ-Groups-Spider" target="_blank" rel="noopener">http://github.com/caspartse/QQ-Groups-Spider</a> QQ 群爬虫。批量抓取 QQ 群信息，包括群名称、群号、群人数、群主、群简介等内容，最终生成 XLSX) / CSV 结果文件。<br><a href="http://github.com/hanc00l/wooyun_public" target="_blank" rel="noopener">http://github.com/hanc00l/wooyun_public</a> 乌云爬虫。 乌云公开漏洞、知识库爬虫和搜索。全部公开漏洞的列表和每个漏洞的文本内容存在MongoDB中，大概约2G内容；如果整站爬全部文本和图片作为离线查询，大概需要10G空间、2小时（10M电信带宽）；爬取全部知识库，总共约500M空间。漏洞搜索使用了Flask作为web server，bootstrap作为前端。<br><a href="http://github.com/simapple/spider" target="_blank" rel="noopener">http://github.com/simapple/spider</a> hao123网站爬虫。以hao123为入口页面，滚动爬取外链，收集网址，并记录网址上的内链和外链数目，记录title等信息，windows7 32位上测试，目前每24个小时，可收集数据为10万左右<br><a href="http://github.com/fankcoder/findtrip" target="_blank" rel="noopener">http://github.com/fankcoder/findtrip</a> 机票爬虫（去哪儿和携程网）。Findtrip是一个基于Scrapy的机票爬虫，目前整合了国内两大机票网站（去哪儿 + 携程）。<br><a href="http://github.com/leyle/163spider" target="_blank" rel="noopener">http://github.com/leyle/163spider</a> 基于requests、MySQLdb、torndb的网易客户端内容爬虫<br><a href="http://github.com/dontcontactme/doubanspiders" target="_blank" rel="noopener">http://github.com/dontcontactme/doubanspiders</a> 豆瓣电影、书籍、小组、相册、东西等爬虫集__<br><a href="http://github.com/LiuXingMing/QQSpider" target="_blank" rel="noopener">http://github.com/LiuXingMing/QQSpider</a> QQ空间爬虫，包括日志、说说、个人信息等，一天可抓取 400 万条数据。<br><a href="http://github.com/Shu-Ji/baidu-music-spider" target="_blank" rel="noopener">http://github.com/Shu-Ji/baidu-music-spider</a> 百度mp3全站爬虫，使用redis支持断点续传。<br><a href="http://github.com/pakoo/tbcrawler" target="_blank" rel="noopener">http://github.com/pakoo/tbcrawler</a> 淘宝和天猫的爬虫,可以根据搜索关键词,物品id来抓去页面的信息，数据存储在mongodb。<br><a href="http://github.com/benitoro/stockholm" target="_blank" rel="noopener">http://github.com/benitoro/stockholm</a> 一个股票数据（沪深）爬虫和选股策略测试框架。根据选定的日期范围抓取所有沪深两市股票的行情数据。支持使用表达式定义选股策略。支持多线程处理。保存数据到JSON文件、CSV文件。<br><a href="http://github.com/k1995/BaiduyunSpider" target="_blank" rel="noopener">http://github.com/k1995/BaiduyunSpider</a> 百度云盘爬虫。<br><a href="http://github.com/Qutan/Spider" target="_blank" rel="noopener">http://github.com/Qutan/Spider</a> 社交数据爬虫。支持微博,知乎,豆瓣。<br><a href="http://github.com/jhao104/proxy_pool" target="_blank" rel="noopener">http://github.com/jhao104/proxy_pool</a> Python爬虫代理IP池proxy pool)。<br><a href="http://github.com/RitterHou/music-163" target="_blank" rel="noopener">http://github.com/RitterHou/music-163</a> 爬取网易云音乐所有歌曲的评论。<br><a href="http://github.com/kulovecc/jandan_spider" target="_blank" rel="noopener">http://github.com/kulovecc/jandan_spider</a> 爬取煎蛋妹纸图片。<br><a href="http://github.com/jackgitgz/CnblogsSpider" target="_blank" rel="noopener">http://github.com/jackgitgz/CnblogsSpider</a> cnblogs列表页爬虫。<br><a href="http://github.com/qiyeboy/spider_smooc" target="_blank" rel="noopener">http://github.com/qiyeboy/spider_smooc</a> 爬取慕课网视频。<br><a href="http://github.com/yanzhou/CnkiSpider" target="_blank" rel="noopener">http://github.com/yanzhou/CnkiSpider</a> 中国知网爬虫。<br><a href="http://github.com/littlethunder/knowsecSpider2" target="_blank" rel="noopener">http://github.com/littlethunder/knowsecSpider2</a> 知道创宇爬虫题目。<br><a href="http://github.com/x-spiders/aiss-spider" target="_blank" rel="noopener">http://github.com/x-spiders/aiss-spider</a> 爱丝APP图片爬虫。<br><a href="http://github.com/szcf-weiya/SinaSpider" target="_blank" rel="noopener">http://github.com/szcf-weiya/SinaSpider</a> 动态IP解决新浪的反爬虫机制，快速抓取内容。<br><a href="http://github.com/Kevinsss/csdn-spider" target="_blank" rel="noopener">http://github.com/Kevinsss/csdn-spider</a> 爬取CSDN上的博客文章。<br><a href="http://github.com/changetjut/ProxySpider" target="_blank" rel="noopener">http://github.com/changetjut/ProxySpider</a> 爬取西刺上的代理IP，并验证代理可用性<br><a href="http://github.com/GuozhuHe/webspider" target="_blank" rel="noopener">http://github.com/GuozhuHe/webspider</a> 本系统是一个主要使用python3, celery和requests来爬取职位数据的爬虫，实现了定时任务，出错重试，日志记录，自动更改Cookies等的功能，并使用ECharts + Bootstrap 来构建前端页面，来展示爬取到的数据。<br><a href="https://github.com/princehaku/pyrailgun" target="_blank" rel="noopener">https://github.com/princehaku/pyrailgun</a><br>京东商品列表 <a href="https://mp.weixin.qq.com/s/G3wrHkhixRAOZjOtoHF3ww" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/G3wrHkhixRAOZjOtoHF3ww</a><br>java抓取京东图书 <a href="https://mp.weixin.qq.com/s/1poZ8oMFonBcC0stQ6V2YA" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/1poZ8oMFonBcC0stQ6V2YA</a><br>抓取京东商品数据 <a href="https://mp.weixin.qq.com/s/YKWwnRagUbGsDmCMoo-mZQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/YKWwnRagUbGsDmCMoo-mZQ</a><br>爬取京东商品列表 <a href="https://mp.weixin.qq.com/s/BeGKesA-gL901wA0EjViUw" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/BeGKesA-gL901wA0EjViUw</a><br>京东商城 前100页商品明细 <a href="https://mp.weixin.qq.com/s/h1cxAu9SpfpnfgH65rSa1A" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/h1cxAu9SpfpnfgH65rSa1A</a><br>京东的商品评价  <a href="https://mp.weixin.qq.com/s/8L_WwwAC6dEhCyF4xRSKqg" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/8L_WwwAC6dEhCyF4xRSKqg</a><br>利用Selenium自动登陆京东签到领金币 <a href="https://mp.weixin.qq.com/s/uzmCx1ZPDmpDKjqJdvm2xA" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/uzmCx1ZPDmpDKjqJdvm2xA</a><br>京东促销 <a href="https://mp.weixin.qq.com/s/OgJu2sMTWEr0wep865MhnQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/OgJu2sMTWEr0wep865MhnQ</a><br>京东登录 <a href="https://mp.weixin.qq.com/s/u4BXvG2oCVmfhSxc4_B-oA" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/u4BXvG2oCVmfhSxc4_B-oA</a><br>京东商品价格走势及图书评论数据  <a href="https://mp.weixin.qq.com/s/cOqIHgr3sLl4K9WbrZUFUQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/cOqIHgr3sLl4K9WbrZUFUQ</a><br>Python爬虫大战京东商城 <a href="https://mp.weixin.qq.com/s/sI2gyfD_QjIxgdEVwbn8GQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/sI2gyfD_QjIxgdEVwbn8GQ</a><br>如何打造类似数据虫巢官网系列教程之二：爬虫是怎么炼成的 <a href="https://mp.weixin.qq.com/s/k6f1l6kd7EWAO4ziAgK4Tg" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/k6f1l6kd7EWAO4ziAgK4Tg</a><br>京东的文胸 <a href="https://mp.weixin.qq.com/s/0OjQxBVXfCgDjtCtFPt0OQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/0OjQxBVXfCgDjtCtFPt0OQ</a><br>京东图书scrapy <a href="https://mp.weixin.qq.com/s/R2pZ_p3Avr-RyjKo6A5BtA" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/R2pZ_p3Avr-RyjKo6A5BtA</a><br><a href="https://www.zhihu.com/people/diqiuyo" target="_blank" rel="noopener">地球的外星人君</a><br><a href="https://e.jd.com/30392604.html" target="_blank" rel="noopener">https://e.jd.com/30392604.html</a><br><a href="https://dx.3.cn/desc/30392604?cdn=2&amp;callback=showdesc" target="_blank" rel="noopener">https://dx.3.cn/desc/30392604?cdn=2&amp;callback=showdesc</a><h2 id="11-7-公众号文章"><a href="#11-7-公众号文章" class="headerlink" title="11.7. 公众号文章"></a>11.7. 公众号文章</h2>[<a href="http://mp.weixin.qq.com/s?__biz=MzIzNzA4NDk3Nw==&amp;mid=2457735655&amp;idx=1&amp;sn=2bb4f6ab950951d6b88e2e85aff94cd9&amp;chksm=ff44b9b9c83330afcb0300b198c2da5170dd8d2fda7e16d44e47ff6ab979456a54a7c381eefd&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">http://mp.weixin.qq.com/s?__biz=MzIzNzA4NDk3Nw==&amp;mid=2457735655&amp;idx=1&amp;sn=2bb4f6ab950951d6b88e2e85aff94cd9&amp;chksm=ff44b9b9c83330afcb0300b198c2da5170dd8d2fda7e16d44e47ff6ab979456a54a7c381eefd&amp;scene=21#wechat_redirect</a> 中文分词原理及常用Python中文分词库介绍]<br>session和cookies<br>[<a href="http://mp.weixin.qq.com/s?__biz=MzIzNzA4NDk3Nw==&amp;mid=2457735603&amp;idx=1&amp;sn=2ac4107c695c7b45a58d8e66d4161a3f&amp;chksm=ff44b9edc83330fbb125b612cea0b06a4ea31031c90030a3f9da3529fc6fd4e8422a972e31f8&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">http://mp.weixin.qq.com/s?__biz=MzIzNzA4NDk3Nw==&amp;mid=2457735603&amp;idx=1&amp;sn=2ac4107c695c7b45a58d8e66d4161a3f&amp;chksm=ff44b9edc83330fbb125b612cea0b06a4ea31031c90030a3f9da3529fc6fd4e8422a972e31f8&amp;scene=21#wechat_redirect</a> Session和Cookies的基本原理]<br>环境<br>[<a href="http://mp.weixin.qq.com/s?__biz=MzIzNzA4NDk3Nw==&amp;mid=2457735532&amp;idx=1&amp;sn=759df7bda6de3e9a832197f142f10f62&amp;chksm=ff44b932c83330241d8d53e31fc75edb82c9c61855ad72f104fea59063ba7154830a5ddb559f&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">http://mp.weixin.qq.com/s?__biz=MzIzNzA4NDk3Nw==&amp;mid=2457735532&amp;idx=1&amp;sn=759df7bda6de3e9a832197f142f10f62&amp;chksm=ff44b932c83330241d8d53e31fc75edb82c9c61855ad72f104fea59063ba7154830a5ddb559f&amp;scene=21#wechat_redirect</a> Requests库作者Kenneth Reitz的另一神作！虚拟环境及包管理工具Pipenv！]<br>框架<br>[<a href="http://mp.weixin.qq.com/s?__biz=MzIzNzA4NDk3Nw==&amp;mid=2457735407&amp;idx=1&amp;sn=ba444a92d5d3af0dba317ac7da14c987&amp;chksm=ff44b8b1c83331a7f95d7248d4a3e4c7cb14898047e3ce1ae534eaf8684a0e806885d905e100&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">http://mp.weixin.qq.com/s?__biz=MzIzNzA4NDk3Nw==&amp;mid=2457735407&amp;idx=1&amp;sn=ba444a92d5d3af0dba317ac7da14c987&amp;chksm=ff44b8b1c83331a7f95d7248d4a3e4c7cb14898047e3ce1ae534eaf8684a0e806885d905e100&amp;scene=21#wechat_redirect</a> 跟繁琐的命令行说拜拜！Gerapy分布式爬虫管理框架来袭！]<br>代理<br>[<a href="http://mp.weixin.qq.com/s?__biz=MzIzNzA4NDk3Nw==&amp;mid=2457735511&amp;idx=1&amp;sn=150d7fbe05364c11caf4805256255108&amp;chksm=ff44b909c833301f3e97277f1ed5fb791920b0c2586eeaa5abb68e33f27a94fbfa5f3451cb70&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">http://mp.weixin.qq.com/s?__biz=MzIzNzA4NDk3Nw==&amp;mid=2457735511&amp;idx=1&amp;sn=150d7fbe05364c11caf4805256255108&amp;chksm=ff44b909c833301f3e97277f1ed5fb791920b0c2586eeaa5abb68e33f27a94fbfa5f3451cb70&amp;scene=21#wechat_redirect</a> 爬虫代理哪家强？十大付费代理详细对比评测出炉！]<br>软件<br>[<a href="http://mp.weixin.qq.com/s?__biz=MzIzNzA4NDk3Nw==&amp;mid=2457735774&amp;idx=1&amp;sn=a625cfa50ceb75e26f173fe4b2f29e9a&amp;chksm=ff44ba00c8333316917f4ce0619324af57c601f9a18d42ba65dcba15f1f8a76cb85bb6c6d2b8&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">http://mp.weixin.qq.com/s?__biz=MzIzNzA4NDk3Nw==&amp;mid=2457735774&amp;idx=1&amp;sn=a625cfa50ceb75e26f173fe4b2f29e9a&amp;chksm=ff44ba00c8333316917f4ce0619324af57c601f9a18d42ba65dcba15f1f8a76cb85bb6c6d2b8&amp;scene=21#wechat_redirect</a> 只会用Selenium爬网页？Appium爬App了解一下]<br>cookies<br>[<a href="http://mp.weixin.qq.com/s?__biz=MzIzNzA4NDk3Nw==&amp;mid=2457735749&amp;idx=1&amp;sn=4bb2566e541931b75941fa55e62c6881&amp;chksm=ff44ba1bc833330d70da3cc4f7f46f7745a990c97aacf5c869d736fc35ab3983794a38846781&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">http://mp.weixin.qq.com/s?__biz=MzIzNzA4NDk3Nw==&amp;mid=2457735749&amp;idx=1&amp;sn=4bb2566e541931b75941fa55e62c6881&amp;chksm=ff44ba1bc833330d70da3cc4f7f46f7745a990c97aacf5c869d736fc35ab3983794a38846781&amp;scene=21#wechat_redirect</a> 妈妈再也不用担心爬虫被封号了！手把手教你搭建Cookies池]<br>软件<br>[<a href="http://mp.weixin.qq.com/s?__biz=MzIzNzA4NDk3Nw==&amp;mid=2457735743&amp;idx=1&amp;sn=19ec53e0e0cc4d1b86aefa583d10b079&amp;chksm=ff44ba61c8333377a032ba78eb57ff463b1df9d6f7eee74905f89974ace3aac47141818de76a&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">http://mp.weixin.qq.com/s?__biz=MzIzNzA4NDk3Nw==&amp;mid=2457735743&amp;idx=1&amp;sn=19ec53e0e0cc4d1b86aefa583d10b079&amp;chksm=ff44ba61c8333377a032ba78eb57ff463b1df9d6f7eee74905f89974ace3aac47141818de76a&amp;scene=21#wechat_redirect</a> App爬虫神器mitmproxy和mitmdump的使用]<br>[<a href="http://mp.weixin.qq.com/s?__biz=Mzg4ODE2MDA4OQ==&amp;mid=2247483800&amp;idx=1&amp;sn=daada33afab8267413ad97011565c51b&amp;amp" target="_blank" rel="noopener">http://mp.weixin.qq.com/s?__biz=Mzg4ODE2MDA4OQ==&amp;mid=2247483800&amp;idx=1&amp;sn=daada33afab8267413ad97011565c51b&amp;amp</a> Scrapy爬取知乎——模拟登录]<br>[<a href="http://mp.weixin.qq.com/s?__biz=Mzg4ODE2MDA4OQ==&amp;mid=2247483798&amp;idx=1&amp;sn=49dbfbae0793c1edef02937251bec043&amp;amp" target="_blank" rel="noopener">http://mp.weixin.qq.com/s?__biz=Mzg4ODE2MDA4OQ==&amp;mid=2247483798&amp;idx=1&amp;sn=49dbfbae0793c1edef02937251bec043&amp;amp</a> 零基础Python修炼笔记]<br>[<a href="http://mp.weixin.qq.com/s?__biz=Mzg4ODE2MDA4OQ==&amp;mid=2247483796&amp;idx=1&amp;sn=c0a74adc1118c7ebf3c9fd50c725a272&amp;amp" target="_blank" rel="noopener">http://mp.weixin.qq.com/s?__biz=Mzg4ODE2MDA4OQ==&amp;mid=2247483796&amp;idx=1&amp;sn=c0a74adc1118c7ebf3c9fd50c725a272&amp;amp</a> 爬取美团网站信息（二）]<br>[<a href="http://mp.weixin.qq.com/s?__biz=Mzg4ODE2MDA4OQ==&amp;mid=2247483772&amp;idx=1&amp;sn=51ee4d2e0bab5e149044622ac57f9b48&amp;amp" target="_blank" rel="noopener">http://mp.weixin.qq.com/s?__biz=Mzg4ODE2MDA4OQ==&amp;mid=2247483772&amp;idx=1&amp;sn=51ee4d2e0bab5e149044622ac57f9b48&amp;amp</a> 爬取美团网站信息（一）]<br>[<a href="http://mp.weixin.qq.com/s?__biz=Mzg4ODE2MDA4OQ==&amp;mid=2247483751&amp;idx=1&amp;sn=065f4c907323f8ac210fb0c7b5cec5c0&amp;amp" target="_blank" rel="noopener">http://mp.weixin.qq.com/s?__biz=Mzg4ODE2MDA4OQ==&amp;mid=2247483751&amp;idx=1&amp;sn=065f4c907323f8ac210fb0c7b5cec5c0&amp;amp</a> 爬虫必须得会的预备知识]<br>[<a href="http://mp.weixin.qq.com/s?__biz=Mzg4ODE2MDA4OQ==&amp;mid=2247483731&amp;idx=1&amp;sn=67160d98bb4667f0529253c3cc95dd06&amp;amp" target="_blank" rel="noopener">http://mp.weixin.qq.com/s?__biz=Mzg4ODE2MDA4OQ==&amp;mid=2247483731&amp;idx=1&amp;sn=67160d98bb4667f0529253c3cc95dd06&amp;amp</a> 黑科技：用Python查看共同好友]<br>[<a href="http://mp.weixin.qq.com/s?__biz=Mzg4ODE2MDA4OQ==&amp;mid=2247483723&amp;idx=1&amp;sn=a549911b45bf9ec0e39012147f9cd5f8&amp;amp" target="_blank" rel="noopener">http://mp.weixin.qq.com/s?__biz=Mzg4ODE2MDA4OQ==&amp;mid=2247483723&amp;idx=1&amp;sn=a549911b45bf9ec0e39012147f9cd5f8&amp;amp</a> 牛逼了|用Python写一个微信提醒备忘录]<br>[<a href="http://mp.weixin.qq.com/s?__biz=Mzg4ODE2MDA4OQ==&amp;mid=2247483719&amp;idx=1&amp;sn=053511fdbf777d491e713b5406b6d9fc&amp;amp" target="_blank" rel="noopener">http://mp.weixin.qq.com/s?__biz=Mzg4ODE2MDA4OQ==&amp;mid=2247483719&amp;idx=1&amp;sn=053511fdbf777d491e713b5406b6d9fc&amp;amp</a> 使用Python开发你的第一个服务器程序]<br>[<a href="http://mp.weixin.qq.com/s?__biz=Mzg4ODE2MDA4OQ==&amp;mid=2247483717&amp;idx=1&amp;sn=60b43a34878e9f59017c4adfd9fa314c&amp;amp" target="_blank" rel="noopener">http://mp.weixin.qq.com/s?__biz=Mzg4ODE2MDA4OQ==&amp;mid=2247483717&amp;idx=1&amp;sn=60b43a34878e9f59017c4adfd9fa314c&amp;amp</a> 用Xpath,bs4,正则三种方式爬51job]<br>[<a href="http://mp.weixin.qq.com/s?__biz=Mzg4ODE2MDA4OQ==&amp;mid=2247483714&amp;idx=1&amp;sn=82c233b1196b88cc5eb0807f70e2311b&amp;amp" target="_blank" rel="noopener">http://mp.weixin.qq.com/s?__biz=Mzg4ODE2MDA4OQ==&amp;mid=2247483714&amp;idx=1&amp;sn=82c233b1196b88cc5eb0807f70e2311b&amp;amp</a> 用Python爬视频超级简单的!]<br>[<a href="http://mp.weixin.qq.com/s?__biz=Mzg4ODE2MDA4OQ==&amp;mid=2247483714&amp;idx=1&amp;sn=82c233b1196b88cc5eb0807f70e2311b&amp;amp" target="_blank" rel="noopener">http://mp.weixin.qq.com/s?__biz=Mzg4ODE2MDA4OQ==&amp;mid=2247483714&amp;idx=1&amp;sn=82c233b1196b88cc5eb0807f70e2311b&amp;amp</a> 用Python爬视频超级简单的!]<br>[<a href="http://mp.weixin.qq.com/s?__biz=Mzg4ODE2MDA4OQ==&amp;mid=2247483712&amp;idx=1&amp;sn=ad3325b848f8dcf368ed386143f8067b&amp;amp" target="_blank" rel="noopener">http://mp.weixin.qq.com/s?__biz=Mzg4ODE2MDA4OQ==&amp;mid=2247483712&amp;idx=1&amp;sn=ad3325b848f8dcf368ed386143f8067b&amp;amp</a> Python爬虫：动态爬取QQ说说并生成词云，分析朋友状况]<br>[<a href="http://mp.weixin.qq.com/s?__biz=Mzg4ODE2MDA4OQ==&amp;mid=2247483710&amp;idx=1&amp;sn=50f245908938bde95f6e56a8a2c7d1f7&amp;amp" target="_blank" rel="noopener">http://mp.weixin.qq.com/s?__biz=Mzg4ODE2MDA4OQ==&amp;mid=2247483710&amp;idx=1&amp;sn=50f245908938bde95f6e56a8a2c7d1f7&amp;amp</a> Python爬虫：学习Selenium并使用Selenium模拟登录知乎]<br>[<a href="http://mp.weixin.qq.com/s?__biz=Mzg4ODE2MDA4OQ==&amp;mid=2247483708&amp;idx=1&amp;sn=c2bc85de38212b7475083d5130ce74ef&amp;amp" target="_blank" rel="noopener">http://mp.weixin.qq.com/s?__biz=Mzg4ODE2MDA4OQ==&amp;mid=2247483708&amp;idx=1&amp;sn=c2bc85de38212b7475083d5130ce74ef&amp;amp</a> Python爬虫：把爬取到的数据插入到execl中]<br>[<a href="http://mp.weixin.qq.com/s?__biz=Mzg4ODE2MDA4OQ==&amp;mid=2247483679&amp;idx=1&amp;sn=7aef4e119c561c7c1f732c6064b3cab4&amp;amp" target="_blank" rel="noopener">http://mp.weixin.qq.com/s?__biz=Mzg4ODE2MDA4OQ==&amp;mid=2247483679&amp;idx=1&amp;sn=7aef4e119c561c7c1f732c6064b3cab4&amp;amp</a> Python爬取大量数据时，如何防止IP被封]<br>[<a href="http://mp.weixin.qq.com/s?__biz=Mzg4ODE2MDA4OQ==&amp;mid=2247483678&amp;idx=1&amp;sn=5f2142dc8b4da2484b218fac601294a2&amp;amp" target="_blank" rel="noopener">http://mp.weixin.qq.com/s?__biz=Mzg4ODE2MDA4OQ==&amp;mid=2247483678&amp;idx=1&amp;sn=5f2142dc8b4da2484b218fac601294a2&amp;amp</a> Python爬虫：现学现用xpath爬取豆瓣音乐]<br>[<a href="http://mp.weixin.qq.com/s?__biz=Mzg4ODE2MDA4OQ==&amp;mid=2247483671&amp;idx=1&amp;sn=9c88e3964b63aa6570010b2664b3fedd&amp;amp" target="_blank" rel="noopener">http://mp.weixin.qq.com/s?__biz=Mzg4ODE2MDA4OQ==&amp;mid=2247483671&amp;idx=1&amp;sn=9c88e3964b63aa6570010b2664b3fedd&amp;amp</a> Python爬虫准备：认识urllib/urllib2与requests]<br>[<a href="http://mp.weixin.qq.com/s?__biz=Mzg4ODE2MDA4OQ==&amp;mid=2247483667&amp;idx=1&amp;sn=2df35e159981fc2a38423b0810b9de09&amp;amp" target="_blank" rel="noopener">http://mp.weixin.qq.com/s?__biz=Mzg4ODE2MDA4OQ==&amp;mid=2247483667&amp;idx=1&amp;sn=2df35e159981fc2a38423b0810b9de09&amp;amp</a> Python 新手入门引导]<h2 id="11-8-崔庆才的博客"><a href="#11-8-崔庆才的博客" class="headerlink" title="11.8. 崔庆才的博客"></a>11.8. 崔庆才的博客</h2><h3 id="11-8-1-一、爬虫入门"><a href="#11-8-1-一、爬虫入门" class="headerlink" title="11.8.1. 一、爬虫入门"></a>11.8.1. 一、爬虫入门</h3></li>
</ul>
<ol>
<li><a href="https://cuiqingcai.com/927.html" target="_blank" rel="noopener">Python爬虫入门一之综述</a></li>
<li><a href="https://cuiqingcai.com/942.html" target="_blank" rel="noopener">Python爬虫入门二之爬虫基础了解</a></li>
<li><a href="https://cuiqingcai.com/947.html" target="_blank" rel="noopener">Python爬虫入门三之Urllib库的基本使用</a></li>
<li><a href="https://cuiqingcai.com/954.html" target="_blank" rel="noopener">Python爬虫入门四之Urllib库的高级用法</a></li>
<li><a href="https://cuiqingcai.com/961.html" target="_blank" rel="noopener">Python爬虫入门五之URLError异常处理</a></li>
<li><a href="https://cuiqingcai.com/968.html" target="_blank" rel="noopener">Python爬虫入门六之Cookie的使用</a></li>
<li><a href="https://cuiqingcai.com/977.html" target="_blank" rel="noopener">Python爬虫入门七之正则表达式</a><h3 id="11-8-2-二、爬虫实战"><a href="#11-8-2-二、爬虫实战" class="headerlink" title="11.8.2. 二、爬虫实战"></a>11.8.2. 二、爬虫实战</h3></li>
<li><a href="https://cuiqingcai.com/990.html" target="_blank" rel="noopener">Python爬虫实战一之爬取糗事百科段子</a></li>
<li><a href="https://cuiqingcai.com/993.html" target="_blank" rel="noopener">Python爬虫实战二之爬取百度贴吧帖子</a></li>
<li><a href="https://cuiqingcai.com/2083.html" target="_blank" rel="noopener">Python爬虫实战三之实现山东大学无线网络掉线自动重连</a></li>
<li><a href="https://cuiqingcai.com/1001.html" target="_blank" rel="noopener">Python爬虫实战四之抓取淘宝MM照片</a></li>
<li><a href="https://cuiqingcai.com/1076.html" target="_blank" rel="noopener">Python爬虫实战五之模拟登录淘宝并获取所有订单</a></li>
<li><a href="https://cuiqingcai.com/1972.html" target="_blank" rel="noopener">Python爬虫实战六之抓取爱问知识人问题并保存至数据库</a></li>
<li><a href="https://cuiqingcai.com/997.html" target="_blank" rel="noopener">Python爬虫实战七之计算大学本学期绩点</a></li>
<li><a href="https://cuiqingcai.com/2852.html" target="_blank" rel="noopener">Python爬虫实战八之利用Selenium抓取淘宝匿名旺旺</a><h3 id="11-8-3-三、爬虫利器"><a href="#11-8-3-三、爬虫利器" class="headerlink" title="11.8.3. 三、爬虫利器"></a>11.8.3. 三、爬虫利器</h3></li>
<li><a href="https://cuiqingcai.com/2556.html" target="_blank" rel="noopener">Python爬虫利器一之Requests库的用法</a></li>
<li><a href="https://cuiqingcai.com/1319.html" target="_blank" rel="noopener">Python爬虫利器二之Beautiful Soup的用法</a></li>
<li><a href="https://cuiqingcai.com/2621.html" target="_blank" rel="noopener">Python爬虫利器三之Xpath语法与lxml库的用法</a></li>
<li><a href="https://cuiqingcai.com/2577.html" target="_blank" rel="noopener">Python爬虫利器四之PhantomJS的用法</a></li>
<li><a href="https://cuiqingcai.com/2599.html" target="_blank" rel="noopener">Python爬虫利器五之Selenium的用法</a></li>
<li><a href="https://cuiqingcai.com/2636.html" target="_blank" rel="noopener">Python爬虫利器六之PyQuery的用法</a><h3 id="11-8-4-四、爬虫进阶"><a href="#11-8-4-四、爬虫进阶" class="headerlink" title="11.8.4. 四、爬虫进阶"></a>11.8.4. 四、爬虫进阶</h3></li>
<li><a href="https://cuiqingcai.com/2433.html" target="_blank" rel="noopener">Python爬虫进阶一之爬虫框架概述</a></li>
<li><a href="https://cuiqingcai.com/2443.html" target="_blank" rel="noopener">Python爬虫进阶二之PySpider框架安装配置</a></li>
<li><a href="https://cuiqingcai.com/912.html" target="_blank" rel="noopener">Python爬虫进阶三之爬虫框架Scrapy安装配置</a></li>
<li><a href="https://cuiqingcai.com/2652.html" target="_blank" rel="noopener">Python爬虫进阶四之PySpider的用法</a></li>
<li><a href="https://cuiqingcai.com/3325.html" target="_blank" rel="noopener">Python爬虫进阶五之多线程的用法</a></li>
<li><a href="https://cuiqingcai.com/3335.html" target="_blank" rel="noopener">Python爬虫进阶六之多进程的用法</a></li>
<li><a href="https://cuiqingcai.com/3443.html" target="_blank" rel="noopener">Python爬虫进阶七之设置ADSL拨号服务器代理</a><h1 id="12-附录"><a href="#12-附录" class="headerlink" title="12. 附录"></a>12. 附录</h1>必备软件的安装与故障排除<h2 id="12-1-平台"><a href="#12-1-平台" class="headerlink" title="12.1. 平台"></a>12.1. 平台</h2>搭建平台、开发环境<br>Linux、Windows、Mac<h2 id="12-2-软件安装及环境配置"><a href="#12-2-软件安装及环境配置" class="headerlink" title="12.2. 软件安装及环境配置"></a>12.2. 软件安装及环境配置</h2>Anaconda： <a href="https://www.continuum.io/downloads" target="_blank" rel="noopener">https://www.continuum.io/downloads</a> 。<br>Jupyter<br>默认端口：8888<br>为什么推荐大家使用Jupyter学习和编写Python脚本呢？<br>Jupyter：交互式编程和展示功能。<br>分段执行，编写和测试时边看边写，加快调试速度。<br>能够把运行和输出的结果保存下来，下次打开这个Notebook时也可以看到之前运行的结果。<br>还可以添加各种元素，比如图片、视频、链接等，同时还支持Markdown，可以充当PPT使用。<br>Alt + Enter jupyter快捷键<br>MongoDB环境配置<br>Robomongo：MongoDB数据库的可视化管理工具。<br>MySQL环境配置<br>Python、Python多版本共存配置<br>PyCharm<br>Redis、修改Redis配置<br>Redis Desktop Manager：Redis的可视化管理工具。<ul>
<li>下载 <a href="https://redisdesktop.com/download" target="_blank" rel="noopener">https://redisdesktop.com/download</a></li>
<li>界面 <a href="https://res.weread.qq.com/wrepub/epub_928559_154" target="_blank" rel="noopener">https://res.weread.qq.com/wrepub/epub_928559_154</a><br>Scrapy<br>SQLAlchemy<h2 id="12-3-第三方库"><a href="#12-3-第三方库" class="headerlink" title="12.3. 第三方库"></a>12.3. 第三方库</h2>Python爬虫常用库的安装<br>Lxml<br>Requests<br>BeautifulSoup</li>
</ul>
</li>
</ol>
<h1 id="13-术语表"><a href="#13-术语表" class="headerlink" title="13. 术语表"></a>13. 术语表</h1><p>Anaconda：Python开发集成环境。南美洲的巨蟒。自带Python、pip和Jupyter。<br>第三方库：可理解为供用户调用的代码组合。在安装某个库之后，可以直接调用其中的功能，使得我们不用一个代码一个代码地实现某个功能。<br>代码缩进：代码要按照结构以Tab键或者4个空格进行缩进严格缩进<br>DT（Data Technology，数据技术）<br>命令提示符。输入一些命令后，可执行对系统的管理。 Windows的cmd，开始按钮→cmd。Mac的terminal。应用程序→terminal。<br>爬虫：<br>pip：Python安装各种第三方库（package）的工具。<br>Python：蟒蛇<br>数据交换：网站与用户的沟通本质。<br>print<br>Python不需要在使用之前声明需要使用的变量和类别。<br>字符串（string）：单引号（’）或双引号（”）<br>连接字符串: +<br>数字（Number）：数字用来存储数值<br>整数（int）<br>浮点数（float）：由整数和小数部分组成。<br>列表（list）:能够包含任意种类的数据类型和任意数量。<br>创建列表非常容易，只要把不同的变量放入方括号中，并用逗号分隔即可，例如list0 = [“a”,2,”c”,4]<br>增删查改、索引、切片<br>字典（Dictionaries）：一种可变容器模型。<br>键（key）和值（value）。key必须唯一，但是值不用。值也可以取任何数据类型。<br>遍历<br>条件语句：满足条件的时候才执行某部分代码。条件为布尔值，也就是只有True和False两个值。<br>    当if判断条件成立时才执行后面的语句；当条件不成立的时候，执行else后面的语句<br>    如果需要判断的有多种条件，就需要用到elif<br>无序：字典<br>有序：列表、元组<br>对象有两种，即可更改（mutable）与不可更改（immutable）对象。在Python中，strings、tuples和numbers是不可更改对象，而list、dict等是可更改对象。<br>循环语句：多次执行一个代码片段。<br>循环分为for循环和while循环。<br>for循环：在一个给定的顺序下重复执行。<br>while循环：不断重复执行，只要能满足一定条件。<br>函数<br>代码庞大复杂时，使得代码易读，可重复使用，并且容易调整顺序。<br>函数的参数与返回值<br>面向过程编程：根据业务逻辑从上到下写代码，最容易被初学者接受。<br>函数式编程：把某些功能封装到函数中，需要用时可以直接调用，不用重复撰写。函数式的编程方法节省了大量时间。只需要写清楚输入和输出变量并执行函数即可。<br>面向对象编程：把函数进行分类和封装后放入对象中，使得开发更快、更强。首先要创建封装对象，然后还要通过对象调用被封装的内容。在某些应用场景下，面向对象编程能够显示出更大的优势。<br>如果各个函数之间独立且无共用的数据，就选用函数式编程；如果各个函数之间有一定的关联性，选用面向对象编程比较好。<br>特性与行为，属性和方法<br>面向对象的两大特性：封装和继承。<br>封装：把内容封装好，再调用封装好的内容。使用构造方法将内容封装到对象中，然后通过对象直接或self间接获取被封装的内容。<br>继承：以普通的类为基础建立专门的类对象。子继承了父的某些特性。将多个类共有的方法提取到父类中，子类继承父类中的方法即可，不必一一实现每个方法。<br>【状态码】<br>200，请求成功<br>4xx，客户端错误<br>5xx，服务器错误<br>【请求头】<br>Headers：提供了关于请求、响应或其他发送实体的信息。<br>如果没有指定请求头或请求的请求头和实际网页不一致，就可能无法返回正确的结果。<br>Chrome浏览器的检查。单击需要请求的网页，在Headers中可以看到Requests Headers的详细信息。<br>请求头的信息为：<br>GET / HTTP/1.1<br>Host: <a href="http://www.santostang.com" target="_blank" rel="noopener">www.santostang.com</a><br>Connection: keep-alive<br>Upgrade-Insecure-Requests: 1<br>User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.98 Safari/537.36<br>Accept:<br>text/html, application/xhtml+xml, application/xml; q=0.9, image/webp, <em>/</em>; q=0.8 Accept-Encoding: gzip, deflate, sdch<br>Accept-Language: en-US, en; q=0.8, zh-CN; q=0.6, zh; q=0.4, zh-TW; q=0.2<br>GET请求，密码会显示在URL中，非常不安全。<br>POST请求，<br>【动态网页】<br>AJAX（Asynchronous Javascript And XML，异步JavaScript和XML），一种异步更新技术。<br>单击“更多”，url地址没有任何改变，有新内容加载出来。<br>数据不会出现在网页源代码中。但是有JavaScript代码。<br>最后呈现出来的数据是通过JavaScript加载的。<br>通过在后台与服务器进行少量数据交换就可以使网页实现异步更新。<br>在不重新加载整个网页的情况下对网页的某部分进行更新。<br>减少了网页重复内容的下载<br>节省了流量<br>更小、更快、更友好<br>传统的网页必须重载整个网页页面<br>动态网页的例子<br><a href="http://www.santostang.com/2018/07/04/hello-world/" target="_blank" rel="noopener">http://www.santostang.com/2018/07/04/hello-world/</a><br>页面下面的评论用JavaScript加载。评论数据没法在在网页源代码找到。<br>注释：#</p>
<p><a href="https://getman.cn/" target="_blank" rel="noopener">https://getman.cn/</a> 在线测试</p>
<ul>
<li>首先看有没有移动端版本</li>
<li>按网页展开比按ID循环爬取要好，某些网站会设定特定的网页，当这些网页被访问时会让设备处于一定时间的封禁状态，展开式爬去绕过这种反爬陷阱可能性相对较高</li>
</ul>
<p>爬虫主要是和[[网页]]打交道。</p>
<p>网络协议 爬取原理 爬取任何一个网页的方法与思路</p>
<p>与爬虫相关的Web前端：[[HTML]]结构、[[CSS]]样式、[[JavaScript]]功能、[[Xpath]]和[[JSON]]。</p>
<p>辅助工具：[[chardet]] [[requests]] [[Firebug]] [[lxml]] [[BeautifulSoup4]] [[mechanize]] [[urllib]] [[Scrapy]] [[PhantomJS]]</p>
<p>爬虫是快速获取数据最重要的方式，相比其它语言，Python爬虫更简单、高效</p>
<p><a href="https://docs.python.org/3/library/html.parser.html" target="_blank" rel="noopener">https://docs.python.org/3/library/html.parser.html</a></p>
<ul>
<li>如何更高效</li>
<li>异步、阻塞、多线程 <a href="https://www.zhihu.com/question/19732473/answer/20851256" target="_blank" rel="noopener">https://www.zhihu.com/question/19732473/answer/20851256</a></li>
<li>区别异步和多线程：异步是目的，而多线程是实现这个目的的方法。<br>== 清单 ==</li>
<li>文件操作</li>
<li>异常处理</li>
<li>如何在Urllib中使用XPath表达式</li>
<li>Excel表格自动合并</li>
<li>CSV</li>
<li>数据库</li>
<li>从txt到数据库</li>
<li>多线程爬虫</li>
<li>JSON及解析</li>
<li>js里面找参数</li>
<li>排序</li>
<li>封装webdrive+phantomjs</li>
<li>面向对象</li>
<li>第三方库</li>
<li>网络爬虫概述</li>
<li>网络爬虫工作原理详解</li>
<li>爬虫流程</li>
<li>抓包分析</li>
<li>异常处理</li>
<li>浏览器伪装（cookie,header怎么填）</li>
<li>js采集（仅仅实现selenimu或者无头）</li>
<li>超时设置</li>
<li>用户和IP代理池 （github上可能有）<br><strong> 用户代理池构建
</strong> IP代理池构建的两种方案<br><strong> 同时使用用户代理池与IP代理池的方法
</strong> 代理 <a href="https://www.baibianip.com/?aff=87352500" target="_blank" rel="noopener">https://www.baibianip.com/?aff=87352500</a><br>** 代理池、高匿代理</li>
<li>head池</li>
<li>机器学习验证码</li>
<li>OCR、识图,分部识图</li>
<li>http协议</li>
<li>分布式爬虫<br><strong> 分布式爬虫实现原理
</strong> 分布式爬虫之Docker基础<br><strong> 分布式爬虫之Redis基础
</strong> 分布式爬虫构建<br>** <a href="https://mp.weixin.qq.com/s/5fl0oe6Z4j6auKJG41Nw4A" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/5fl0oe6Z4j6auKJG41Nw4A</a> 京东商品和评论的分布式爬虫<br>== 面试相关 ==<br>面试相关：</li>
<li><a href="https://www.cnblogs.com/tianyiliang/tag/python面试常见题" target="_blank" rel="noopener">https://www.cnblogs.com/tianyiliang/tag/python面试常见题</a></li>
<li>面试答题技巧 <a href="https://github.com/cjql/201902/issues/2" target="_blank" rel="noopener">https://github.com/cjql/201902/issues/2</a></li>
<li>300 道python面试 <a href="https://mp.weixin.qq.com/s/m-2ZOIZJlZESo4er_xRZZA" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/m-2ZOIZJlZESo4er_xRZZA</a></li>
<li>爬虫面试 <a href="https://www.cnblogs.com/skiler/p/6939688.html" target="_blank" rel="noopener">https://www.cnblogs.com/skiler/p/6939688.html</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/35794035" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/35794035</a></li>
<li><a href="https://m.jb51.net/article/136867.htm" target="_blank" rel="noopener">https://m.jb51.net/article/136867.htm</a></li>
<li><a href="https://www.cnblogs.com/zdong0103/p/8492779.html" target="_blank" rel="noopener">https://www.cnblogs.com/zdong0103/p/8492779.html</a></li>
</ul>
<p>== 累积 ==</p>
<ul>
<li>json格式如果信息显示不全，可自己拼接API(&amp;和json正文是最佳提示)</li>
<li>哪些页面出现多少问题</li>
<li>每分钟爬多少</li>
</ul>
<p>==案例==</p>
<ul>
<li>Docker部署Scrapy-redis分布式爬虫框架实践（整合Selenium+Headless Chrome网页渲染） <a href="https://mp.weixin.qq.com/s/k43v0ABKvf8gPgE4IBO6ag" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/k43v0ABKvf8gPgE4IBO6ag</a>  </li>
<li>分布式爬虫系统设计、实现与实战：爬取京东、苏宁易购全网手机商品数据+MySQL、HBase存储 <a href="https://mp.weixin.qq.com/s/eBSiFhWOQW3hPlWSmwXDvA" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/eBSiFhWOQW3hPlWSmwXDvA</a></li>
<li><a href="https://github.com/cjql/spider/issues/1" target="_blank" rel="noopener">https://github.com/cjql/spider/issues/1</a> 33个网络爬虫</li>
<li>糗事百科爬虫</li>
<li>tt商品图片爬虫</li>
<li>tt商品大型爬虫项目与自动写入数据库</li>
<li>微信爬虫</li>
<li>京东图书</li>
<li>新闻爬虫</li>
<li>博文信息的爬取</li>
<li>Scrapy实现当当网商品爬虫</li>
<li>Scrapy模拟登录</li>
<li>Scrapy新闻爬虫项目</li>
<li>Scrapy豆瓣网登陆爬虫与验证码自动识别项目</li>
<li>腾讯视频评论爬虫</li>
<li>腾讯动漫爬虫项目（JS动态触发+id随机生成反爬破解）</li>
<li><a href="https://github.com/cjql/201903/issues/46" target="_blank" rel="noopener">https://github.com/cjql/201903/issues/46</a> 爬GitHub</li>
<li><a href="https://github.com/cjql/201903/issues/47" target="_blank" rel="noopener">https://github.com/cjql/201903/issues/47</a> 爬GitHub</li>
<li><a href="https://www.jianshu.com/p/36f5f74b6c04" target="_blank" rel="noopener">https://www.jianshu.com/p/36f5f74b6c04</a> Python爬虫爬取微信公众号历史文章全部链接</li>
<li><a href="https://www.tenable.com/downloads/cis-compliance-audit-policies" target="_blank" rel="noopener">https://www.tenable.com/downloads/cis-compliance-audit-policies</a>  爬视频和软件</li>
<li>简单爬虫的编写</li>
<li>出版社信息的爬取<br>== 他人GitHub项目 ==</li>
<li><a href="https://github.com/LunaticTian/TieBa-API-Sentiment" target="_blank" rel="noopener">https://github.com/LunaticTian/TieBa-API-Sentiment</a> 可参照</li>
<li><a href="https://github.com/LUCY78765580/Python-web-scraping" target="_blank" rel="noopener">https://github.com/LUCY78765580/Python-web-scraping</a></li>
<li><a href="https://github.com/wkunzhi/SpiderUtilPackage" target="_blank" rel="noopener">https://github.com/wkunzhi/SpiderUtilPackage</a></li>
<li><a href="https://github.com/zyingzhou/music163-spiders" target="_blank" rel="noopener">https://github.com/zyingzhou/music163-spiders</a></li>
<li><a href="https://github.com/xjkj123/Lianjia" target="_blank" rel="noopener">https://github.com/xjkj123/Lianjia</a><br>== 他人爬虫 ==</li>
<li><a href="https://github.com/cjql/201903/issues/45" target="_blank" rel="noopener">https://github.com/cjql/201903/issues/45</a> 163</li>
<li>京东定向ajax</li>
<li><a href="https://github.com/Erma-Wang/Spider" target="_blank" rel="noopener">https://github.com/Erma-Wang/Spider</a></li>
<li><a href="https://github.com/iamyaojie/Spider" target="_blank" rel="noopener">https://github.com/iamyaojie/Spider</a></li>
<li><a href="https://mp.weixin.qq.com/s/VD79jLvR1MUukrD5D7K5Wg" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/VD79jLvR1MUukrD5D7K5Wg</a></li>
<li><a href="https://github.com/cjql/201903" target="_blank" rel="noopener">https://github.com/cjql/201903</a> 爬虫登录<br>== 进阶 ==</li>
<li>自动搜索爬虫</li>
<li>用Python爬虫做了一个开源搜索引擎 <a href="https://zhuanlan.zhihu.com/p/33414953" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/33414953</a><br>== 遇到问题 ==</li>
<li>浏览器无法右键网页→fiddler</li>
<li>fiddler识别的是汉字，requests库识别处理的是乱码→用手机版网址</li>
<li>访问太快（88）遇反爬→headers池不起作用→非要与原网页结构一致的header才生效</li>
<li>换header依然只能爬一部分（130+）</li>
<li>访问到特定页面会被网站判定为爬虫 <a href="http://m.99lib.net/book/115/index.html" target="_blank" rel="noopener">http://m.99lib.net/book/115/index.html</a> （无法预期，重构）<br>== todo ==</li>
<li>阿里旅行</li>
<li>待突破技术</li>
</ul>
<p>基础知识及案例。<br><!-- more -->  </p>
<h1 id="1-库的说明"><a href="#1-库的说明" class="headerlink" title="1. 库的说明"></a>1. 库的说明</h1><h2 id="1-1-re"><a href="#1-1-re" class="headerlink" title="1.1. re"></a>1.1. re</h2><p>Python正则表达式文档：<a href="https://docs.python.org/3/library/re.html" target="_blank" rel="noopener">https://docs.python.org/3/library/re.html</a><br>Python正则表达式的3种方法，分别是match、search和findall。</p>
<h3 id="re-match"><a href="#re-match" class="headerlink" title="re.match"></a>re.match</h3><p>re.match：从字符串起始位置匹配，有则返回re.Match object，没有则返回none。</p>
<blockquote>
<p>re.match(pattern, string, flags=0)</p>
<ul>
<li>pattern：正则表达式，包含一些特殊的字符</li>
<li>string：被匹配的原字符串</li>
<li>flags：控制正则表达式的匹配方式，如是否区分大小写、多行匹配等。</li>
</ul>
</blockquote>
<p>字符串匹配：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">m = re.match(<span class="string">'www'</span>, <span class="string">'www.santostang.com'</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"匹配的结果：  "</span>, m)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"匹配的起始与终点：  "</span>, m.span())</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"匹配的起始位置：  "</span>, m.start())</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"匹配的终点位置：  "</span>, m.end())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到的结果为：</span></span><br><span class="line"><span class="comment"># 匹配的结果： &lt;re.Match object; span=(0, 3), match='www'&gt;</span></span><br><span class="line"><span class="comment"># 匹配的起始与终点： (0, 3)</span></span><br><span class="line"><span class="comment"># 匹配的起始位置： 0</span></span><br><span class="line"><span class="comment"># 匹配的终点位置： 3</span></span><br></pre></td></tr></table></figure></p>
<p>正则匹配：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">line = <span class="string">"Fat cats are smarter than dogs, is it right? "</span></span><br><span class="line">m = re.match( <span class="string">r'(.＊) are (.＊? ) dogs'</span>, line)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'匹配的整句话'</span>, m.group(<span class="number">0</span>))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'匹配的第一个结果'</span>, m.group(<span class="number">1</span>))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'匹配的第二个结果'</span>, m.group(<span class="number">2</span>))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'匹配的结果列表'</span>, m.groups())</span><br><span class="line"><span class="comment"># 得到的结果为：</span></span><br><span class="line"><span class="comment"># 匹配的整句话Fat cats are smarter than dogs</span></span><br><span class="line"><span class="comment"># 匹配的第一个结果Fat cats</span></span><br><span class="line"><span class="comment"># 匹配的第二个结果smarter than</span></span><br><span class="line"><span class="comment"># 匹配的结果列表 ('Fat cats', 'smarter than')</span></span><br></pre></td></tr></table></figure></p>
<p><a href="https://docs.python.org/3/library/re.html#match-objects" target="_blank" rel="noopener">https://docs.python.org/3/library/re.html#match-objects</a> </p>
<h3 id="re-search"><a href="#re-search" class="headerlink" title="re.search"></a>re.search</h3><ul>
<li>re.match只能从字符串的【起始】位置进行匹配。</li>
<li>re.search扫描整个字符串并返回【第一个】成功的匹配。</li>
<li>其他方面re.search与re.match一样。<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">m_match = re.match(<span class="string">'com'</span>, <span class="string">'www.santostang.com'</span>)</span><br><span class="line">m_search = re.search(<span class="string">'com'</span>, <span class="string">'www.santostang.com'</span>)</span><br><span class="line"><span class="keyword">print</span> (m_match)</span><br><span class="line"><span class="keyword">print</span> (m_search)</span><br><span class="line"><span class="comment"># 得到结果为：</span></span><br><span class="line"><span class="comment"># None</span></span><br><span class="line"><span class="comment"># &lt;re.Match object; span=(15, 18), match='com'&gt;</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="re-findall方法"><a href="#re-findall方法" class="headerlink" title="re.findall方法"></a>re.findall方法</h3><ul>
<li>match和search，只能找到一个匹配所写的模式</li>
<li>findall可以找到所有的匹配，返回列表<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">m_match = re.match(<span class="string">'[0-9]+'</span>, <span class="string">'12345 is the first number, 23456 is the sencond'</span>)</span><br><span class="line">m_search = re.search(<span class="string">'[0-9]+'</span>, <span class="string">'The first number is 12345, 23456 is the sencond'</span>)</span><br><span class="line">m_findall = re.findall(<span class="string">'[0-9]+'</span>, <span class="string">'12345 is the first number, 23456 is the sencond'</span>)</span><br><span class="line"><span class="keyword">print</span> (m_match.group())</span><br><span class="line"><span class="keyword">print</span> (m_search.group())</span><br><span class="line"><span class="keyword">print</span> (m_findall)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>上述代码的’[0-9]+’表示任意长度的数字，然后在后面的字符串中进行匹配。</p>
<h3 id="为什么要在match的模式前加上r"><a href="#为什么要在match的模式前加上r" class="headerlink" title="为什么要在match的模式前加上r"></a>为什么要在match的模式前加上r</h3><p>r：raw string，纯粹的字符串。使用它就不会对引S号里面的反斜杠<code>\</code>进行特殊处理。<br>在正则表达式中有一些类似<code>\d</code>（匹配任何数字）的模式，都要进行转译。<br>假如你需要匹配文本中的字符<code>\</code>，使用编程语言表示的正则表达式里就需要4个反斜杠<code>\\\\</code>：</p>
<ul>
<li>前两个反斜杠<code>\\</code>和后两个反斜杠<code>\\</code>各自在编程语言里转义成一个反斜杠<code>\</code></li>
<li>所以4个反斜杠<code>\\\\</code>就转义成了两个反斜<code>\\</code></li>
<li>这两个反斜杠<code>\\</code>最终在正则表达式里转义成一个反斜杠<code>\</code><br>Python里的原生字符串很好地解决了这个问题，在正则表达式里不会再转义，这个例子中的正则表达式可以使用r<code>\\</code>表示。</li>
</ul>
<p>(.<em>) are会尽量匹配最多的字符。贪婪模式<br>(.</em>? )会尽量匹配尽量少的字符。非贪婪模式</p>
<h2 id="1-2-bs4"><a href="#1-2-bs4" class="headerlink" title="1.2. bs4"></a>1.2. bs4</h2><p>使用BeautifulSoup解析网页<br>BeautifulSoup是一个工具箱。通过【解析文档】来提取数据。</p>
<ul>
<li>可以从HTML或XML文件中提取数据。</li>
<li>可以提供一些简单的、Python式的函数用来处理导航、搜索、修改分析树等。<br>简单，不需要多少代码就可以写出一个完整的应用程序。非常强大。<br>支持Python标准库中的HTML解析器，还支持一些第三方的解析器。</li>
</ul>
<p>BeautifulSoup 4主要特性、适合做什么、怎样使用<br>使用BeautifulSoup获取博客标题<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">link = <span class="string">"http://www.santostang.com/"</span></span><br><span class="line">headers = &#123;<span class="string">'User-Agent'</span> : <span class="string">'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'</span>&#125; </span><br><span class="line">r = requests.get(link, headers= headers)</span><br><span class="line"></span><br><span class="line">soup = BeautifulSoup(r.text,<span class="string">"html.parser"</span>) <span class="comment"># 将网页响应体的字符串转化为soup对象</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># &lt;h1&gt;元素，class为' post-title'，提取&lt;a&gt;元素中的文字，strip()的功能是把字符串左右的空格去掉。find只是用来找到第一条结果。</span></span><br><span class="line">first_title = soup.find(<span class="string">"h1"</span>, class_=<span class="string">"post-title"</span>).a.text.strip() </span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"第一篇文章的标题是："</span>, first_title)</span><br><span class="line"></span><br><span class="line">title_list = soup.find_all(<span class="string">"h1"</span>, class_=<span class="string">"post-title"</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(title_list)):</span><br><span class="line">    title = title_list[i].a.text.strip()</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'第 %s 篇文章的标题是：%s'</span> %(i+<span class="number">1</span>, title))</span><br></pre></td></tr></table></figure></p>
<p>找所有结果，用find_all。find_all返回列表。</p>
<p>BeautifulSoup的其他功能<br>soup.prettify()  代码美化<br>首先，需要把：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">soup = BeautifulSoup(html, <span class="string">"html.parser"</span>)</span><br></pre></td></tr></table></figure></p>
<p>代码转化成BeautifulSoup对象。<br>BeautifulSoup对象是一个复杂的【树】形结构，它的每一个【节点】都是一个【Python对象】。</p>
<p>提取对象的3种方法：</p>
<blockquote>
<p>遍历文档树<br>搜索文档树<br>CSS选择器</p>
</blockquote>
<p>1．遍历文档树<br>先爬树干，然后小树干，最后树枝。<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">soup.header.h3：获取取&lt;h3&gt;标签。如结果为：&lt;h3 id="name"&gt;大数据@唐松Santos&lt;/h3&gt;）。</span><br><span class="line">soup.header.div.contents：列出某个标签的所有子节点。只能获取第一代子标签。</span><br><span class="line">soup.header.div.contents[<span class="number">1</span>]：索引为<span class="number">1</span>的子标签。</span><br><span class="line">soup.header.div.children：获得所有子标签。只能获取第一代子标签。</span><br><span class="line">soup.header.div.descendants：获得所有子子孙孙标签</span><br><span class="line">soup.header.div.a.parent：获得父节点的内容：</span><br></pre></td></tr></table></figure></p>
<p>遍历文档树的方法其实使用得比较少。</p>
<p>2．搜索文档树<br>最常用的是搜索文档树。<br>最常用的是find()和find_all()。<br>find()和find_all()方法还可以和re正则结合起来使用<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> tag <span class="keyword">in</span> soup.find_all(re.compile(<span class="string">"^h"</span>)):  <span class="comment"># 找出所有以h开头的标签，这表示&lt;header&gt;和&lt;h3&gt;的标签都会被找到  </span></span><br><span class="line">    print(tag.name)</span><br><span class="line"><span class="comment"># 输出的结果是：</span></span><br><span class="line"><span class="comment"># header</span></span><br><span class="line"><span class="comment"># h3</span></span><br></pre></td></tr></table></figure></p>
<p>如果传入正则表达式作为参数，Beautiful Soup就会通过正则表达式的match()来匹配内容。</p>
<ol>
<li>CSS选择器</li>
</ol>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">通过tag标签逐层查找：soup.select("header h3")⇒得到的结果是：[<span class="tag">&lt;<span class="name">h3</span> <span class="attr">id</span>=<span class="string">"name"</span>&gt;</span>大数据@唐松Santos<span class="tag">&lt;/<span class="name">h3</span>&gt;</span>]</span><br><span class="line">通过某个tag标签下的直接子标签遍历，：</span><br><span class="line">soup.select("header &gt; h3") ⇒[<span class="tag">&lt;<span class="name">h3</span> <span class="attr">id</span>=<span class="string">"name"</span>&gt;</span>大数据@唐松Santos<span class="tag">&lt;/<span class="name">h3</span>&gt;</span>]</span><br><span class="line">soup.select("div &gt; a") ⇒ <span class="tag">&lt;<span class="name">div</span>&gt;</span>下所有的<span class="tag">&lt; <span class="attr">a</span>&gt;</span>标签</span><br><span class="line">[<span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://www.santostang.com/feed/"</span> <span class="attr">rel</span>=<span class="string">"nofollow"</span> <span class="attr">target</span>=<span class="string">"_blank"</span><span class="attr">title</span>=<span class="string">"RSS"</span>&gt;</span><span class="tag">&lt;<span class="name">i</span>　<span class="attr">aria-hidden</span>=<span class="string">"true"</span>　<span class="attr">class</span>=<span class="string">"fa　fa-rss"</span>&gt;</span><span class="tag">&lt;/<span class="name">i</span>&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span>,　<span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://weibo.com/santostang"</span> <span class="attr">rel</span>=<span class="string">"nofollow"</span> <span class="attr">target</span>=<span class="string">"_blank"</span> <span class="attr">title</span>=<span class="string">"Weibo"</span>&gt;</span><span class="tag">&lt;<span class="name">i</span> <span class="attr">aria-hidden</span>=<span class="string">"true"</span> <span class="attr">class</span>=<span class="string">"fa fa-weibo"</span>&gt;</span><span class="tag">&lt;/<span class="name">i</span>&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span>, …]</span><br><span class="line"></span><br><span class="line">soup.select('a[href^="http://www.santostang.com/"]')：找所有链接以http://www.santostang.com/开始的<span class="tag">&lt;<span class="name">a</span>&gt;</span>标签</span><br><span class="line">得到的结果是：</span><br><span class="line">[<span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://www.santostang.com/feed/"</span> <span class="attr">rel</span>=<span class="string">"nofollow"</span> <span class="attr">target</span>=<span class="string">"_blank"</span><span class="attr">title</span>=<span class="string">"RSS"</span>&gt;</span><span class="tag">&lt;<span class="name">i</span> <span class="attr">aria-hidden</span>=<span class="string">"true"</span> <span class="attr">class</span>=<span class="string">"fa fa-rss"</span>&gt;</span><span class="tag">&lt;/<span class="name">i</span>&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span>,</span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://www.santostang.com/"</span>&gt;</span>首页<span class="tag">&lt;/<span class="name">a</span>&gt;</span>,</span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://www.santostang.com/about-me/"</span>&gt;</span>关于我<span class="tag">&lt;/<span class="name">a</span>&gt;</span>,</span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://www.santostang.com/post-search/"</span>&gt;</span>文章搜索<span class="tag">&lt;/<span class="name">a</span>&gt;</span>,</span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"http://www.santostang.com/wp-login.php"</span>&gt;</span>登录<span class="tag">&lt;/<span class="name">a</span>&gt;</span>]</span><br></pre></td></tr></table></figure>
<p>5.3 使用lxml解析网页<br>一些比较流行的解析库<br>Xpath语法（如lxml），同样是效率比较高的解析方法。lxml使用C语言编写，解析速度比不使用lxml解析器的BeautifulSoup快一些。</p>
<p>5.3.2 使用lxml获取博客标题<br>使用lxml提取网页源代码数据的3种方法<br>    XPath选择器<br>    CSS选择器<br>    BeautifulSoup的find()方法</p>
<p>和BeautifulSoup相比，lxml还多了一种XPath选择器方法。</p>
<p>XPath是一门在XML文档中查找信息的语言。<br>XPath使用路径表达式来选取XML文档中的节点或节点集，也可以用在HTML获取数据中。<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">link = <span class="string">"http://www.santostang.com/"</span></span><br><span class="line">headers = &#123;<span class="string">'User-Agent'</span> : <span class="string">'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'</span>&#125; </span><br><span class="line">r = requests.get(link, headers= headers)</span><br><span class="line"></span><br><span class="line">html = etree.HTML(r.text) <span class="comment"># 解析为lxml的格式</span></span><br><span class="line">title_list = html.xpath(<span class="string">'//h1[@class="post-title"]/a/text()'</span>) <span class="comment"># 用XPath读取里面的内容</span></span><br><span class="line"><span class="keyword">print</span> (title_list)</span><br><span class="line"></span><br><span class="line">//：无论在文档中什么位置</span><br><span class="line">//h1：所有&lt;h1&gt;元素</span><br><span class="line">//h1[@class="post-title"]：&lt;h1&gt;中class为"post-title"的元素</span><br><span class="line">/a表示选取&lt;h1&gt;子元素的&lt;a&gt;元素</span><br><span class="line">/text()表示提取&lt;a&gt;元素中的所有文本。</span><br></pre></td></tr></table></figure></p>
<p>chrome审查，右键，选取元素，Copy→Copy XPath</p>
<p>5.3.3 XPath的选取方法<br>XPath使用路径表达式可以在网页源代码中选取节点，它是沿着路径来选取的，如表5-3所示。<br>XPath路径表达式及其描述 <a href="https://res.weread.qq.com/wrepub/epub_928559_47" target="_blank" rel="noopener">https://res.weread.qq.com/wrepub/epub_928559_47</a><br>下面是一个XML文档，我们将用XPath提取其中的一些数据。<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;? xml version=&quot;1.0&quot; encoding=&quot;ISO-8859-1&quot;? &gt;</span><br><span class="line">&lt;bookstore&gt;</span><br><span class="line">    &lt;book&gt;  </span><br><span class="line">        &lt;title lang=&quot;en&quot;&gt;Harry Potter&lt;/title&gt;  </span><br><span class="line">        &lt;author&gt;J K. Rowling&lt;/author&gt;  </span><br><span class="line">        &lt;year&gt;2005&lt;/year&gt;  </span><br><span class="line">        &lt;price&gt;29.99&lt;/price&gt;</span><br><span class="line">    &lt;/book&gt;</span><br><span class="line">&lt;/bookstore&gt;</span><br></pre></td></tr></table></figure></p>
<p>XPath的一些路径表达式及其结果：<a href="https://res.weread.qq.com/wrepub/epub_928559_48" target="_blank" rel="noopener">https://res.weread.qq.com/wrepub/epub_928559_48</a></p>
<p><a href="https://github.com/Santostang/PythonScraping/blob/master/%E7%AC%AC%E4%B8%80%E7%89%88/Cha%205%20-%E8%A7%A3%E6%9E%90%E7%BD%91%E9%A1%B5/Cha%205%20-%E8%A7%A3%E6%9E%90%E7%BD%91%E9%A1%B5.ipynb" target="_blank" rel="noopener">https://github.com/Santostang/PythonScraping/blob/master/%E7%AC%AC%E4%B8%80%E7%89%88/Cha%205%20-%E8%A7%A3%E6%9E%90%E7%BD%91%E9%A1%B5/Cha%205%20-%E8%A7%A3%E6%9E%90%E7%BD%91%E9%A1%B5.ipynb</a></p>
<p>5.5 BeautifulSoup爬虫实践：房屋价格数据<br>目的：获取安居客网站上北京二手房的数据。获取前10页二手房源的名称、价格、几房几厅、大小、建造年份、联系人、地址、标签。<br>网址：<a href="https://beijing.anjuke.com/sale/。" target="_blank" rel="noopener">https://beijing.anjuke.com/sale/。</a><br>5.5.1 网站分析</p>
<p>5.5.2 项目实践<br>通过以上分析已经能够获得各个数据所在的地址，接下来用requests加上BeautifulSoup获取安居客北京二手房结果的第一页数据，代码如下：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">headers = &#123;<span class="string">'User-Agent'</span> : <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.98 Safari/537.36'</span>&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">11</span>):</span><br><span class="line">    link = <span class="string">'https://beijing.anjuke.com/sale/p'</span> + str(i)</span><br><span class="line">    r = requests.get(link, headers = headers)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'现在爬取的是第'</span>, i, <span class="string">'页'</span>)</span><br><span class="line"></span><br><span class="line">    soup = BeautifulSoup(r.text, <span class="string">'lxml'</span>)</span><br><span class="line">    house_list = soup.find_all(<span class="string">'li'</span>, class_=<span class="string">"list-item"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> house <span class="keyword">in</span> house_list:</span><br><span class="line">        name = house.find(<span class="string">'div'</span>, class_ =<span class="string">'house-title'</span>).a.text.strip()</span><br><span class="line">        price = house.find(<span class="string">'span'</span>, class_=<span class="string">'price-det'</span>).text.strip()</span><br><span class="line">        price_area = house.find(<span class="string">'span'</span>, class_=<span class="string">'unit-price'</span>).text.strip()</span><br><span class="line"></span><br><span class="line">        no_room = house.find(<span class="string">'div'</span>, class_=<span class="string">'details-item'</span>).span.text</span><br><span class="line">        area = house.find(<span class="string">'div'</span>, class_=<span class="string">'details-item'</span>).contents[<span class="number">3</span>].text</span><br><span class="line">        floor = house.find(<span class="string">'div'</span>, class_=<span class="string">'details-item'</span>).contents[<span class="number">5</span>].text</span><br><span class="line">        year = house.find(<span class="string">'div'</span>, class_=<span class="string">'details-item'</span>).contents[<span class="number">7</span>].text</span><br><span class="line">        broker = house.find(<span class="string">'span'</span>, class_=<span class="string">'brokername'</span>).text</span><br><span class="line">        broker = broker[<span class="number">1</span>:]</span><br><span class="line">        address = house.find(<span class="string">'span'</span>, class_=<span class="string">'comm-address'</span>).text.strip()</span><br><span class="line">        address = address.replace(<span class="string">'\xa0\xa0\n                    '</span>,<span class="string">'  '</span>)</span><br><span class="line">        tag_list = house.find_all(<span class="string">'span'</span>, class_=<span class="string">'item-tags'</span>)</span><br><span class="line">        tags = [i.text <span class="keyword">for</span> i <span class="keyword">in</span> tag_list] </span><br><span class="line">        <span class="keyword">print</span> (name, price, price_area, no_room, area, floor, year, broker, address, tags)</span><br><span class="line">    time.sleep(<span class="number">5</span>)</span><br><span class="line">```  </span><br><span class="line">进阶：获取其中的各项数据，如小区名称、房屋类型、房屋朝向、参考首付等。</span><br><span class="line"></span><br><span class="line">https://github.com/Santostang/PythonScraping/blob/master/%E7%AC%AC%E4%B8%<span class="number">80</span>%E7%<span class="number">89</span>%<span class="number">88</span>/Cha%<span class="number">205</span>%<span class="number">20</span>-%E8%A7%A3%E6%<span class="number">9</span>E%<span class="number">90</span>%E7%BD%<span class="number">91</span>%E9%A1%B5/Cha%<span class="number">205</span>%<span class="number">20</span>_%E7%AB%A0%E6%<span class="number">9</span>C%AB%E5%AE%<span class="number">9</span>E%E6%<span class="number">88</span>%<span class="number">98.</span>ipynb</span><br><span class="line"><span class="comment">### 1.2.1. requests  </span></span><br><span class="line">```py  </span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">r = requests.get(<span class="string">'http://www.santostang.com/'</span>)  </span><br><span class="line"><span class="comment"># ========【r的方法】========  </span></span><br><span class="line"><span class="comment"># r response响应对象，存储了服务器响应的内容，以从中获取需要的信息  </span></span><br><span class="line"><span class="comment"># r.encoding  服务器内容使用的文本编码。  </span></span><br><span class="line"><span class="comment"># r.status_code 响应状态码。检测请求是否正确响应。  </span></span><br><span class="line"><span class="comment"># r.text  字符串方式的响应体。会自动根据响应头部的字符编码进行解码。  </span></span><br><span class="line"><span class="comment"># r.content 字节方式的响应体。会自动解码gzip和deflate编码的响应数据。gzip文件用这个。  </span></span><br><span class="line"><span class="comment"># r.json()  Requests中内置的JSON解码器。  </span></span><br><span class="line"><span class="comment"># r.url r对应的请求的页面网址  </span></span><br><span class="line"><span class="comment"># ========【requests.get的参数设置】========  </span></span><br><span class="line"><span class="comment">## URL参数、请求头、发送POST请求、设置超时  </span></span><br><span class="line"></span><br><span class="line"><span class="comment">## ----------【params】：dict ----------  </span></span><br><span class="line"><span class="comment">### get传递url参数。http://httpbin.org/get?key1=value1&amp;key2=value2  </span></span><br><span class="line">key_dict = &#123;<span class="string">'key1'</span>: <span class="string">'value1'</span>, <span class="string">'key2'</span>: <span class="string">'value2'</span>&#125;  </span><br><span class="line">r = requests.get(<span class="string">'http://httpbin.org/get'</span>, params=key_dict)  </span><br><span class="line"><span class="comment">## ----------【headers】：dict ----------  </span></span><br><span class="line"><span class="comment">### 有的网站不带请求头会返回错误的数据。带请求头使程序更像人的手动行为  </span></span><br><span class="line">headers = &#123;  </span><br><span class="line"><span class="string">'user-agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.82 Safari/537.36'</span>,  </span><br><span class="line"><span class="string">'Host'</span>: <span class="string">'www.santostang.com'</span>  </span><br><span class="line">&#125;  </span><br><span class="line">r = requests.get(<span class="string">'http://www.santostang.com/'</span>, headers=headers)  </span><br><span class="line"><span class="comment">## ----------【data】: dict ----------  </span></span><br><span class="line"><span class="comment">### 用于提交表单。data在发出请求的时候会自动编码为表单形式。  </span></span><br><span class="line">key_dict = &#123;<span class="string">'key1'</span>: <span class="string">'value1'</span>, <span class="string">'key2'</span>: <span class="string">'value2'</span>&#125;  </span><br><span class="line">r = requests.post(<span class="string">'http://httpbin.org/post'</span>, data=key_dict)  </span><br><span class="line"><span class="comment">## ----------【timeout】: 单位为秒 ----------  </span></span><br><span class="line"><span class="comment">### 如果服务器在timeout秒内没有应答，就返回异常。一般会把这个值设置为20秒。  </span></span><br><span class="line">link = <span class="string">"http://www.santostang.com/"</span>  </span><br><span class="line">r = requests.get(link, timeout= <span class="number">0.001</span>)  </span><br><span class="line"><span class="comment">## 返回的异常为：  </span></span><br><span class="line"></span><br><span class="line"><span class="comment">## ConnectTimeout: HTTPConnectionPool(host='www.santostang.com', port=80): Max retries exceeded with url: / (Caused by ConnectTimeoutError(&lt;requests.packages.urllib3.connection.HTTPConnection object at 0x00000000077806D8&gt;, 'Connection to www.santostang.com timed out. (connect timeout=0.001)'))  </span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 异常值的意思是，时间限制在0.001秒内，连接到地址为www.santostang.com的时间已到。  </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># https://github.com/Santostang/PythonScraping/blob/master/第一版/Cha 3 -静态网页抓取/Cha 3 -静态网页抓取.ipynb  </span></span><br><span class="line">```  </span><br><span class="line"><span class="comment">### 1.2.2. Selenium  </span></span><br><span class="line">Selenium选择元素的方法有很多。  </span><br><span class="line">xpath和css_selector是比较好的方法，一方面比较清晰，另一方面相对其他方法定位元素比较准确。  </span><br><span class="line">```xpath</span><br><span class="line">查找单个元素：  </span><br><span class="line">find_element_by_class_name：<span class="class"><span class="keyword">class</span>选择  </span></span><br><span class="line">    如&lt;p class="content"&gt;Site content goes here.&lt;/p&gt;⇒driver.find_element_by_class_name('content')。  </span><br><span class="line">find_element_by_css_selector：<span class="class"><span class="keyword">class</span>选择  </span></span><br><span class="line">    如&lt;div class='bdy-inner'&gt;test&lt;/div&gt;⇒driver.find_element_by_css_selector ('div.bdy-inner')。  </span><br><span class="line">find_element_by_id：id选择  </span><br><span class="line">    如&lt;div id='bdy-inner'&gt;test&lt;/div&gt;⇒driver.find_element_by_id('bdy-inner')。  </span><br><span class="line">find_element_by_link_text：链接地址选择  </span><br><span class="line">    如&lt;a href="continue.html"&gt;Continue&lt;/a&gt;⇒driver.find_element_by_link_text('Continue')。  </span><br><span class="line">find_element_by_name：name选择  </span><br><span class="line">    如&lt;input name=<span class="string">"username"</span>type=<span class="string">"text"</span> /&gt;⇒driver.find_element_by_name(<span class="string">'username'</span>)。  </span><br><span class="line">find_element_by_partial_link_text：链接的部分地址选择  </span><br><span class="line">    如 &lt;a href="continue.html"&gt;Continue&lt;/a&gt;⇒driver.find_element_by_partial_link_text('Conti')。  </span><br><span class="line">find_element_by_tag_name：名称选择  </span><br><span class="line">    如&lt;h1&gt;Welcome&lt;/h1&gt;⇒driver.find_element_by_tag_name('h1')。  </span><br><span class="line">find_element_by_xpath：通过xpath选择  </span><br><span class="line">    如&lt;form id=<span class="string">"loginForm"</span>&gt; ⇒driver.find_element_by_xpath(<span class="string">"//form[@id='loginForm']"</span>)。  </span><br><span class="line"></span><br><span class="line">查找多个元素时，[element]后加上s：  </span><br><span class="line">find_elements_by_class_name  </span><br><span class="line">find_elements_by_css_selector  </span><br><span class="line">find_elements_by_link_text  </span><br><span class="line">find_elements_by_name  </span><br><span class="line">find_elements_by_partial_link_text  </span><br><span class="line">find_elements_by_tag_name  </span><br><span class="line">find_elements_by_xpath  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">除了Selenium的click操作元素方法，常见的操作元素方法：  </span><br><span class="line">● Clear清除元素的内容。  </span><br><span class="line">● send_keys模拟按键输入。  </span><br><span class="line">● Click单击元素。  </span><br><span class="line">● Submit提交表单。</span><br></pre></td></tr></table></figure></p>
<p>comment = driver.find_element_by_css_selector(‘div.bdy-inner’)<br>content = comment.find_element_by_tag_name(‘p’) </p>
<p>Selenium的高级操作：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">fp = webdriver.FirefoxProfile()</span><br><span class="line"><span class="comment"># 1. 限制CSS的页面</span></span><br><span class="line">fp.set_preference(<span class="string">"permissions.default.stylesheet"</span>,<span class="number">2</span>) </span><br><span class="line"><span class="comment"># 2. 限制图片的显示。极大地提高网络爬虫的效率。图片文件相对于文字、CSS、JavaScript等文件都比较大，加载需要较长时间。</span></span><br><span class="line">fp.set_preference(<span class="string">"permissions.default.image"</span>,<span class="number">2</span>) </span><br><span class="line"><span class="comment"># 3. 控制JavaScript的运行。大多数网页都会利用JavaScript异步加载很多内容，如果这些内容不是需要的，其加载会浪费时间。</span></span><br><span class="line">fp.set_preference(<span class="string">"javascript.enabled"</span>, <span class="keyword">False</span>)</span><br></pre></td></tr></table></figure></p>
<p>全部限制对于加载速度的提升效果最好。如果能够限制，那么最好限制多种加载，这样的效果最好。<br>具体的加载速度提升还得看相应的网页，若网页的图片比较多，则限制图片的加载肯定效果很好。</p>
<p>参考链接：<a href="https://github.com/Santostang/PythonScraping/blob/master/%E7%AC%AC%E4%B8%80%E7%89%88/Cha%204%20-%E5%8A%A8%E6%80%81%E7%BD%91%E9%A1%B5%E6%8A%93%E5%8F%96/Cha%204%20-%E5%8A%A8%E6%80%81%E7%BD%91%E9%A1%B5%E6%8A%93%E5%8F%96.ipynb" target="_blank" rel="noopener">selenium</a></p>
<h1 id="2-典型应用"><a href="#2-典型应用" class="headerlink" title="2. 典型应用"></a>2. 典型应用</h1><h3 id="2-0-3-简单的爬虫"><a href="#2-0-3-简单的爬虫" class="headerlink" title="2.0.3. 简单的爬虫"></a>2.0.3. 简单的爬虫</h3><p>【用到的库】requests + bs4<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://github.com/Santostang/PythonScraping/blob/master/第一版/Cha 2 - 编写你的第一个网络爬虫/Cha 2 _章末实战.ipynb  </span></span><br><span class="line"><span class="comment">#!/usr/bin/python  </span></span><br><span class="line"><span class="comment"># coding: utf-8  </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> requests  </span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup   <span class="comment">#从bs4这个库中导入BeautifulSoup  </span></span><br><span class="line"><span class="comment"># 第一步：获取页面  </span></span><br><span class="line">link = <span class="string">"http://www.santostang.com/"</span>  </span><br><span class="line">headers = &#123;<span class="string">'User-Agent'</span> : <span class="string">'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'</span>&#125;  </span><br><span class="line">r = requests.get(link, headers= headers) <span class="comment"># requests的headers伪装成浏览器访问。r是requests的Response回复对象。  </span></span><br><span class="line"><span class="comment"># 第二步：提取需要的数据  </span></span><br><span class="line">soup = BeautifulSoup(r.text, <span class="string">"html.parser"</span>)      <span class="comment"># 使用BeautifulSoup解析这段网页。把HTML代码转化为soup对象。r.text是获取的网页内容代码  </span></span><br><span class="line">title = soup.find(<span class="string">"h1"</span>, class_=<span class="string">"post-title"</span>).a.text.strip() <span class="comment"># 提取第一篇文章的标题  </span></span><br><span class="line"><span class="keyword">print</span> (title)  </span><br><span class="line"><span class="comment"># 第三步：存储数据  </span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'title_test.txt'</span>, <span class="string">"a+"</span>) <span class="keyword">as</span> f:  </span><br><span class="line">    f.write(title)  </span><br><span class="line">    </span><br><span class="line">```  </span><br><span class="line"><span class="comment">### 2.0.4. 爬取豆瓣电影TOP250  </span></span><br><span class="line">【用到的库】requests + bs4  </span><br><span class="line">获取豆瓣电影TOP250的所有电影的名称  </span><br><span class="line">网页地址为：https://movie.douban.com/top250  </span><br><span class="line">第一页有<span class="number">25</span>个电影  </span><br><span class="line">获取所有的<span class="number">250</span>页电影  </span><br><span class="line">总共<span class="number">10</span>页的内容  </span><br><span class="line">第二页：https://movie.douban.com/top250? start=25  </span><br><span class="line">第三页：https://movie.douban.com/top250? start=50  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```py  </span><br><span class="line"><span class="keyword">import</span> requests  </span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup  </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_movies</span><span class="params">()</span>:</span>  </span><br><span class="line">    headers = &#123;  </span><br><span class="line">    <span class="string">'user-agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.82 Safari/537.36'</span>,  </span><br><span class="line">    <span class="string">'Host'</span>: <span class="string">'movie.douban.com'</span>  </span><br><span class="line">    &#125;  </span><br><span class="line">    movie_list = []  </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">10</span>):  </span><br><span class="line">        link = <span class="string">'https://movie.douban.com/top250?start='</span> + str(i * <span class="number">25</span>)  </span><br><span class="line">        r = requests.get(link, headers=headers, timeout= <span class="number">10</span>)  </span><br><span class="line">        <span class="keyword">print</span> (str(i+<span class="number">1</span>),<span class="string">"页响应状态码:"</span>, r.status_code)  </span><br><span class="line">   </span><br><span class="line">        soup = BeautifulSoup(r.text, <span class="string">"lxml"</span>)  </span><br><span class="line">        div_list = soup.find_all(<span class="string">'div'</span>, class_=<span class="string">'hd'</span>)  </span><br><span class="line">        <span class="keyword">for</span> each <span class="keyword">in</span> div_list:  </span><br><span class="line">            movie = each.a.span.text.strip()  </span><br><span class="line">            movie_list.append(movie)  </span><br><span class="line">    <span class="keyword">return</span> movie_list  </span><br><span class="line">   </span><br><span class="line">movies = get_movies()  </span><br><span class="line"><span class="keyword">print</span> (movies)  </span><br><span class="line"><span class="comment"># 原文有误  </span></span><br><span class="line"><span class="comment"># 用 ]: 便于在 ipynb 中查找下一项  </span></span><br><span class="line">```  </span><br><span class="line">参考链接：豆瓣电影(https://github.com/Santostang/PythonScraping/blob/master/第一版/Cha <span class="number">3</span> -静态网页抓取/Cha <span class="number">3</span> _章末实战.ipy)  </span><br><span class="line"></span><br><span class="line">进阶问题：获取TOP <span class="number">250</span>电影的英文名、港台名、导演、主演、上映年份、电影分类以及评分。  </span><br><span class="line"><span class="comment">### 2.0.5. 爬取动态网页  </span></span><br><span class="line">【用到的库】requests + json  </span><br><span class="line">AJAX加载的动态网页，有两种爬取方法：  </span><br><span class="line">（<span class="number">1</span>）通过浏览器审查元素解析地址。  </span><br><span class="line">（<span class="number">2</span>）通过Selenium模拟浏览器抓取。  </span><br><span class="line"></span><br><span class="line">两个特别重要的变量，即offset和limit。  </span><br><span class="line">limit：每一页评论数量的最大值  </span><br><span class="line">offset：本页的第一条评论是总的第几条  </span><br><span class="line"></span><br><span class="line">```py  </span><br><span class="line"><span class="keyword">import</span> requests  </span><br><span class="line"><span class="keyword">import</span> json  </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">single_page_comment</span><span class="params">(link)</span>:</span>  </span><br><span class="line">    headers = &#123;<span class="string">'User-Agent'</span> : <span class="string">'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'</span>&#125;  </span><br><span class="line">    r = requests.get(link, headers= headers)  </span><br><span class="line">    <span class="comment"># 获取 json 的 string  </span></span><br><span class="line">    json_string = r.text  </span><br><span class="line">    json_string = json_string[json_string.find(<span class="string">'&#123;'</span>):<span class="number">-2</span>]  </span><br><span class="line">    json_data = json.loads(json_string) <span class="comment"># 使用json.loads()把字符串格式的响应体数据转化为json数据  </span></span><br><span class="line">    comment_list = json_data[<span class="string">'results'</span>][<span class="string">'parents'</span>] <span class="comment"># json数据的结构提取  </span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> eachone <span class="keyword">in</span> comment_list:  </span><br><span class="line">        message = eachone[<span class="string">'content'</span>]  </span><br><span class="line">        <span class="keyword">print</span> (message)  </span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">4</span>):  </span><br><span class="line">    link1 = <span class="string">"https://api-zero.livere.com/v1/comments/list?callback=jQuery112407875296433383039_1506267778283&amp;limit=10&amp;offset="</span>  </span><br><span class="line">    link2 = <span class="string">"&amp;repSeq=3871836&amp;requestPath=%2Fv1%2Fcomments%2Flist&amp;consumerSeq=1020&amp;livereSeq=28583&amp;smartloginSeq=5154&amp;_=1506267778285"</span>  </span><br><span class="line">    page_str = str(page)  </span><br><span class="line">    link = link1 + page_str + link2  </span><br><span class="line">    <span class="keyword">print</span> (link)  </span><br><span class="line">    single_page_comment(link)  </span><br><span class="line">```  </span><br><span class="line">参考链接：  </span><br><span class="line">https://github.com/Santostang/PythonScraping/blob/master/第一版/Cha <span class="number">4</span> -动态网页抓取/Cha <span class="number">4</span> -动态网页抓取.ipynb  </span><br><span class="line"><span class="comment">### 2.0.6. 通过Selenium模拟浏览器抓取  </span></span><br><span class="line">```py</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line">driver = webdriver.Firefox()</span><br><span class="line">driver.get(<span class="string">"https://www.dianping.com/search/category/7/10/p1"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果运行之后，发现程序报错：</span></span><br><span class="line"><span class="comment">#     selenium.common.exceptions.WebDriverException: Message: 'geckodriver' executable needs to be in PATH.</span></span><br><span class="line"><span class="comment"># 可以到https://github.com/mozilla/geckodriver/releases下载最新版的geckodriver，解压后可以放在Python安装目录（可能是Script子文件夹）下（可能需并放在环境变量的PATH中）。</span></span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.firefox.firefox_binary <span class="keyword">import</span> FirefoxBinary   </span><br><span class="line">caps = webdriver.DesiredCapabilities().FIREFOX</span><br><span class="line">caps[<span class="string">"marionette"</span>] = <span class="keyword">False</span></span><br><span class="line">   </span><br><span class="line">path =  <span class="string">r'D:\\Program Files\\Mozilla Firefox\\firefox.exe'</span></span><br><span class="line">binary = FirefoxBinary(path) <span class="comment"># Firefox程序的地址  </span></span><br><span class="line">driver = webdriver.Firefox(firefox_binary=binary, capabilities=caps)</span><br><span class="line">driver.get(<span class="string">"http://www.santostang.com/2017/03/02/hello-world/"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    load_more = driver.find_element_by_css_selector(<span class="string">'div.tie-load-more'</span>)   <span class="comment"># 更多或下一页</span></span><br><span class="line">    load_more.click()            <span class="comment"># 模拟单击</span></span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    <span class="keyword">pass</span>    </span><br><span class="line">comments = driver.find_elements_by_css_selector(<span class="string">'div.bdy- inner'</span>) </span><br><span class="line">time.sleep(<span class="number">5</span>)  </span><br><span class="line"></span><br><span class="line">user = driver.find_element_by_name(<span class="string">"username"</span>)  <span class="comment">#找到用户名输入框</span></span><br><span class="line">user.clear  <span class="comment">#清除用户名输入框内容</span></span><br><span class="line">user.send_keys(<span class="string">"1234567"</span>)  <span class="comment">#在框中输入用户名</span></span><br><span class="line">pwd = driver.find_element_by_name(<span class="string">"password"</span>)  <span class="comment">#找到密码输入框</span></span><br><span class="line">pwd.clear  <span class="comment">#清除密码输入框内容</span></span><br><span class="line">pwd.send_keys(<span class="string">"＊＊＊＊＊＊"</span>)    <span class="comment">#在框中输入密码</span></span><br><span class="line">driver.find_element_by_id(<span class="string">"loginBtn"</span>).click()  <span class="comment">#单击登录</span></span><br></pre></td></tr></table></figure></p>
<h3 id="2-0-7-深圳短租"><a href="#2-0-7-深圳短租" class="headerlink" title="2.0.7. 深圳短租"></a>2.0.7. 深圳短租</h3><p>目的：获取Airbnb深圳前20页的短租房源的名称、价格、评价数量、房屋类型、床数量和房客数量。监控和了解竞争对手的房屋名称和价格，让自己的房子有竞争力。<br>网址：<a href="https://zh.airbnb.com/s/Shenzhen--China?page=1" target="_blank" rel="noopener">https://zh.airbnb.com/s/Shenzhen--China?page=1</a></p>
<p>4.4.1 网站分析</p>
<p>一个房子的所有数据。地址为：div.infoContainer_v72lrv。<br>价格数据，地址为：div.priceContainer_4ml1ll<br>评价数据，地址为：span.text_5mbkop-o_O-size_micro_16wifzf-o_O-inline_g86r3e<br>房屋名称数据，地址为：div.listingNameContainer_kq7ac0-o_O-ellipsized_1iurgbx<br>房间类型、床数量和房客数量，地址为：span.detailWithoutWrap_j1kt73</p>
<p>4.4.2 项目实践<br>用Selenium获取Airbnb第一页的数据。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.firefox.firefox_binary <span class="keyword">import</span> FirefoxBinary</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">caps = webdriver.DesiredCapabilities().FIREFOX</span><br><span class="line">caps[<span class="string">"marionette"</span>] = <span class="keyword">True</span></span><br><span class="line">binary = FirefoxBinary(<span class="string">r'C:\Program Files\Firefox Developer Edition\firefox.exe'</span>)</span><br><span class="line"><span class="comment"># 把上述地址改成你电脑中Firefox程序的地址 </span></span><br><span class="line"><span class="comment"># 如果没改，会出现selenium.common.exceptions.SessionNotCreatedException: Message: Unable to find a matching set of capabilities</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#用 selenium 的 driver 来启动 firefox</span></span><br><span class="line">driver = webdriver.Firefox(firefox_binary=binary, capabilities=caps)</span><br><span class="line"><span class="comment">#在虚拟浏览器中打开 Airbnb 页面。使用Selenium打开该页面</span></span><br><span class="line">driver.get(<span class="string">"https://zh.airbnb.com/s/Shenzhen--China?page=1"</span>)</span><br><span class="line"></span><br><span class="line">time.sleep(<span class="number">20</span>)</span><br><span class="line"><span class="comment">#找到页面中所有的出租房。用Selenium的css selector获取所有房屋的div数据</span></span><br><span class="line">rent_list = driver.find_elements_by_css_selector(<span class="string">'div._1788tsr0'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#对于每一个出租房</span></span><br><span class="line"><span class="keyword">for</span> eachhouse <span class="keyword">in</span> rent_list:</span><br><span class="line">    <span class="comment">#找到评论数量</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        comment = eachhouse.find_element_by_css_selector(<span class="string">'span._gb7fydm'</span>)</span><br><span class="line">        comment = comment.text</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        comment = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#找到价格</span></span><br><span class="line">    price = eachhouse.find_element_by_css_selector(<span class="string">'span._hylizj6'</span>)</span><br><span class="line">    price = price.text[<span class="number">4</span>:]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#找到名称</span></span><br><span class="line">    name = eachhouse.find_element_by_css_selector(<span class="string">'div._ew0cqip'</span>)</span><br><span class="line">    name = name.text</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#找到房屋类型，大小</span></span><br><span class="line">    details = eachhouse.find_elements_by_css_selector(<span class="string">'div._saba1yg small div span'</span>)</span><br><span class="line">    details = details[<span class="number">0</span>].text</span><br><span class="line">    house_type = details.split(<span class="string">" · "</span>)[<span class="number">0</span>]</span><br><span class="line">    bed_number = details.split(<span class="string">" · "</span>)[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">print</span> (comment, price, name, house_type, bed_number)</span><br></pre></td></tr></table></figure>
<p>进阶：将Selenium的控制CSS加载、控制图片加载和控制JavaScript加载加入本实践项目的代码中，从而提升爬虫的速度。</p>
<p><a href="https://github.com/Santostang/PythonScraping/blob/master/%E7%AC%AC%E4%B8%80%E7%89%88/Cha%204%20-%E5%8A%A8%E6%80%81%E7%BD%91%E9%A1%B5%E6%8A%93%E5%8F%96/Cha%204%20_%E7%AB%A0%E6%9C%AB%E5%AE%9E%E6%88%98.ipynb" target="_blank" rel="noopener">https://github.com/Santostang/PythonScraping/blob/master/%E7%AC%AC%E4%B8%80%E7%89%88/Cha%204%20-%E5%8A%A8%E6%80%81%E7%BD%91%E9%A1%B5%E6%8A%93%E5%8F%96/Cha%204%20_%E7%AB%A0%E6%9C%AB%E5%AE%9E%E6%88%98.ipynb</a></p>
<h1 id="3-工具及资源列表"><a href="#3-工具及资源列表" class="headerlink" title="3. 工具及资源列表"></a>3. 工具及资源列表</h1><h2 id="3-1-网络下载"><a href="#3-1-网络下载" class="headerlink" title="3.1. 网络下载"></a>3.1. 网络下载</h2><p>Anaconda： <a href="https://www.continuum.io/downloads" target="_blank" rel="noopener">https://www.continuum.io/downloads</a> 。<br>Robomongo：MongoDB数据库的可视化管理工具。<br>Redis Desktop Manager：Redis的可视化管理工具。  </p>
<ul>
<li>下载 <a href="https://redisdesktop.com/download" target="_blank" rel="noopener">https://redisdesktop.com/download</a>  </li>
<li>界面 <a href="https://res.weread.qq.com/wrepub/epub_928559_154" target="_blank" rel="noopener">https://res.weread.qq.com/wrepub/epub_928559_154</a>  </li>
</ul>
<p>Alt + Enter jupyter快捷键  </p>
<h2 id="3-2-书籍辅助"><a href="#3-2-书籍辅助" class="headerlink" title="3.2. 书籍辅助"></a>3.2. 书籍辅助</h2><ul>
<li>Python网络爬虫从入门到实践，唐松  </li>
</ul>
<ul>
<li>Github：<a href="https://github.com/Santostang/PythonScraping" target="_blank" rel="noopener">https://github.com/Santostang/PythonScraping</a><br>百度网：<a href="http://pan.baidu.com/s/1c2w9rck" target="_blank" rel="noopener">http://pan.baidu.com/s/1c2w9rck</a><br>书本对应的Python网络爬虫的教学:<a href="http://www.santostang.com" target="_blank" rel="noopener">www.santostang.com</a><br>网站不会更改设计和框架，本书的网络爬虫代码可以一直使用<br>作者自己的博客网站，可以避免一些法律上的风险  <h2 id="3-3-端口"><a href="#3-3-端口" class="headerlink" title="3.3. 端口"></a>3.3. 端口</h2>jupyter：8888  <h1 id="4-库"><a href="#4-库" class="headerlink" title="4. 库"></a>4. 库</h1><h2 id="4-1-Python第三方库"><a href="#4-1-Python第三方库" class="headerlink" title="4.1. Python第三方库"></a>4.1. Python第三方库</h2>基本格式：（安装时，把name替换为要安装的第三方库）  </li>
<li><code>pip install name</code>  </li>
<li><code>pip install -i https://pypi.tuna.tsinghua.edu.cn/simple name</code>  </li>
</ul>
<ul>
<li>科学计算的包，如Numpy、Scipy、Pandas和Matplotlib。  </li>
<li>机器学习、生物医学和天体物理学计算，如Scikit-Learn、BioPython。  </li>
<li>获取网页：requests、urllib、selenium  </li>
<li>解析数据：lxml、bs4的BeautifulSoup、re(标准库)  </li>
<li>存储数据：MySQL、MongoDB  <h1 id="5-附录"><a href="#5-附录" class="headerlink" title="5. 附录"></a>5. 附录</h1><h1 id="6-单项分析"><a href="#6-单项分析" class="headerlink" title="6. 单项分析"></a>6. 单项分析</h1><h2 id="6-1-是什么"><a href="#6-1-是什么" class="headerlink" title="6.1. 是什么"></a>6.1. 是什么</h2>Anaconda：Python开发集成环境。南美洲的巨蟒。自带Python、pip和Jupyter。<br>第三方库：可理解为供用户调用的代码组合。在安装某个库之后，可以直接调用其中的功能，使得我们不用一个代码一个代码地实现某个功能。<br>DT（Data Technology，数据技术）  </li>
</ul>
<p>命令提示符。输入一些命令后，可执行对系统的管理。 Windows的cmd，开始按钮→cmd。Mac的terminal。应用程序→terminal。<br>爬虫：<br>pip：Python安装各种第三方库（package）的工具。<br>Python：蟒蛇<br>数据交换：网站与用户的沟通本质。  </p>
<p>print<br>代码缩进：代码要按照结构以Tab键或者4个空格进行缩进严格缩进<br>注释：#  </p>
<p>Python不需要在使用之前声明需要使用的变量和类别。<br>字符串（string）：单引号（’）或双引号（”）<br>连接字符串: +  </p>
<p>数字（Number）：数字用来存储数值<br>整数（int）<br>浮点数（float）：由整数和小数部分组成。  </p>
<p>列表（list）:能够包含任意种类的数据类型和任意数量。<br>创建列表非常容易，只要把不同的变量放入方括号中，并用逗号分隔即可，例如list0 = [“a”,2,”c”,4]<br>增删查改、索引、切片<br>字典（Dictionaries）：一种可变容器模型。<br>键（key）和值（value）。key必须唯一，但是值不用。值也可以取任何数据类型。<br>遍历<br>条件语句：满足条件的时候才执行某部分代码。条件为布尔值，也就是只有True和False两个值。<br>    当if判断条件成立时才执行后面的语句；当条件不成立的时候，执行else后面的语句<br>    如果需要判断的有多种条件，就需要用到elif<br>无序：字典<br>有序：列表、元组<br>对象有两种，即可更改（mutable）与不可更改（immutable）对象。在Python中，strings、tuples和numbers是不可更改对象，而list、dict等是可更改对象。  </p>
<p>循环语句：多次执行一个代码片段。<br>循环分为for循环和while循环。<br>for循环：在一个给定的顺序下重复执行。<br>while循环：不断重复执行，只要能满足一定条件。  </p>
<p>函数<br>代码庞大复杂时，使得代码易读，可重复使用，并且容易调整顺序。<br>函数的参数与返回值  </p>
<p>面向过程编程：根据业务逻辑从上到下写代码，最容易被初学者接受。<br>函数式编程：把某些功能封装到函数中，需要用时可以直接调用，不用重复撰写。函数式的编程方法节省了大量时间。只需要写清楚输入和输出变量并执行函数即可。<br>面向对象编程：把函数进行分类和封装后放入对象中，使得开发更快、更强。首先要创建封装对象，然后还要通过对象调用被封装的内容。在某些应用场景下，面向对象编程能够显示出更大的优势。<br>如果各个函数之间独立且无共用的数据，就选用函数式编程；如果各个函数之间有一定的关联性，选用面向对象编程比较好。<br>特性与行为，属性和方法<br>面向对象的两大特性：封装和继承。<br>封装：把内容封装好，再调用封装好的内容。使用构造方法将内容封装到对象中，然后通过对象直接或self间接获取被封装的内容。<br>继承：以普通的类为基础建立专门的类对象。子继承了父的某些特性。将多个类共有的方法提取到父类中，子类继承父类中的方法即可，不必一一实现每个方法。  </p>
<p>【状态码】<br>200，请求成功<br>4xx，客户端错误<br>5xx，服务器错误<br>【请求头】<br>Headers：提供了关于请求、响应或其他发送实体的信息。<br>如果没有指定请求头或请求的请求头和实际网页不一致，就可能无法返回正确的结果。  </p>
<p>Chrome浏览器的检查。单击需要请求的网页，在Headers中可以看到Requests Headers的详细信息。  </p>
<p>请求头的信息为：<br>GET / HTTP/1.1<br>Host: <a href="http://www.santostang.com" target="_blank" rel="noopener">www.santostang.com</a><br>Connection: keep-alive<br>Upgrade-Insecure-Requests: 1<br>User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.98 Safari/537.36<br>Accept:<br>text/html, application/xhtml+xml, application/xml; q=0.9, image/webp, <em>/</em>; q=0.8 Accept-Encoding: gzip, deflate, sdch<br>Accept-Language: en-US, en; q=0.8, zh-CN; q=0.6, zh; q=0.4, zh-TW; q=0.2  </p>
<p>GET请求，密码会显示在URL中，非常不安全。<br>POST请求，<br>【动态网页】<br>AJAX（Asynchronous Javascript And XML，异步JavaScript和XML），一种异步更新技术。<br>单击“更多”，url地址没有任何改变，有新内容加载出来。<br>数据不会出现在网页源代码中。但是有JavaScript代码。<br>最后呈现出来的数据是通过JavaScript加载的。  </p>
<p>通过在后台与服务器进行少量数据交换就可以使网页实现异步更新。<br>在不重新加载整个网页的情况下对网页的某部分进行更新。<br>减少了网页重复内容的下载<br>节省了流量<br>更小、更快、更友好  </p>
<p>传统的网页必须重载整个网页页面  </p>
<p>动态网页的例子<br><a href="http://www.santostang.com/2018/07/04/hello-world/" target="_blank" rel="noopener">http://www.santostang.com/2018/07/04/hello-world/</a>  </p>
<p>页面下面的评论用JavaScript加载。评论数据没法在在网页源代码找到。  </p>
<h3 id="6-1-1-Selenium"><a href="#6-1-1-Selenium" class="headerlink" title="6.1.1. Selenium"></a>6.1.1. Selenium</h3><p>Selenium官方文档：<a href="http://selenium-python.readthedocs.io/index.html。" target="_blank" rel="noopener">http://selenium-python.readthedocs.io/index.html。</a>  </p>
<p>Selenium要在整个网页加载出来后才开始爬取内容，速度往往较慢。  </p>
<p>Selenium可以实现的功能：<br>操作元素对浏览器中的网页进行各种操作，包括登录。<br>模拟鼠标单击、双击、拖拽<br>获得网页中各个元素的大小<br>模拟键盘<br>浏览器渲染引擎。直接用浏览器在显示网页时解析HTML、应用CSS样式并执行JavaScript的语句。Selenium使用浏览器渲染，数据已经渲染到了HTML代码中。用chrome定位标签即可。<br>用脚本控制浏览器操作。Python的Selenium库模拟浏览器完成抓取。<br>Selenium：用于Web应用程序测试的工具。Selenium测试直接运行在浏览器中，浏览器自动按照脚本代码做出单击、输入、打开、验证等操作，就像真正的用户在操作一样。</p>
<p>用Selenium控制浏览器加载的内容，可加快Selenium的爬取速度。此类常用的方法有：<br>（1）控制CSS的加载。<br>（2）控制图片文件的显示。<br>（3）控制JavaScript的运行。<br>（1）控制CSS。因为抓取过程中仅仅抓取页面的内容，CSS样式文件是用来控制页面的外观和元素放置位置的，对内容并没有影响，所以我们可以限制网页加载CSS，从而减少抓取时间。  </p>
<p>支持多个浏览器的调用：IE（7、8、9、10、11）、Firefox、Safari、Google Chrome、Opera等。最常用的是Firefox。</p>
<h3 id="6-1-2-正则表达式"><a href="#6-1-2-正则表达式" class="headerlink" title="6.1.2. 正则表达式"></a>6.1.2. 正则表达式</h3><p>元字符：<a href="https://res.weread.qq.com/wrepub/epub_928559_43" target="_blank" rel="noopener">https://res.weread.qq.com/wrepub/epub_928559_43</a> 问加合星<br><a href="https://regex101.com/" target="_blank" rel="noopener">https://regex101.com/</a><br>正则表达式：字符串操作的逻辑公式。用事先定义好的特定字符组合成规则字符串，用该规则字符串来过滤字符串。<br>正则表达式可以迅速地用极简单的方式达到字符串的复杂控制。</p>
<h2 id="6-2-为什么"><a href="#6-2-为什么" class="headerlink" title="6.2. 为什么"></a>6.2. 为什么</h2><p>为什么、好处、重要性、作用、意义、优势、不足、历史、现状、趋势、大背景。  </p>
<h3 id="6-2-1-大数据及爬虫"><a href="#6-2-1-大数据及爬虫" class="headerlink" title="6.2.1. 大数据及爬虫"></a>6.2.1. 大数据及爬虫</h3><p>技术创新驱动变革的潮流。<br>数据量爆发式增长的互联网时代。<br>大数据分析的火热。<br>大数据成为业界与学术界最火热的话题之一。<br>数据已经成为每个公司极为重要的资产。<br>互联网大量的公开数据为个人和公司提供了以往想象不到的可以获取的数据量。<br>网络爬虫技术是大数据分析的第一环。有助于获取有用的公开数据集。<br>理解了信息的获取、存储和整理，才有可能系统地收集和应用不同源头和千变万化的网站信息。<br>DT的核心是从信息的源头去理解和分析，以做出能打动对方的行动决策方案。<br>由谷歌搜索到现在的大数据时代，爬虫技术的重要性和广泛性一直很突出。<br>爬取目标网站的资料、分析和建立应用。 获取数据自动、实时、及时、省时。<br>电商市场的重要性日益凸显。了解对手的产品特点、价格以及销量情况，及时跟进产品开发进度和营销策略，从而知己知彼，赢得竞争。过去，两个痛点——无法自动化和无法实时获取。产品研发部门会手动访问一个个电商产品页面，人工复制并粘贴到Excel表格中，制作竞品分析报告。但是这种重复性的手动工作不仅浪费宝贵的时间，一不留神复制少了一个数字还会导致数据错误；对手产品的销量则是由某一家咨询公司提供报告，每周一次，但是报告缺乏实时性，难以针对快速多变的市场及时调整价格和营销策略。<br>学会一项新的技术<br>第一方企业（也就是拥有这些数据的企业）做出更好的决策<br>第三方企业也可从中受益<br>数据共享<br>Python：热门的开源软件（这意味着有人源源不断地开发更新且更强大的包给你用）<br>Python：简单、简洁、易学、有效、可扩展性的计算机语言。 最受欢迎的程序语言之一。 强大而丰富的库。<br>C语言：底层，学习成本大。  </p>
<h3 id="6-2-2-Jupyter"><a href="#6-2-2-Jupyter" class="headerlink" title="6.2.2. Jupyter"></a>6.2.2. Jupyter</h3><p>为什么推荐大家使用Jupyter学习和编写Python脚本呢？<br>Jupyter：交互式编程和展示功能。<br>分段执行，编写和测试时边看边写，加快调试速度。<br>能够把运行和输出的结果保存下来，下次打开这个Notebook时也可以看到之前运行的结果。<br>还可以添加各种元素，比如图片、视频、链接等，同时还支持Markdown，可以充当PPT使用。  </p>
<h2 id="6-3-如何"><a href="#6-3-如何" class="headerlink" title="6.3. 如何"></a>6.3. 如何</h2><p>不断解决遇到的疑惑。<br>科技如何给大家带来实效<br>数据的存储对公司有什么影响<br>如何存储数据⇒高效利用 方便对接其他部门和业务<br>如何使用淘宝网上所有绿色产品（如空气净化器）的销量数据来做潜在市场评估<br>如何一直高效率、持续不断地从日新月异的网站中获取信息  </p>
<h3 id="6-3-1-快捷"><a href="#6-3-1-快捷" class="headerlink" title="6.3.1. 快捷"></a>6.3.1. 快捷</h3><p>对初学者来说，使用BeautifulSoup从网页中提取需要的数据更加简单易用。  </p>
<p>谷歌的有效信息检索速度比百度快<br>Stack Overflow上的回答可以比较快地解决问题<br>最新最好的回答很有可能是英文的  </p>
<h3 id="6-3-2-获取动态网页的真实地址"><a href="#6-3-2-获取动态网页的真实地址" class="headerlink" title="6.3.2. 获取动态网页的真实地址"></a>6.3.2. 获取动态网页的真实地址</h3><p>Chrome浏览器的检查（审查元素）功能：浏览器右键⇒检查⇒Network⇒XHR或JS选项<br>Network：显示浏览器从网页服务器中得到的所有文件。一般这些数据以json文件格式获取。<br>在Network选项卡下，找到真正的评论文件。<br>单击Preview标签即可查看数据。可以按 ctrl+F 进行查找。顶部search也可以。<br>Elements会出现相应的code所在的地方。  </p>
<h2 id="6-4-应用场景"><a href="#6-4-应用场景" class="headerlink" title="6.4. 应用场景"></a>6.4. 应用场景</h2><h3 id="6-4-1-爬虫"><a href="#6-4-1-爬虫" class="headerlink" title="6.4.1. 爬虫"></a>6.4.1. 爬虫</h3><p>一些附加值更高的“事”，如人工智能、统计建模等。<br>机器学习和统计算法分析<br>在营销领域可以帮助企业做好4P（Product：产品创新，Place：智能选址，Price：动态价格，Promotion：数据驱动的营销活动）<br>在金融领域，数据驱动的征信等应用会带来越来越大的价值。<br>公开数据的应用价值<br>所有网络数据<br>社交媒体的每一条发帖。社交媒体在用户生态圈的自我交互下产生大量文本、图片和视频数据。<br>团购网站的价格及点评。电商商产品的描述、价格<br>招聘网站的招聘信息<br>搜索引擎从数据库中提取搜索结果  </p>
<h2 id="6-5-注意事项"><a href="#6-5-注意事项" class="headerlink" title="6.5. 注意事项"></a>6.5. 注意事项</h2><p>爬虫有哪些潜在的法律纠纷、公司的爬虫合不合法 。<br>建立共利的互联网环境，不能把爬虫作为窃取数据的工具。<br>爬虫必须在合情、合法、合理的情况下获取和应用。<br>尊重数据供应者的知识产权和正常运作才能产生长久共利的环境。<br>保障对方平台的正常运作是每个程序员都应当做到的<br>法律：<br>互联网世界已经通过自身的协议建立起一定的道德规范（Robots协议）。该协议是国际互联网界通行的道德规范，虽然没有写入法律，但是每一个爬虫都应该遵守这项协议。<br>法律部分还在建立和完善中。<br>如果抓取的数据属于个人使用或科研范畴，基本不存在问题。当你爬取网站数据时，无论是否仅供个人使用，都应该遵守Robots协议。<br>而如果数据属于商业盈利范畴，就要就事而论，有可能属于违法行为，也有可能不违法。<br>大部分网站不欢迎使用程序进行登录，因为需要登录才能查看的数据不属于公开数据。最好不要使用此程序获取非公开数据或批量注册，若出现了问题，可能需负法律责任。  </p>
<p>建议使用API。  </p>
<p>Robots协议<br>Robots协议（爬虫协议）的全称是“网络爬虫排除标准”（Robots Exclusion Protocol），网站通过Robots协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取。  </p>
<p><a href="https://www.taobao.com/robots.txt。" target="_blank" rel="noopener">https://www.taobao.com/robots.txt。</a><br>Allow开头的URL是允许robot访问的。例如，Allow:/article允许百度爬虫引擎访问/article.htm、/article/12345.com等。<br>Disallow不允许百度爬虫引擎访问的。例如，Disallow:/product/不允许百度爬虫引擎访问/product/12345.com等。<br>Disallow:/禁止百度爬虫访问除了Allow规定页面外的其他所有页面。  </p>
<p>taobao的robots.txt对不同的搜索引擎所允许爬行范围不同。/product项对应淘宝内部的产品信息。当在搜索框中搜索“淘宝iphone7”的时候，Google可搜到淘宝中的产品，而百度不能。  </p>
<p>过于快速或者频密的网络爬虫都会对服务器产生巨大的压力。→调集资源限制爬虫，保护用户的流量和减少有价值数据的流失。  </p>
<p>反爬方维权：网站封锁你IP，法律行动。  </p>
<p>将请求的速度限定在一个合理的范围之内。  </p>
<p>每年的三月份会迎来一个爬虫高峰期。因为有大量的大学生五月份交论文，在写论文的时候会选择爬取数据，也就是3月份爬取数据，4月份分析数据，5月份交论文。  </p>
<p>2007年，爱帮网利用垂直搜索技术获取了大众点评网上的商户简介和消费者点评，并且直接大量使用，大众点评网多次要求爱帮网停止使用这些内容，而爱帮网以自己是使用垂直搜索获得的数据为由，拒绝停止抓取大众点评网上的内容，并且质疑大众点评网对这些内容所享有的著作权。为此，双方开打了两场官司。2011年1月，北京海淀法院做出判决：爱帮网侵犯大众点评网著作权成立，应当停止侵权并赔偿大众点评网经济损失和诉讼必要支出。<br>2013年10月，百度诉360违反Robots协议。百度方面认为，360违反了Robots协议，擅自抓取、复制百度网站内容并生成快照向用户提供。2014年8月7日，北京市第一中级人民法院做出一审判决，法院认为被告奇虎360的行为违反了《反不正当竞争法》相关规定，应赔偿原告百度公司70万元。<br>虽然说大众点评上的点评数据、百度知道的问答由用户创建而非企业，但是搭建平台需要投入运营、技术和人力成本，所以平台拥有对数据的所有权、使用权和分发权。【网站的知识产权】<br>以上两起败诉告诉我们，在爬取网站的时候需要限制自己的爬虫，遵守Robots协议和约束网络爬虫程序的速度。如果违反了这些规定，很可能会吃官司，并且败诉的概率相当高。  </p>
<h1 id="7-多项关系"><a href="#7-多项关系" class="headerlink" title="7. 多项关系"></a>7. 多项关系</h1><h2 id="7-1-流程图"><a href="#7-1-流程图" class="headerlink" title="7.1. 流程图"></a>7.1. 流程图</h2><p>具体步骤及各步骤之间的关系。  </p>
<h3 id="7-1-1-网络爬虫、数据采集"><a href="#7-1-1-网络爬虫、数据采集" class="headerlink" title="7.1.1. 网络爬虫、数据采集"></a>7.1.1. 网络爬虫、数据采集</h3><p>获【取】网页、解【析】网页（提取数据）、【存】储数据、整【理】。  </p>
<ul>
<li>获取网页：给一个网址发送请求，该网址会返回整个网页的数据。类似于在浏览器中键入网址并按回车键，然后可以看到网站的整个页面。  </li>
<li>解析网页：从整个网页的数据中提取想要的数据。类似于在浏览器中看到网站的整个页面，但是你想找的是产品的价格，价格就是你想要的数据。  </li>
<li>存储数据：把数据存储下来。  </li>
</ul>
<p>三个流程的技术实现:  </p>
<ul>
<li>获取网页<br>获取网页的基础技术：request、urllib和selenium（模拟浏览器）。<br>获取网页的进阶技术：多进程多线程抓取、登录抓取、突破IP封禁和服务器抓取。  </li>
<li>解析网页<br>解析网页的基础技术：re正则表达式、BeautifulSoup和lxml。<br>解析网页的进阶技术：解决中文乱码。  </li>
<li>存储数据<br>存储数据的基础技术：存入txt文件和存入csv文件。<br>存储数据的进阶技术：存入MySQL数据库和存入MongoDB数据库。  </li>
</ul>
<h2 id="7-2-分类树"><a href="#7-2-分类树" class="headerlink" title="7.2. 分类树"></a>7.2. 分类树</h2><h2 id="7-3-对比分析"><a href="#7-3-对比分析" class="headerlink" title="7.3. 对比分析"></a>7.3. 对比分析</h2><p>主要的解析器及其优缺点<br><a href="https://res.weread.qq.com/wrepub/epub_928559_44" target="_blank" rel="noopener">https://res.weread.qq.com/wrepub/epub_928559_44</a><br><a href="https://res.weread.qq.com/wrepub/epub_928559_49" target="_blank" rel="noopener">https://res.weread.qq.com/wrepub/epub_928559_49</a><br>使用lxml的解析器将会解析得更快。</p>
<h2 id="7-4-关系图"><a href="#7-4-关系图" class="headerlink" title="7.4. 关系图"></a>7.4. 关系图</h2><p>互联网的运作和结构<br>爬虫程序是收集信息的基础。  </p>
<p>==============================  </p>
<h1 id="8-元学习（与物）"><a href="#8-元学习（与物）" class="headerlink" title="8. 元学习（与物）"></a>8. 元学习（与物）</h1><p>起始、终止、空格和换行，循环次数<br>是啥 为啥 逻辑清晰、循序渐进 查阅此书<br>动其心者，当具有大本大源<br>不断学习新技术，自我提高，实现目标和理想。不断更新和进步：互联网科技、网站信息也随之不断改变。<br>不能应用的技术称为魔术，只能用于表演。<br>学习的道路没有什么捷径可走，唯一的方法就是不断尝试、不断失败、不断改进。<br>通过实战解决实际问题。问题及解决方案实践<br>增强学习效果<br>富有逻辑的框架解构学习。将网络爬虫技术进行框架性的解构<br>认真阅读、手输代码，反复练习，熟能生巧。提升你的编程能力和编程效率<br>从实践中检验自己学习了多少知识<br>进一步巩固<br>进阶问题<br>答案并不是唯一解，对比思路  </p>
<h1 id="9-个人提升（与人）"><a href="#9-个人提升（与人）" class="headerlink" title="9. 个人提升（与人）"></a>9. 个人提升（与人）</h1><p>了解技术团队的运作模式<br>向香港中文大学市场营销学的研究生讲解Python网络爬虫技术，让这些商科学生掌握一些大数据时代重要的技术能力。<br>KYM框架<br>Know Your Company（了解你的公司）<br>Know Your Competitor（了解你的竞争对手）<br>Know Your Customer（了解你的客户）  </p>
<h1 id="10-代码清单"><a href="#10-代码清单" class="headerlink" title="10. 代码清单"></a>10. 代码清单</h1><h2 id="10-1-基础语法"><a href="#10-1-基础语法" class="headerlink" title="10.1. 基础语法"></a>10.1. 基础语法</h2><h3 id="10-1-1-py"><a href="#10-1-1-py" class="headerlink" title="10.1.1. py"></a>10.1.1. py</h3><p>int(number)<br>float(number)<br>for key,value in dict.items()<br>Python 100例 <a href="https://www.w3cschool.cn/python/python-100-examples.html" target="_blank" rel="noopener">https://www.w3cschool.cn/python/python-100-examples.html</a>  </p>
<h4 id="10-1-1-1-类"><a href="#10-1-1-1-类" class="headerlink" title="10.1.1.1. 类"></a>10.1.1.1. 类</h4><pre><code class="py"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>:</span>   <span class="comment"># 创建类          </span>
    <span class="function"><span class="keyword">def</span> <span class="title">_init_</span><span class="params">(self,name,age)</span>:</span>  
        <span class="comment"># _init_()方法称为类的构造方法。会自动执行。初始化以及规定传递的参数。self后面的参数列表。实例则传给self  </span>
        <span class="comment"># self 仅在类的定义中使用。表示对整个传递来的对象进行操作。  </span>
        <span class="comment"># 运行时类中self.会自动转为传进来obInstance.进行运算，即self = obInstance【自我理解】  </span>

        self.name = name              
        self.age = age          
    <span class="function"><span class="keyword">def</span> <span class="title">detail</span><span class="params">(self)</span>:</span> <span class="comment">#通过self调用被封装的内容              </span>
        <span class="keyword">print</span> (self.name)              
        <span class="keyword">print</span> (self.age)      

obj1 = Person(<span class="string">'santos'</span>,<span class="number">18</span>)      
obj1.detail()  
<span class="comment"># Python将obj1传给self参数，'santos'和18传给类的构造方法_init_中的name和age  </span>

<span class="comment"># 猫可以：喵喵叫、吃、喝、拉、撒  </span>
<span class="comment"># 狗可以：汪汪叫、吃、喝、拉、撒  </span>

<span class="comment"># 如果用继承的思想，就可以写成：  </span>
<span class="comment"># 动物：吃喝拉撒  </span>
<span class="comment"># 猫：喵喵叫（猫继承动物的功能）  </span>
<span class="comment"># 狗：汪汪叫（狗继承动物的功能）  </span>

<span class="class"><span class="keyword">class</span> <span class="title">Animal</span>:</span>  
    <span class="function"><span class="keyword">def</span> <span class="title">eat</span><span class="params">(self)</span>:</span>  
        <span class="keyword">print</span> (<span class="string">"%s吃 "</span> %self.name)  
    <span class="function"><span class="keyword">def</span> <span class="title">drink</span><span class="params">(self)</span>:</span>  
        <span class="keyword">print</span> (<span class="string">"%s喝 "</span> %self.name)  
    <span class="function"><span class="keyword">def</span> <span class="title">shit</span><span class="params">(self)</span>:</span>  
        <span class="keyword">print</span> (<span class="string">"%s拉 "</span> %self.name)  
    <span class="function"><span class="keyword">def</span> <span class="title">pee</span><span class="params">(self)</span>:</span>  
        <span class="keyword">print</span> (<span class="string">"%s撒 "</span> %self.name)  
<span class="class"><span class="keyword">class</span> <span class="title">Cat</span><span class="params">(Animal)</span>:</span>  
    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,name)</span>:</span>  
        self.name = name  
    <span class="function"><span class="keyword">def</span> <span class="title">cry</span><span class="params">(self)</span>:</span>  
        <span class="keyword">print</span> (<span class="string">'喵喵叫'</span>)  
<span class="class"><span class="keyword">class</span> <span class="title">Dog</span><span class="params">(Animal)</span>:</span>  
    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,name)</span>:</span>  
        self.name = name  
    <span class="function"><span class="keyword">def</span> <span class="title">cry</span><span class="params">(self)</span>:</span>  
        <span class="keyword">print</span> (<span class="string">'汪汪叫'</span>)  
c1 = Cat(<span class="string">'小白家的小黑猫'</span>)  
c1.eat()  
c1.cry()  
d1 = Dog(<span class="string">'胖子家的小瘦狗'</span>)  
d1.eat()  

<span class="comment"># 小白家的小黑猫吃  </span>
<span class="comment"># 喵喵叫  </span>
<span class="comment"># 胖子家的小瘦狗吃  </span>
</code></pre>
<h3 id="10-1-2-函数、类，可变与不可变"><a href="#10-1-2-函数、类，可变与不可变" class="headerlink" title="10.1.2. 函数、类，可变与不可变"></a>10.1.2. 函数、类，可变与不可变</h3><pre><code class="py">a = <span class="number">1</span>  
<span class="function"><span class="keyword">def</span> <span class="title">fun</span><span class="params">(a)</span>:</span>  
    a = <span class="number">2</span>  
fun(a)  
<span class="keyword">print</span> (a)  
&gt;&gt;&gt;<span class="number">1</span>  

a为数字int，函数改变不了函数以外a的值。当一个引用传递给函数时，函数自动复制一份引用。函数里和函数外的引用是不一样的。  



a = []  
<span class="function"><span class="keyword">def</span> <span class="title">fun</span><span class="params">(a)</span>:</span>  
    a.append(<span class="number">1</span>)  
fun(a)  
<span class="keyword">print</span> (a)  
&gt;&gt;&gt;[<span class="number">1</span>]  
a为列表，函数将函数以外的a值改变了。函数内的引用指向的是可变对象列表a，函数内的列表a和函数外的列表a是同一个。  
<span class="class"><span class="keyword">class</span> <span class="title">Person</span>:</span>  
    name=<span class="string">"aaa"</span>  

p1=Person()  
p2=Person()  
p1.name=<span class="string">"bbb"</span>  
<span class="keyword">print</span> (p1.name)  
<span class="keyword">print</span> (p2.name)  
<span class="keyword">print</span> (Person.name)  

&gt;&gt;&gt;bbb  
&gt;&gt;&gt;aaa  
&gt;&gt;&gt;aaa  
p1.name=<span class="string">"bbb"</span>表示实例调用了类变量，其实就是函数传参的问题。p1.name一开始指向类变量name=<span class="string">"aaa"</span>，但是在实例的作用域里把类变量的引用改变了，就变成了一个实例变量，self.name不再引用Person的类变量name了。  
<span class="class"><span class="keyword">class</span> <span class="title">Person</span>:</span>  
    name=[]  

p1=Person()  
p2=Person()  
p1.name.append(<span class="number">1</span>)  
<span class="keyword">print</span> (p1.name)  
<span class="keyword">print</span> (p2.name)  
<span class="keyword">print</span> (Person.name)  
&gt;&gt;&gt;[<span class="number">1</span>]  
&gt;&gt;&gt;[<span class="number">1</span>]  
&gt;&gt;&gt;[<span class="number">1</span>]  
!类中的可变量的慎重使用！！！！！！！！！！！！！！！ist、dict等是可更改对象，因此修改一个指向的对象时会把类变量也改变了。  
</code></pre>
<h2 id="10-2-基础算法"><a href="#10-2-基础算法" class="headerlink" title="10.2. 基础算法"></a>10.2. 基础算法</h2><h3 id="10-2-1-循环打印输出从1到100的所有奇数"><a href="#10-2-1-循环打印输出从1到100的所有奇数" class="headerlink" title="10.2.1. 循环打印输出从1到100的所有奇数"></a>10.2.1. 循环打印输出从1到100的所有奇数</h3><pre><code class="py"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">101</span>):  
    <span class="keyword">if</span> i % <span class="number">2</span> == <span class="number">1</span>:  
        <span class="keyword">print</span> (i)  
</code></pre>
<h3 id="10-2-2-字符串批量替换"><a href="#10-2-2-字符串批量替换" class="headerlink" title="10.2.2. 字符串批量替换"></a>10.2.2. 字符串批量替换</h3><p>请将字符串“你好$$$我正在学Python@#@#现在需要&amp;<em>&amp;</em>&amp;修改字符串”中的符号变成一个空格，需要输出的格式为：“你好 我正在学Python现在需要 修改字符串”。  </p>
<pre><code class="py"><span class="comment"># 方法1  </span>
str1 = <span class="string">'你好$$$我正在学Python@#@#现在需要&amp;%&amp;%&amp;修改字符串'</span>  
str2 = str1.replace(<span class="string">'$$$'</span>, <span class="string">' '</span>).replace(<span class="string">'@#@#'</span>, <span class="string">' '</span>).replace(<span class="string">'&amp;%&amp;%&amp;'</span>, <span class="string">' '</span>)  
<span class="keyword">print</span> (str2)  
<span class="comment"># 方法2  </span>
<span class="keyword">import</span> re  
str1 = <span class="string">'你好$$$我正在学Python@#@#现在需要&amp;%&amp;%&amp;修改字符串'</span>  
str2 = re.sub(<span class="string">'[$@#&amp;%]+'</span>, <span class="string">' '</span> ,str1)  
<span class="keyword">print</span> (str2)  
</code></pre>
<h3 id="10-2-3-输出9×9乘法口诀表"><a href="#10-2-3-输出9×9乘法口诀表" class="headerlink" title="10.2.3. 输出9×9乘法口诀表"></a>10.2.3. 输出9×9乘法口诀表</h3><pre><code class="py"><span class="comment"># 此法会有多余的换行和末尾对于的空格  </span>
<span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">10</span>):  
    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>,i+<span class="number">1</span>):  
        print(<span class="string">'{}×{}={}'</span>.format(j,i,j*i),end=<span class="string">' '</span>)  
    print(<span class="string">'\n'</span>)  

<span class="comment"># 更好的方法，没有对齐  </span>

<span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">10</span>):  
    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>,i+<span class="number">1</span>):  
        print(<span class="string">'{}×{}={} '</span>.format(j,i,j*i),end=<span class="string">''</span>)  
    print(<span class="string">''</span>)  

<span class="comment"># 最好的方法 这里是对齐的。由此可见，'\t'是用来【显示】对齐的，但似乎len就是1  </span>
<span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">10</span>):  
    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>,i+<span class="number">1</span>):  
        print(<span class="string">'{}×{}={}\t'</span>.format(j,i,j*i),end=<span class="string">''</span>)  
    print(<span class="string">''</span>)  

<span class="comment"># 最好的方法 这里是对齐的  </span>
<span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">10</span>):  
    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, i+<span class="number">1</span>):  
        <span class="keyword">print</span> (<span class="string">"%dx%d=%d\t"</span> % (j, i, i*j), end=<span class="string">""</span>)  
    print(<span class="string">""</span>)  

<span class="comment"># 1×1=1  </span>

<span class="comment"># 1×2=2 2×2=4  </span>

<span class="comment"># 1×3=3 2×3=6 3×3=9  </span>

<span class="comment"># 1×4=4 2×4=8 3×4=12 4×4=16  </span>

<span class="comment"># 1×5=5 2×5=10 3×5=15 4×5=20 5×5=25   </span>

<span class="comment"># 1×6=6 2×6=12 3×6=18 4×6=24 5×6=30 6×6=36  </span>

<span class="comment"># 1×7=7 2×7=14 3×7=21 4×7=28 5×7=35 6×7=42 7×7=49  </span>

<span class="comment"># 1×8=8 2×8=16 3×8=24 4×8=32 5×8=40 6×8=48 7×8=56 8×8=64  </span>

<span class="comment"># 1×9=9 2×9=18 3×9=27 4×9=36 5×9=45 6×9=54 7×9=63 8×9=72 9×9=81  </span>

<span class="comment"># 1×1=1  </span>
<span class="comment"># 1×2=2 2×2=4  </span>
<span class="comment"># 1×3=3 2×3=6 3×3=9  </span>
<span class="comment"># 1×4=4 2×4=8 3×4=12 4×4=16  </span>
<span class="comment"># 1×5=5 2×5=10 3×5=15 4×5=20 5×5=25  </span>
<span class="comment"># 1×6=6 2×6=12 3×6=18 4×6=24 5×6=30 6×6=36  </span>
<span class="comment"># 1×7=7 2×7=14 3×7=21 4×7=28 5×7=35 6×7=42 7×7=49  </span>
<span class="comment"># 1×8=8 2×8=16 3×8=24 4×8=32 5×8=40 6×8=48 7×8=56 8×8=64  </span>
<span class="comment"># 1×9=9 2×9=18 3×9=27 4×9=36 5×9=45 6×9=54 7×9=63 8×9=72 9×9=81  </span>
<span class="comment"># 1×1=1  </span>
<span class="comment"># 1×2=2   2×2=4  </span>
<span class="comment"># 1×3=3   2×3=6   3×3=9  </span>
<span class="comment"># 1×4=4   2×4=8   3×4=12  4×4=16  </span>
<span class="comment"># 1×5=5   2×5=10  3×5=15  4×5=20  5×5=25  </span>
<span class="comment"># 1×6=6   2×6=12  3×6=18  4×6=24  5×6=30  6×6=36  </span>
<span class="comment"># 1×7=7   2×7=14  3×7=21  4×7=28  5×7=35  6×7=42  7×7=49  </span>
<span class="comment"># 1×8=8   2×8=16  3×8=24  4×8=32  5×8=40  6×8=48  7×8=56  8×8=64  </span>
<span class="comment"># 1×9=9   2×9=18  3×9=27  4×9=36  5×9=45  6×9=54  7×9=63  8×9=72  9×9=81  </span>
<span class="comment"># 1x1=1  </span>
<span class="comment"># 1x2=2   2x2=4  </span>
<span class="comment"># 1x3=3   2x3=6   3x3=9  </span>
<span class="comment"># 1x4=4   2x4=8   3x4=12  4x4=16  </span>
<span class="comment"># 1x5=5   2x5=10  3x5=15  4x5=20  5x5=25  </span>
<span class="comment"># 1x6=6   2x6=12  3x6=18  4x6=24  5x6=30  6x6=36  </span>
<span class="comment"># 1x7=7   2x7=14  3x7=21  4x7=28  5x7=35  6x7=42  7x7=49  </span>
<span class="comment"># 1x8=8   2x8=16  3x8=24  4x8=32  5x8=40  6x8=48  7x8=56  8x8=64  </span>
<span class="comment"># 1x9=9   2x9=18  3x9=27  4x9=36  5x9=45  6x9=54  7x9=63  8x9=72  9x9=81  </span>

print(len(<span class="string">'{}\t'</span>.format(<span class="number">5</span>*<span class="number">6</span>)))  
print(len(<span class="string">'{}\t'</span>.format(<span class="number">5</span>*<span class="number">60</span>)))  
print(len(<span class="string">'{}\t'</span>.format(<span class="number">5</span>*<span class="number">600</span>)))  
print(len(<span class="string">'{}\t'</span>.format(<span class="number">5</span>*<span class="number">6000</span>)))  

<span class="comment"># 3  </span>
<span class="comment"># 4  </span>
<span class="comment"># 5  </span>
<span class="comment"># 6  </span>
</code></pre>
<h3 id="10-2-4-利润分段计算"><a href="#10-2-4-利润分段计算" class="headerlink" title="10.2.4. 利润分段计算"></a>10.2.4. 利润分段计算</h3><p>请写出一个函数，当输入函数变量月利润为I时，能返回应发放奖金的总数。例如，输出“利润为100000元时，应发放奖金总数为10000元”。<br>其中，企业发放的奖金根据利润提成。<br>利润（I）低于或等于10万元时，奖金可提10%；<br>利润高于10万元，低于20万元时，低于10万元的部分按10%提成，高于10万元的部分，可提成7.5%；<br>利润在20万元到40万元之间时，高于20万元的部分可提成5%；<br>利润在40万元到60万元之间时，高于40万元的部分可提成3%；<br>利润在60万元到100万元之间时，高于60万元的部分可提成1.5%；<br>利润高于100万元时，超过100万元的部分按1%提成。  </p>
<pre><code class="py"><span class="function"><span class="keyword">def</span> <span class="title">calcute_profit</span><span class="params">(I)</span>:</span>  
    I = I / <span class="number">10000</span>  
    <span class="keyword">if</span> I &lt;= <span class="number">10</span>:  
        a = I * <span class="number">0.01</span>  
        <span class="keyword">return</span> a * <span class="number">10000</span>  
    <span class="keyword">elif</span> I &lt;= <span class="number">20</span> <span class="keyword">and</span> I &gt; <span class="number">10</span>:  
        b =<span class="number">0.25</span> + I * <span class="number">0.075</span>  
        <span class="keyword">return</span> b * <span class="number">10000</span>  
    <span class="keyword">elif</span> I &lt;= <span class="number">40</span> <span class="keyword">and</span> I &gt; <span class="number">20</span>:  
        c = <span class="number">0.75</span> + I * <span class="number">0.05</span>  
        <span class="keyword">return</span> c * <span class="number">10000</span>  
    <span class="keyword">elif</span> I &lt;= <span class="number">60</span> <span class="keyword">and</span> I &gt; <span class="number">40</span>:  
        d = <span class="number">0.95</span> + I * <span class="number">0.03</span>  
        <span class="keyword">return</span> d * <span class="number">10000</span>  
    <span class="keyword">elif</span> I &lt;= <span class="number">60</span> <span class="keyword">and</span> I &gt; <span class="number">100</span>:  
        e = <span class="number">2</span> + I * <span class="number">0.015</span>  
        <span class="keyword">return</span> e * <span class="number">10000</span>  
    <span class="keyword">else</span>:  
        f = <span class="number">2.95</span> + I * <span class="number">0.01</span>  
        <span class="keyword">return</span> f * <span class="number">10000</span>  

I = int(input(<span class="string">'净利润:'</span>))  
profit = calcute_profit(I)  
<span class="keyword">print</span> (<span class="string">'利润为%d元时，应发奖金总数为%d元'</span> % (I, profit))  

<span class="function"><span class="keyword">def</span> <span class="title">calcute_profit</span><span class="params">(I)</span>:</span>  
    arr = [<span class="number">1000000</span>,<span class="number">600000</span>,<span class="number">400000</span>,<span class="number">200000</span>,<span class="number">100000</span>,<span class="number">0</span>] <span class="comment">#这应该就是各个分界值了，把它们放在列表里方便访问  </span>
    rat = [<span class="number">0.01</span>,<span class="number">0.015</span>,<span class="number">0.03</span>,<span class="number">0.05</span>,<span class="number">0.075</span>,<span class="number">0.1</span>] <span class="comment">#这是各个分界值所对应的奖金比例值  </span>
    r = <span class="number">0</span>                       <span class="comment">#这是总奖金的初始值  </span>
    <span class="keyword">for</span> idx <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">6</span>):      <span class="comment">#有6个分界值当然要循环6次  </span>
        <span class="keyword">if</span> I &gt; arr[idx]:  
            r = r + (I - arr[idx]) * rat[idx]  
            I = arr[idx]  
    <span class="keyword">return</span> r  

I = int(input(<span class="string">'净利润:'</span>))  
profit = calcute_profit(I)  
<span class="keyword">print</span> (<span class="string">'利润为%d元时，应发奖金总数为%d元'</span> % (I, profit))  
</code></pre>
<h3 id="10-2-5-字典排序"><a href="#10-2-5-字典排序" class="headerlink" title="10.2.5. 字典排序"></a>10.2.5. 字典排序</h3><p>用字典的值对字典进行排序，将{1: 2, 3: 4, 4:3, 2:1, 0:0}按照字典的值从大到小进行排序。  </p>
<pre><code class="py"><span class="keyword">import</span> operator  
x = {<span class="number">1</span>: <span class="number">2</span>, <span class="number">3</span>: <span class="number">4</span>, <span class="number">4</span>:<span class="number">3</span>, <span class="number">2</span>:<span class="number">1</span>, <span class="number">0</span>:<span class="number">0</span>}  
sorted_x = sorted(x.items(), key=operator.itemgetter(<span class="number">1</span>))  
<span class="keyword">print</span> (sorted_x)  
</code></pre>
<p>[(0, 0), (2, 1), (1, 2), (4, 3), (3, 4)]<br>对字典进行排序是【不可能】的，只有把字典【转换】成另一种方式才能排序。字典本身是无序的，但是如列表元组等其他类型是有序的，所以需要用一个元组列表来表示排序的字典。  </p>
<h2 id="可能遇到的问题"><a href="#可能遇到的问题" class="headerlink" title="可能遇到的问题"></a>可能遇到的问题</h2><p>批量书合成一页还是每本书单独有序号|很多个人网站每个条目有自己的序号，按序号抓可不重不漏<br>正则表达式的引号问题|转义<br>网页可能不存在|try、except<br>匹配结果为0的情况|try、except<br>编译器vscode跑太慢|换回python自带IDLE，快很多<br>为啥跑着跑着就停了|<br>一级风险：原网站换<br>五级风险：原作者取消网盘链接</p>

        
      </div>
      
    </div>
    
      
    <div class="copyright">
        <p><span>本文标题:</span><a href="/爬虫知识结构/">爬虫知识结构</a></p>
        <p><span>文章作者:</span><a href="/" title="访问 CJ 的个人博客">CJ</a></p>
        <p><span>优先级别:</span>2035-01-01</p>
        <p><span>最后更新:</span>2020-04-30, 17:44</p>
        <p>
            <span>原始链接:</span><a class="post-url" href="/爬虫知识结构/" title="爬虫知识结构">https://cjql.github.io/爬虫知识结构/</a>
            <span class="copy-path" data-clipboard-text="原文: https://cjql.github.io/爬虫知识结构/　　作者: CJ" title="点击复制文章链接"><i class="fa fa-clipboard"></i></span>
            <script> var clipboard = new Clipboard('.copy-path'); </script>
        </p>
        
            <p>
                <span>更新历史:</span>
                <i class="fa fa-github"></i>
                <a href="https://github.com/cjql/blog/blame/master/source/_posts/爬虫知识结构.md" title="顺序查看文章各部分修改记录" target="_blank">Blame</a>                
                <i class="fa fa-check-square-o"></i>
                <a href="https://github.com/cjql/blog/edit/master/source/_posts/爬虫知识结构.md" title="直接在线编辑" target="_blank">Edit</a>
                <i class="fa fa-github-alt"></i>
                <a href="https://github.com/cjql/blog/commits/master/source/_posts/爬虫知识结构.md" title="查看文章有关更新记录" target="_blank">History</a>
                <i class="fa fa-file-text-o"></i>	
                <a href="https://raw.githubusercontent.com/cjql/blog/master/source/_posts/爬虫知识结构.md" title="查看 & 下载文章 Markdown 原始文本" target="_blank">mdRaw</a>
                <i class="fa fa-file-movie-o"></i>
                <a href="https://github.com/cjql/blog/blob/master/source/_posts/爬虫知识结构.md" title="GitHub查看" target="_blank">NormalView</a>
            </p>
        
        <p>
            <span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" title="中国大陆 (CC BY-NC-SA 3.0 CN)" target="_blank">"署名-非商用-相同方式共享 3.0"</a> 转载请保留原文链接及作者。
        </p>
    </div>

      
    <nav id="article-nav">
        
            <div id="article-nav-newer" class="article-nav-title">
                <a href="/NLTK/">
                    NLTK
                </a>
            </div>
        
        
            <div id="article-nav-older" class="article-nav-title">
                <a href="/Jupyter/">
                    Jupyter
                </a>
            </div>
        
    </nav>

    
  </article>
  
  
  
  
  
  
    <div id="toc" class="toc-article">
        <strong class="toc-title">文章目录</strong>
        
            <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-爬虫基础"><span class="toc-text">1. 爬虫基础</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-为什么要学"><span class="toc-text">1.1. 为什么要学</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-类型"><span class="toc-text">1.2. 类型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-3-技能"><span class="toc-text">1.3. 技能</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-4-应用场景"><span class="toc-text">1.4. 应用场景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-5-流程图"><span class="toc-text">1.5. 流程图</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-获取网页"><span class="toc-text">2. 获取网页</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-法律和道德、注意事项"><span class="toc-text">2.1. 法律和道德、注意事项</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-网页特点"><span class="toc-text">2.2. 网页特点</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-1-获取动态网页的真实地址"><span class="toc-text">2.2.1. 获取动态网页的真实地址</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-抓包分析工具"><span class="toc-text">2.3. 抓包分析工具</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4-API"><span class="toc-text">2.4. API</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-5-常用库"><span class="toc-text">2.5. 常用库</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-6-requests"><span class="toc-text">2.6. requests</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-7-urllib"><span class="toc-text">2.7. urllib</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-7-1-核心代码"><span class="toc-text">2.7.1. 核心代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-7-2-eg-1-直接GET"><span class="toc-text">2.7.2. eg.1 直接GET</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-7-3-eg-2-带参数的GET"><span class="toc-text">2.7.3. eg.2 带参数的GET</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-7-4-eg-3-POST"><span class="toc-text">2.7.4. eg.3 POST</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-7-5-等效写法"><span class="toc-text">2.7.5. 等效写法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-7-6-类方法解析"><span class="toc-text">2.7.6. 类方法解析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-7-7-response"><span class="toc-text">2.7.7. response</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-7-8-设置代理"><span class="toc-text">2.7.8. 设置代理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-7-9-设置headers"><span class="toc-text">2.7.9. 设置headers</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-8-Selenium"><span class="toc-text">2.8. Selenium</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-9-爬虫与反爬"><span class="toc-text">2.9. 爬虫与反爬</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-9-1-爬虫的尽头"><span class="toc-text">2.9.1. 爬虫的尽头</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-9-2-反爬的尽头"><span class="toc-text">2.9.2. 反爬的尽头</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-9-3-网站为什么要“反爬虫”"><span class="toc-text">2.9.3. 网站为什么要“反爬虫”</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-9-4-爬虫与反爬一览"><span class="toc-text">2.9.4. 爬虫与反爬一览</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-9-5-常见的反爬措施"><span class="toc-text">2.9.5. 常见的反爬措施</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-9-5-1-浏览器伪装技术"><span class="toc-text">2.9.5.1. 浏览器伪装技术</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-9-5-2-Cookie的使用"><span class="toc-text">2.9.5.2. Cookie的使用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-9-5-3-访问频率"><span class="toc-text">2.9.5.3. 访问频率</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-9-5-4-登录限制"><span class="toc-text">2.9.5.4. 登录限制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-9-5-5-通过Header封杀"><span class="toc-text">2.9.5.5. 通过Header封杀</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-9-5-6-JavaScript脚本动态获取网站数据"><span class="toc-text">2.9.5.6. JavaScript脚本动态获取网站数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-9-5-7-验证码"><span class="toc-text">2.9.5.7. 验证码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-9-5-8-ip限制"><span class="toc-text">2.9.5.8. ip限制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-9-5-9-服务器采集"><span class="toc-text">2.9.5.9. 服务器采集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-9-5-10-网站内容反爬"><span class="toc-text">2.9.5.10. 网站内容反爬</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-10-编码"><span class="toc-text">2.10. 编码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-11-爬虫进阶"><span class="toc-text">2.11. 爬虫进阶</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-12-异常处理"><span class="toc-text">2.12. 异常处理</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-数据解析、清洗和组织"><span class="toc-text">3. 数据解析、清洗和组织</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-bs4"><span class="toc-text">3.1. bs4</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-Xpath"><span class="toc-text">3.2. Xpath</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-text">2016 腾讯软件开发面试题（部分）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-解析器对比分析"><span class="toc-text">3.3. 解析器对比分析</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-数据存储"><span class="toc-text">4. 数据存储</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-数据库"><span class="toc-text">4.1. 数据库</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-高性能爬取策略"><span class="toc-text">5. 高性能爬取策略</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#5-1-性能"><span class="toc-text">5.1. 性能</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-Scrapy分布式爬虫"><span class="toc-text">6. Scrapy分布式爬虫</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#6-1-概述"><span class="toc-text">6.1. 概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-2-部署到Scrapinghub"><span class="toc-text">6.2. 部署到Scrapinghub</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-3-代码"><span class="toc-text">6.3. 代码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-4-使用Scrapy填充数据库"><span class="toc-text">6.4. 使用Scrapy填充数据库</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-5-Scrapyd"><span class="toc-text">6.5. Scrapyd</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-6-Scrapy项目"><span class="toc-text">6.6. Scrapy项目</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-数据分析"><span class="toc-text">7. 数据分析</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#7-1-可视化"><span class="toc-text">7.1. 可视化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-2-NumPy"><span class="toc-text">7.2. NumPy</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-3-pandas"><span class="toc-text">7.3. pandas</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8-框架"><span class="toc-text">8. 框架</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#9-项目实施"><span class="toc-text">9. 项目实施</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#9-1-框架搭建"><span class="toc-text">9.1. 框架搭建</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-2-扩展技能"><span class="toc-text">9.2. 扩展技能</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-3-后起"><span class="toc-text">9.3. 后起</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-4-加分项目"><span class="toc-text">9.4. 加分项目</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#10-实际爬虫"><span class="toc-text">10. 实际爬虫</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#10-0-1-简单的爬虫"><span class="toc-text">10.0.1. 简单的爬虫</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-0-2-爬取豆瓣电影TOP250"><span class="toc-text">10.0.2. 爬取豆瓣电影TOP250</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-0-3-爬取动态网页"><span class="toc-text">10.0.3. 爬取动态网页</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-0-4-通过Selenium模拟浏览器抓取"><span class="toc-text">10.0.4. 通过Selenium模拟浏览器抓取</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-0-5-深圳短租"><span class="toc-text">10.0.5. 深圳短租</span></a></li></ol></li></ol><li class="toc-item toc-level-1"><a class="toc-link" href="#11-相关参考"><span class="toc-text">11. 相关参考</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#11-1-allitebooks"><span class="toc-text">11.1. allitebooks</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-2-其他"><span class="toc-text">11.2. 其他</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-3-中文书籍"><span class="toc-text">11.3. 中文书籍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-4-暂缺电子书"><span class="toc-text">11.4. 暂缺电子书</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-5-网址"><span class="toc-text">11.5. 网址</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-6-GitHub上的爬虫项目"><span class="toc-text">11.6. GitHub上的爬虫项目</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#11-6-1-爬虫框架"><span class="toc-text">11.6.1. 爬虫框架</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-6-2-财经类"><span class="toc-text">11.6.2. 财经类</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-7-公众号文章"><span class="toc-text">11.7. 公众号文章</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-8-崔庆才的博客"><span class="toc-text">11.8. 崔庆才的博客</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#11-8-1-一、爬虫入门"><span class="toc-text">11.8.1. 一、爬虫入门</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-8-2-二、爬虫实战"><span class="toc-text">11.8.2. 二、爬虫实战</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-8-3-三、爬虫利器"><span class="toc-text">11.8.3. 三、爬虫利器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-8-4-四、爬虫进阶"><span class="toc-text">11.8.4. 四、爬虫进阶</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#12-附录"><span class="toc-text">12. 附录</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#12-1-平台"><span class="toc-text">12.1. 平台</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-2-软件安装及环境配置"><span class="toc-text">12.2. 软件安装及环境配置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-3-第三方库"><span class="toc-text">12.3. 第三方库</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#13-术语表"><span class="toc-text">13. 术语表</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-库的说明"><span class="toc-text">1. 库的说明</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-re"><span class="toc-text">1.1. re</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#re-match"><span class="toc-text">re.match</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#re-search"><span class="toc-text">re.search</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#re-findall方法"><span class="toc-text">re.findall方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#为什么要在match的模式前加上r"><span class="toc-text">为什么要在match的模式前加上r</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-bs4"><span class="toc-text">1.2. bs4</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-典型应用"><span class="toc-text">2. 典型应用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-0-3-简单的爬虫"><span class="toc-text">2.0.3. 简单的爬虫</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-0-7-深圳短租"><span class="toc-text">2.0.7. 深圳短租</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-工具及资源列表"><span class="toc-text">3. 工具及资源列表</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-网络下载"><span class="toc-text">3.1. 网络下载</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-书籍辅助"><span class="toc-text">3.2. 书籍辅助</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-端口"><span class="toc-text">3.3. 端口</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-库"><span class="toc-text">4. 库</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-Python第三方库"><span class="toc-text">4.1. Python第三方库</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-附录"><span class="toc-text">5. 附录</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-单项分析"><span class="toc-text">6. 单项分析</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#6-1-是什么"><span class="toc-text">6.1. 是什么</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-1-Selenium"><span class="toc-text">6.1.1. Selenium</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-2-正则表达式"><span class="toc-text">6.1.2. 正则表达式</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-2-为什么"><span class="toc-text">6.2. 为什么</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-1-大数据及爬虫"><span class="toc-text">6.2.1. 大数据及爬虫</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-2-Jupyter"><span class="toc-text">6.2.2. Jupyter</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-3-如何"><span class="toc-text">6.3. 如何</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-1-快捷"><span class="toc-text">6.3.1. 快捷</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-2-获取动态网页的真实地址"><span class="toc-text">6.3.2. 获取动态网页的真实地址</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-4-应用场景"><span class="toc-text">6.4. 应用场景</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-4-1-爬虫"><span class="toc-text">6.4.1. 爬虫</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-5-注意事项"><span class="toc-text">6.5. 注意事项</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-多项关系"><span class="toc-text">7. 多项关系</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#7-1-流程图"><span class="toc-text">7.1. 流程图</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-1-网络爬虫、数据采集"><span class="toc-text">7.1.1. 网络爬虫、数据采集</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-2-分类树"><span class="toc-text">7.2. 分类树</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-3-对比分析"><span class="toc-text">7.3. 对比分析</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-4-关系图"><span class="toc-text">7.4. 关系图</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8-元学习（与物）"><span class="toc-text">8. 元学习（与物）</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#9-个人提升（与人）"><span class="toc-text">9. 个人提升（与人）</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#10-代码清单"><span class="toc-text">10. 代码清单</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#10-1-基础语法"><span class="toc-text">10.1. 基础语法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#10-1-1-py"><span class="toc-text">10.1.1. py</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#10-1-1-1-类"><span class="toc-text">10.1.1.1. 类</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-1-2-函数、类，可变与不可变"><span class="toc-text">10.1.2. 函数、类，可变与不可变</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-2-基础算法"><span class="toc-text">10.2. 基础算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#10-2-1-循环打印输出从1到100的所有奇数"><span class="toc-text">10.2.1. 循环打印输出从1到100的所有奇数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-2-2-字符串批量替换"><span class="toc-text">10.2.2. 字符串批量替换</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-2-3-输出9×9乘法口诀表"><span class="toc-text">10.2.3. 输出9×9乘法口诀表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-2-4-利润分段计算"><span class="toc-text">10.2.4. 利润分段计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-2-5-字典排序"><span class="toc-text">10.2.5. 字典排序</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#可能遇到的问题"><span class="toc-text">可能遇到的问题</span></a></li></ol></li>
        
    </div>
    <style>
        .left-col .switch-btn,
        .left-col .switch-area {
            display: none;
        }
        .toc-level-6 i,
        .toc-level-6 ol {
            display: none !important;
        }
    </style>

    <input type="button" id="tocButton" value="隐藏目录" title="点击按钮隐藏或者显示文章目录">

    <script>
        yiliaConfig.toc = ["隐藏目录", "显示目录", !!"true"];
    </script>

  
  
  
  
      
  
  
  

    <div class="scroll" id="post-nav-button">
        
            <a href="/NLTK/" title="上一篇: NLTK">
                <i class="fa fa-angle-left"></i>
            </a>
        

        <a title="文章列表"><i class="fa fa-bars"></i><i class="fa fa-times"></i></a>

        
            <a href="/Jupyter/" title="下一篇: Jupyter">
                <i class="fa fa-angle-right"></i>
            </a>
        
    </div>

    <ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/顶级目录/">顶级目录</a></li><li class="post-list-item"><a class="post-list-link" href="/Words/">Words</a></li><li class="post-list-item"><a class="post-list-link" href="/计算机网络/">计算机网络</a></li><li class="post-list-item"><a class="post-list-link" href="/编号之美/">编号之美</a></li><li class="post-list-item"><a class="post-list-link" href="/知识整理术语表/">知识整理术语表</a></li><li class="post-list-item"><a class="post-list-link" href="/计算机学习路线/">计算机学习路线</a></li><li class="post-list-item"><a class="post-list-link" href="/计算机知识框架/">计算机知识框架</a></li><li class="post-list-item"><a class="post-list-link" href="/输入输出系统/">计算机组成原理_输入输出系统</a></li><li class="post-list-item"><a class="post-list-link" href="/计算机组成原理/">计算机组成原理</a></li><li class="post-list-item"><a class="post-list-link" href="/代码大全/">代码大全</a></li><li class="post-list-item"><a class="post-list-link" href="/命名规范/">命名规范</a></li><li class="post-list-item"><a class="post-list-link" href="/number-of-recent-calls/">number-of-recent-calls</a></li><li class="post-list-item"><a class="post-list-link" href="/search-in-a-binary-search-tree/">search-in-a-binary-search-tree</a></li><li class="post-list-item"><a class="post-list-link" href="/self-dividing-numbers/">self-dividing-numbers</a></li><li class="post-list-item"><a class="post-list-link" href="/di-string-match/">di-string-match</a></li><li class="post-list-item"><a class="post-list-link" href="/hamming-distance/">hamming-distance</a></li><li class="post-list-item"><a class="post-list-link" href="/swap-salary/">swap-salary</a></li><li class="post-list-item"><a class="post-list-link" href="/twoSum/">twoSum</a></li><li class="post-list-item"><a class="post-list-link" href="/array-partition-i/">array-partition-i</a></li><li class="post-list-item"><a class="post-list-link" href="/fibonacci-number/">fibonacci-number</a></li><li class="post-list-item"><a class="post-list-link" href="/univalued-binary-tree/">univalued-binary-tree</a></li><li class="post-list-item"><a class="post-list-link" href="/merge-two-binary-trees/">merge-two-binary-trees</a></li><li class="post-list-item"><a class="post-list-link" href="/k-closest-points-to-origin/">k-closest-points-to-origin</a></li><li class="post-list-item"><a class="post-list-link" href="/n-ary-tree-preorder-traversal/">n-ary-tree-preorder-traversal</a></li><li class="post-list-item"><a class="post-list-link" href="/n-ary-tree-postorder-traversal/">n-ary-tree-postorder-traversal</a></li><li class="post-list-item"><a class="post-list-link" href="/peak-index-in-a-mountain-array/">peak-index-in-a-mountain-array</a></li><li class="post-list-item"><a class="post-list-link" href="/sort-array-by-parity-ii/">sort-array-by-parity-ii</a></li><li class="post-list-item"><a class="post-list-link" href="/数据结构与算法/">数据结构与算法</a></li><li class="post-list-item"><a class="post-list-link" href="/Flask语法释义/">Flask语法释义</a></li><li class="post-list-item"><a class="post-list-link" href="/Hexo/">Hexo</a></li><li class="post-list-item"><a class="post-list-link" href="/windows下用Flask开发微信公众号/">Windows环境下用Flask开发微信公众号</a></li><li class="post-list-item"><a class="post-list-link" href="/SQL/">SQL</a></li><li class="post-list-item"><a class="post-list-link" href="/Python知识结构/">Python知识结构</a></li><li class="post-list-item"><a class="post-list-link" href="/文件/">文件操作</a></li><li class="post-list-item"><a class="post-list-link" href="/NLTK/">NLTK</a></li><li class="post-list-item"><a class="post-list-link" href="/爬虫知识结构/">爬虫知识结构</a></li><li class="post-list-item"><a class="post-list-link" href="/Jupyter/">Jupyter</a></li><li class="post-list-item"><a class="post-list-link" href="/git/">git</a></li><li class="post-list-item"><a class="post-list-link" href="/GitHub/">GitHub</a></li><li class="post-list-item"><a class="post-list-link" href="/Docker/">Docker</a></li></ul>


  
  
      <script>
          
      </script>
  </div>
      <footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                2018-2020 <i class="fa fa-heart"></i> CJ
            </div>
            <div class="footer-right">
                Thanks for <a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的博客框架">Hexo</a> <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减 Hexo 双栏博客主题 v3.5">Yelee</a><span> and Moxfive <i class="fa fa-copyright"></i> 
            </span></div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_pv" style="display:none">
                        <span id="site-visit" title="本站到访数"><i class="fa fa-user" aria-hidden="true"></i><span id="busuanzi_value_site_uv"></span>
                        </span>
                    </span>
                
                
                    <span>| </span>
                
                
                    <span id="busuanzi_container_page_pv" style="display:none">
                        <span id="page-visit" title="本页阅读量"><i class="fa fa-eye animated infinite pulse" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>
    </div>
    
    <script src="/js/GithubRepoWidget.js"></script>

<script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.2.0/require.min.js"></script>

    <script>
        $(document).ready(function() {
            var iPad = window.navigator.userAgent.indexOf('iPad');
            if (iPad > -1 || $(".left-col").css("display") === "none") {
                var bgColorList = ["#9db3f4", "#414141", "#e5a859", "#f5dfc6", "#c084a0", "#847e72", "#cd8390", "#996731"];
                var bgColor = Math.ceil(Math.random() * (bgColorList.length - 1));
                $("body").css({"background-color": bgColorList[bgColor], "background-size": "cover"});
            }
            else {
                var backgroundnum = 4;
                var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
                $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
            }
        })
    </script>





<div class="scroll" id="scroll">
    <a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" onclick="load$hide();" title="查看评论"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
    <a href="/archives/" title="优先级别"><i class="fa fa-list"></i></a>
    <a href="/" title="返回首页"><i class="fa fa-bank"></i></a>
    <a href="/tags/" title="看标签云"><i class="fa fa-ils"></i></a>
</div>
<script>
    // Open in New Window
    
        var oOpenInNew = {
             github: ".github-widget a", 
            
             post: ".article-entry a[href]", 
            
            
            
             archives: ".archive-article-title", 
             miniArchives: "a.post-list-link", 
            
             friends: "#js-friends a", 
             socail: ".social a" 
        }
        for (var x in oOpenInNew) {
            $(oOpenInNew[x]).attr("target", "_blank");
        }
    
</script>
<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>
</body>
</html>